[{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerate Generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI By Ram Vittal, Amit Modi, Rahul Easwar, and Sandeep Raveesh-Babu on 10 JUL 2025 in Amazon SageMaker AI, Announcements, Technical How-to\nAmazon SageMaker now offers fully managed support for MLflow 3.0, simplifying AI experimentation and accelerating your Generative AI journey from idea to production. This release turns managed MLflow from an experiment-tracking tool into an end-to-end observability solution, reducing time to market for Generative AI.\nAs customers across industries accelerate Generative AI development, they need experiment tracking, behavior observability, and performance evaluation for models and AI applications. Data scientists and developers often struggle to analyze model and application performance from experimentation to production, making root-cause analysis and remediation difficult. Teams spend significant time integrating tools instead of improving the quality of their models or generative AI applications.\nWith the launch of fully managed MLflow 3.0 on Amazon SageMaker AI, you can speed up Generative AI development by tracking experiments and observing model and application behavior with a single tool. MLflow 3.0’s tracing capabilities let customers record inputs, outputs, and metadata at every step of a Generative AI application, helping developers quickly identify the origin of errors or unexpected behavior. By keeping a history of each model and application version, MLflow 3.0 provides traceability, connecting AI feedback back to the underlying components. This lets developers trace issues back to the exact code, data, or parameters that caused them.\nCustomers using Amazon SageMaker HyperPod to train and deploy foundation models (FMs) can now use managed MLflow to track experiments, monitor training, gain deeper insights into model and application behavior, and manage the ML lifecycle at scale. This reduces troubleshooting time and allows teams to focus more on innovation.\nThis post introduces core concepts of fully managed MLflow 3.0 on SageMaker and provides technical guidance so you can leverage the new features to accelerate your next Generative AI application.\nGet started You can get started with fully managed MLflow 3.0 on Amazon SageMaker to track experiments, manage models, and optimize the Generative AI/ML lifecycle via the AWS Management Console, the AWS CLI, or the API.\nPrerequisites To get started you need:\nAn AWS account with billing enabled\nAn Amazon SageMaker Studio AI domain (see the guide: Guide to getting set up with Amazon SageMaker AI)\nConfigure your environment to use the SageMaker-managed MLflow tracking server Configuration steps: In the SageMaker Studio UI, open the Applications panel, choose MLflow, then select Create. Enter a unique name for the tracking server and specify the Amazon S3 URI where experiment artifacts will be stored. Choose Create. (SageMaker defaults to MLflow v3.0.)\nOptional: choose Update to adjust settings such as server size, tags, and IAM role.\nThe server is provisioned and started automatically and typically takes about 25 minutes. After setup completes, you can launch the MLflow UI from SageMaker Studio to begin tracking ML and Generative AI experiments. For more details on tracking server configuration, see Machine learning experiments using Amazon SageMaker AI with MLflow in the SageMaker Developer Guide: https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html\nTo start logging experiments to the SageMaker managed MLflow tracking server you created, install both MLflow and the SageMaker MLflow Python package in your environment. You can use SageMaker Studio managed Jupyter Lab, SageMaker Studio Code Editor, a local IDE, or any environment that supports AI workloads to connect to the SageMaker managed MLflow tracking server.\nTo install both Python packages with pip: pip install mlflow==3.0 sagemaker-mlflow==0.1.0\nTo connect and start logging your AI experiments, parameters, and models directly to the managed MLflow on SageMaker, replace the Amazon Resource Name (ARN) of your SageMaker MLflow tracking server:\nimport mlflow # SageMaker MLflow ARN tracking_server_arn = \u0026#34;arn:aws:sagemaker:\u0026lt;Region\u0026gt;:\u0026lt;Account_id\u0026gt;:mlflow-tracking-server/\u0026lt;Name\u0026gt;\u0026#34; # Enter ARN mlflow.set_tracking_uri(tracking_server_arn) mlflow.set_experiment(\u0026#34;customer_support_genai_app\u0026#34;) Your environment is now configured and ready to track experiments with the SageMaker managed MLflow tracking server.\nImplement tracing and version tracking for Generative AI applications Generative AI applications have many components — code, configuration, and data — which can become hard to manage without systematic versioning. A LoggedModel entity in managed MLflow 3.0 represents your AI model, agent, or Generative AI application within an experiment. It provides unified tracking of model artifacts, execution traces, evaluation metrics, and metadata across the development lifecycle. A trace is a recorded log of inputs, outputs, and intermediate steps from a single application execution. Traces offer deep visibility into application performance, execution flow, and response quality, aiding debugging and evaluation. With LoggedModel, you can track and compare different versions of an application, making it easy to identify issues, deploy the best version, and maintain a clear record of what was deployed and when.\nTo implement version tracking and tracing with managed MLflow 3.0 on SageMaker, you can establish a model identity with a version using the Git commit hash, set it as the active model context so subsequent traces automatically link to that specific version, enable automatic logging for interactions with Amazon Bedrock, and then make an API call to Anthropic’s Claude 3.5 Sonnet — that call will be fully traced, with inputs, outputs, and metadata automatically recorded in the established model context. Managed MLflow 3.0 tracing integrates with a number of Generative AI libraries and provides one-line automatic tracing for all supported libraries. For details on supported libraries, see Supported Integrations in the MLflow documentation.\n# 1. Define your application version using the git commit logged_model= \u0026#34;customer_support_agent\u0026#34; logged_model_name = f\u0026#34;{logged_model}-{git_commit}\u0026#34; # 2.Set the active model context - traces will be linked to this mlflow.set_active_model(name=logged_model_name) # 3.Set auto logging for your model provider mlflow.bedrock.autolog() # 4. Chat with your LLM provider # Ensure that your boto3 client has the necessary auth information bedrock = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;\u0026lt;REPLACE_WITH_YOUR_AWS_REGION\u0026gt;\u0026#34;, ) model = \u0026#34;anthropic.claude-3-5-sonnet-20241022-v2:0\u0026#34; messages = [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello!\u0026#34;}]}] # All intermediate executions within the chat session will be logged bedrock.converse(modelId=model, messages=messages) After recording this information you can view Generative AI experiments and the LoggedModel for the agent in the Managed MLflow 3.0 tracking server UI, as shown in the screenshot below.\nIn addition to one-line auto tracing, MLflow provides a Python SDK so you can manually instrument your code and work with traces. See the sample notebook sagemaker_mlflow_strands.ipynb in the aws-samples GitHub repository, where we use MLflow manual instrumentation to trace Strands Agents. With tracing in fully managed MLflow 3.0 you can capture inputs, outputs, and metadata for each intermediate step of a request to help identify the root cause of errors and unexpected behavior.\nThese capabilities provide observability for your AI workloads by collecting detailed information about the execution of workload services, nodes, and tools, and you can inspect them under the Traces tab.\nYou can inspect an individual trace, as illustrated below, by selecting the request ID for the trace you want to view in the Traces tab.\nFully managed MLflow 3.0 on Amazon SageMaker also introduces tagging for traces. Tags are mutable key–value pairs you can attach to traces to add metadata and contextual information. Trace tags make it easy to organize, search, and filter traces by criteria such as user session, environment, model version, or performance characteristics. You can add, update, or remove tags at any time — while a trace is executing using mlflow.update_current_trace() or after a trace has been logged using the MLflow APIs or UI.\nManaged MLflow 3.0 makes searching and analyzing traces seamless, helping teams quickly identify issues, compare agent behavior, and optimize performance. Both the tracing UI and the Python API support powerful filtering, so you can dive into traces based on attributes such as status, tags, user, environment, or execution time, as shown in the screenshot below.\nFor example, you can immediately find all traces with errors, filter by production environment, or find traces for a specific request. This capability is important for debugging, cost analysis, and continuous improvement of Generative AI applications.\nThe screenshot below shows the traces returned when searching for the tag \u0026lsquo;Production\u0026rsquo;.\nThe following code shows how you can search for all traces in the production environment with a successful status:\n# Search for traces in production environment with successful status\ntraces = mlflow.search_traces(filter_string=\u0026ldquo;attributes.status = \u0026lsquo;OK\u0026rsquo; AND tags.environment = \u0026lsquo;production\u0026rsquo;\u0026rdquo;)\nUsing MLflow tracing for Generative AI Building and deploying generative AI agents — such as chat-based assistants, code generators, or customer support assistants — requires deep observability into how those agents interact with large language models (LLMs) and external tools. In a typical agentic workflow, the agent iterates through reasoning steps, calls LLMs, and uses tools or subsystems like search APIs or Model Context Protocol (MCP) servers until the user task is completed. These complex, multi-step interactions make debugging, optimization, and cost tracking particularly challenging.\nTraditional observability tools fall short for generative AI because agent decisions, tool calls, and LLM responses are dynamic and context-dependent. Managed MLflow 3.0 tracing provides comprehensive observability by recording every LLM call, tool invocation, and decision point in the agent workflow. You can use this end-to-end trace data to:\nDebug agent behavior — pinpoint where an agent’s reasoning went off track or why it produced an unexpected output.\nMonitor tool usage — discover when and how external tools are invoked and analyze their impact on quality and cost.\nTrack performance and cost — measure latency, token usage, and API cost at each step of the agentic loop.\nAudit and govern — maintain detailed logs for compliance and analysis.\nImagine a practical scenario using the managed MLflow 3.0 tracing UI for a sample finance customer support agent equipped with a tool for retrieving financial data from a datastore. While developing a generative AI customer support agent or analyzing agent behavior in production, you can observe whether the agent’s response and execution invoke the product database tool to provide more accurate recommendations.\nExample: the first trace (shown below) captures the agent handling a user query without calling any tools. The trace records the prompt, agent response, and decision points. The agent’s response lacks product-specific detail. The trace clearly shows that no external tool was invoked, which helps you quickly identify this behavior in the agent’s reasoning chain.\nThe second trace (shown below) records the same agent but this time the agent decides to call the product database tool. This trace logs the tool invocation, the returned product data, and how the agent integrates that information into the final response. Here, you can observe improved answer quality, slightly increased latency, and additional API cost due to higher token usage.\nBy comparing these traces side-by-side you can debug why the agent sometimes skips using the tool, optimize when and how tools are called, and balance quality with latency and cost. MLflow’s Tracing UI makes agentic loops transparent, actionable, and straightforward to analyze at scale. The sample agent used in this post and the full source code are available in the aws-samples GitHub repository so you can reproduce and adapt it for your own applications.\nClean up resources After creation, a SageMaker managed MLflow tracking server incurs charges until you stop or delete it. Tracking server charges are based on server uptime, selected size, and the amount of data logged to the tracking server. You can stop the tracking server when not in use to save costs, or delete it via the API or the SageMaker Studio UI. For pricing details, see Amazon SageMaker pricing.\nConclusion Fully managed MLflow 3.0 on Amazon SageMaker AI is now available. Get started with the sample code in the aws-samples GitHub repository. We invite you to explore the new features and experience the increased productivity and control they provide for your ML projects. To learn more, visit Machine Learning Experiments using Amazon SageMaker with MLflow.\nFor more information, see the SageMaker Developer Guide and send feedback to AWS re:Post for SageMaker or through your usual AWS Support channels.\nAbout the authors Ram Vittal Ram Vittal is a Principal ML Solutions Architect at AWS. He has over three decades of experience in architecting and building distributed, hybrid, and cloud-native applications. He is passionate about creating secure, scalable, and reliable AI/ML and big data solutions that help enterprise customers accelerate their cloud adoption and optimization journey to improve business outcomes. In his spare time, he enjoys motorcycle riding and walking his three-year-old sheep-a-doodle dog. Sandeep Raveesh Sandeep Raveesh is a GenAI Specialist Solutions Architect at AWS. He works with customers throughout their AIOps journey, including model training, Retrieval-Augmented Generation (RAG), GenAI Agents, and scaling Generative AI use cases. He also focuses on Go-To-Market strategies to help AWS design and tailor offerings that address industry challenges in the Generative AI domain. You can find Sandeep on LinkedIn. Amit Modi Amit Modi is the Head of Product for SageMaker AIOps and Governance, as well as Responsible AI at AWS. With over a decade of B2B experience, he has built scalable products and teams that drive innovation and deliver customer value globally. Rahul Easwar Rahul Easwar is a Senior Product Manager at AWS, leading Managed MLflow and Partner AI Apps within the SageMaker AIOps group. With over 15 years of experience spanning startups to enterprise technology, he leverages his entrepreneurial background and an MBA from Chicago Booth to build scalable ML platforms that simplify AI adoption for organizations worldwide. Connect with Rahul on LinkedIn to learn more about his work in ML platforms and enterprise AI solutions. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AI-enhanced Subsurface Infrastructure Mapping on AWS By: Santi Adavani, Jacques Guigne, Ryan Qi, Souvik Mukherjee, Srinivas Tadepalli, and Vidyasagar Ananthan — 13 May 2025 — AWS Batch, AWS ParallelCluster, Customer Solutions, High Performance Computing, Thought Leadership\nSubsurface infrastructure mapping is the process of identifying and visualizing buried structures such as pipes, cables, tanks, and foundations located beneath the surface without excavation. This technology is critical for urban planning, utility maintenance, oil \u0026amp; gas operations, construction safety, and environmental protection. Without accurate subsurface maps, construction projects risk costly delays, dangerous utility strikes, and environmental damage. For example, when Hurricane Ivan damaged an offshore oil platform in 2004, important infrastructure was buried under 35–45 meters of sediment, creating an invisible hazard that traditional mapping techniques could not fully detect.\nThrough a collaboration between S2 Labs, Empact AI, and Kraken Robotics, a breakthrough in subsurface infrastructure mapping has emerged on AWS. The approach combines advanced magnetic imaging with physics-informed AI to deliver unprecedentedly clear images of subsurface structures — especially in scenarios where traditional methods fail. The combination of cloud computing and AI is changing how the industry visualizes and understands critical buried infrastructure.\nDetection methods and limitations Traditional subsurface imaging uses a range of geophysical techniques, each suited to specific materials and conditions. For example, electromagnetic methods detect metallic pipes and cables via conductivity, while magnetometers measure variations in Earth’s magnetic field to identify ferrous materials such as steel pipes. Ground-penetrating radar (GPR) is particularly effective at imaging concrete structures and geological layers; specialized frequencies can reveal plastic pipes or water-bearing assets because of dielectric contrasts.\nManual interpretation of survey data typically involves 2D signal analysis and basic depth estimation — a fast but approximate approach. Two main challenges arise: first, different subsurface configurations can produce nearly identical sensor responses, making it ambiguous which configuration is correct without additional data. Second, real-world environments contain heterogeneous soils, moisture levels, and material densities that vary over short distances, producing complex signals that traditional algorithms struggle to interpret accurately.\nExamining a magnetic survey result, surface measurements show magnetic intensity variations across a 50 m area as illustrated in Figure 1(a). When processed with conventional methods, we obtain a blurred image suggesting a pipe-like structure down to about 5 m, but the result lacks detail (Figure 1(b)). This is where an AI-based approach shows its strength — producing a much sharper image that reveals a pipe-like structure at 1–1.5 m depth (Figure 1(c)). The AI-based result is far more precise while remaining consistent with the original magnetic measurements.\nFigure 1. (a) Map view over a 50 m survey patch. (b) Conventional small-square inversion. (c) Deep-learning-based inversion.\nPhysics-informed deep learning solution S2 Labs applies physics-informed AI and AWS high performance computing (HPC) to solve hard engineering problems across oil \u0026amp; gas, manufacturing, healthcare, and biotech — delivering scientifically accurate results with reduced compute time. S2 Labs partnered with two domain specialists: Empact AI, which provides 3D subsurface pipe mapping, and Kraken Robotics, which contributes high-resolution underwater imagery via Synthetic Aperture Sonar. This collaboration integrates advanced sonar, 3D subsurface analysis, and AI-driven pattern recognition on AWS Cloud to locate and characterize pipeline leaks with higher accuracy and speed.\nOur AI approach combines the physics of magnetic fields with deep learning to better interpret what lies underground. By training AI on simulated data that models real-world structures like tanks and pipes, we teach it to “read” magnetic measurements like a map. Using a specialized neural network architecture called U-Net, the model learns to translate magnetic signals into crisp images of subsurface structures, identifying not only location but also composition and shape. For technical details, see the recent research paper published by S2 Labs.\nModel training The physics-informed deep learning model was trained on AWS by combining high performance compute, scalable data storage, and parallel processing services, as illustrated in the architecture diagram in Figure 2.\nUsing Amazon EC2 instances, we generated 202,000 3D susceptibility models (each with 226,000 cells) representing many different subsurface scenarios — including pipes at multiple orientations, multiple-pipe configurations, and tanks.\nThe models were parameterized by domain knowledge and stored as NumPy files in Amazon S3 buckets. S2 Labs’ proprietary magnetostatic solver was containerized and stored in Amazon ECR for consistent deployment across compute resources. The solver processed models sequentially from S3 and wrote the response data back to S3.\nWe also employed distributed computing using AWS Batch to generate synthetic data, utilizing Spot Instances to optimize cost. We used P4d instances, each providing eight NVIDIA A100 GPUs, to compute field responses at 1,800 measurement points spaced 2 meters apart. The pipeline synchronized data between Amazon S3 and local storage, trained a 2D U-Net architecture (500M parameters) for 110 epochs, achieving training loss 0.0018 and validation loss 0.0019. The full computation required 100,000 CPU hours.\nFigure 2. Architecture diagram for synthetic data generation and model training on AWS.\nScalable inference workflow for large magnetic surveys Our magnetic survey pipeline applies a systematic four-stage workflow to process large surveys efficiently while preserving high-quality, reproducible reconstructions of subsurface infrastructure, illustrated in Figure 3.\nStage 1 — Data Acquisition: Field data collection uses magnetometer systems customized to the survey environment — drone-mounted for airborne surveys, ground systems for land surveys, or underwater systems for marine applications. Surveys follow a systematic grid sampling pattern with consistent sensor heights and transect spacing to ensure uniform coverage of the target area.\nStage 2 — Survey Domain Preparation: Rather than processing the entire survey area at once, we adopt a modular approach by dividing the survey domain into smaller tiles sized to the AI training input. Adjacent tiles include overlap regions that are critical to ensuring smooth transitions in the final reconstruction and avoiding edge artifacts.\nStage 3 — Parallel Processing Architecture: The workflow leverages parallel computing to process many tiles simultaneously, dramatically reducing compute time while preserving consistency with the trained model parameters. This distributed approach efficiently uses compute resources by processing tiles independently. For example, our deployment can process a survey volume of 400 m x 400 m x 60 m in under 5 seconds.\nStage 4 — AI-Based Inference: The trained AI model performs inference on each tile independently, reconstructing subsurface magnetic susceptibility distributions from field magnetic measurements. The reconstructions are then blended smoothly using weighted blending across overlap regions to ensure seamless transitions between neighboring tiles. This modular pipeline enables scalable surveying at any size while maintaining consistent resolution and optimizing memory usage via effective parallel processing, making it practical for real-world applications from infrastructure mapping to geological surveys.\nFigure 3. Modular processing workflow for large-scale magnetic surveys.\nCase study: mapping buried offshore well conductors in the Gulf of Mexico Hurricane Ivan (2004) damaged an offshore oil platform in the Gulf of Mexico, burying well conductors under 35–45 meters of sediment. Initial acoustic imaging in 2022 had some success but remained limited where gas-bearing sediments obscured critical areas. A high-resolution magnetometer array was deployed 3.5 m above the seafloor to detect iron-rich conductors through hydrocarbon-saturated sediment.\nThe model described earlier successfully mapped conductors buried at 35–45 m depths, revealing a primary conductor bundle and a secondary fragment 40 m northeast of the well bay (see Figure 4). The results show strong discrimination of magnetic signatures even in complex debris fields, verified where possible with borehole locations and acoustic imagery. This demonstrates the power of deep learning in cases where traditional acoustic methods fail.\nFigure 4. Plan view (a) and oblique view (b) of relative susceptibility distribution.\nConclusion Our work demonstrates how AI-enhanced magnetic imaging is transforming subsurface infrastructure mapping across industries — from onshore utilities to deep offshore well conductors. Physics-informed deep learning trained on AWS combines HPC resources, scalable storage, and parallel processing services to overcome limitations of traditional magnetic data interpretation.\nThrough real-world case studies, we demonstrated that deep learning can exceed traditional magnetic interpretation limits and map structures successfully at 40 m depth beneath the seafloor — structures that remained “invisible” to acoustic methods for 18 years.\nThe impact of this technology spans oil \u0026amp; gas decommissioning, urban utility mapping, environmental protection, and marine operations. While results are promising, opportunities remain in multi-physics integration, real-time processing, and higher-resolution imaging.\nTo collaborate or learn more about deployment, please contact us at santi@s2labs.co or ryanqi@amazon.com.\nAuthors Santi Adavani Dr. Santi Adavani is the founder and CEO of S2 Labs, a deep-tech startup building AI products to accelerate scientific discovery. Prior to S2 Labs, Santi founded and served as CTO of RocketML, where he built an MLOps platform backed by HPC. He also served as Head of Product and AI at PostgresML, leading development of an in-memory Postgres-based vector database, and earlier held senior product leadership roles at Intel. Santi holds a PhD in Computational Science and Engineering from University of Pennsylvania. Jacques Guigné Professor Jacques Yves Guigné is a Senior Advisor to Kraken Robotics in Newfoundland, Canada. He serves as Chief Scientific Officer of Subsea Micropiles Ltd., which operates in Ireland and the U.K., and is CEO of Acoustic Zoom Inc., a leading geophysical research company. Jacques brings deep experience in acoustic imaging and has made significant contributions to mapping complex seabed features. His scientific achievements include over 80 patents and 70 publications with strong citation counts on ResearchGate. He has been honored in physics with the Deryck Chesterman and Rayleigh Medals, holds degrees including a DSc and a PhD, and is recognized as a Geoscience Canada member and a director of PEGNL (Professional Engineers and Geoscientists Newfoundland and Labrador). Ryan Qi Ryan has 19 years of experience in multiphysics modeling and simulation, strategy, and business development across industrial and digital domains. At AWS, Ryan is a Principal Worldwide BD/GTM Leader focused on simulation technologies and autonomous systems. Souvik Mukherjee Dr. Souvik Mukherjee is a founding member of EmPact-AI and Principal Technical Advisor. His 15+ year career spans energy and technology roles as a geophysicist, data scientist, and product leader. He has received industry recognition such as the Shell GameChanger (2015) award and the URTeC 2019 Best Paper \u0026 Innovation award, among others. He has led and delivered multi-million-dollar projects including commercialization of the award-winning QUANTUM hydraulic-fracture delineation technology for Carbo Ceramics and coordinated the Shell Frontier Exploration Study, managing a cross-disciplinary team of 15 technical specialists that influenced a $100M lease acquisition strategy. Srinivas Tadepalli Srinivas is the Global HPC Go-to-Market lead at AWS, responsible for building a comprehensive GTM strategy across HPC and accelerated computing workloads serving commercial and public-sector customers. Previously he worked at Dassault Systems and holds a PhD in biomedical engineering. Vidyasagar Ananthan Vidyasagar specializes in high performance computing, numerical simulations, optimization engineering, and software development across industry and academia. At AWS, Vidyasagar is a Senior Solutions Architect building predictive models and simulation technologies. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Unlocking the Full Potential of Amazon Connect By Puneet Badlani and Eliza Taylor – May 12, 2025; categories: Amazon Connect, Best Practices, Foundational (100), Thought Leadership\nConsumers today have very high expectations—and your customers are no exception. Every business is racing to adopt the latest technology innovations that can improve service, reduce costs, and support strategic growth. Amazon Connect is one of those solutions—powered by AWS and AI, it’s a modern, scalable contact-center platform. However, cutting corners during implementation can undermine the benefits.\nHere we look at how change-management best practices can not only protect but accelerate your investment—risks to watch for, which metrics really matter, and how to maximize limited time and resources to deliver high-impact change.\nDon’t \u0026ldquo;Trip Up\u0026rdquo; on Stakeholders \u0026ldquo;Stakeholders\u0026rdquo; are often misunderstood. Many organizations focus too much on either internal or external groups. For a customer-service technology and process transformation, it’s critical to focus on external stakeholders because they have direct impact.\nExternal stakeholders include customers, partners, and vendors—the people who contact your service center. Their ability to receive information, resolve issues, and feel heard matters greatly to program success and whether they’ll remain engaged with you long term.\nInternally, large programs require cross-organizational coordination. An executive sponsor can marshal resources and help ensure success. Beyond IT, operations and marketing, you’ll need finance, HR, and other teams involved. Not all stakeholders are equal—the level of engagement and oversight depends on role and influence.\nUnderstand What You Actually Need Amazon Connect is a powerful, customizable platform serving many use cases—from omnichannel contact, interaction automation, agent assist with generative AI, dynamic reporting, automated quality evaluation, and more.\nBut without a clear understanding of current processes, pain points, and business requirements, you risk scope drift and losing benefits. Do you need an intelligent IVR to reduce call volume and shorten transfers to agents? Do you need Connect integrated with a legacy CRM? Are SLAs slipping because of sudden call spikes or scheduling issues?\nIdentify these factors early and a deployment roadmap will emerge, backed by a clear business case. You’ll have a compelling story with metrics to prove ROI and clear goals for the teams. This is also a strength of AWS Partner CloudInteract—using AI to uncover insights and bring precision to improvement planning.\nEnsure Executive Sponsorship A committed executive sponsor plays a vital role in driving momentum and raising organizational awareness for any transformation program. They need a convincing vision and by actively promoting the project they can help surface and address potential risks.\nThis approach helps remove blockers early while fostering a collaborative environment where stakeholders feel encouraged to contribute. Executive presence and support also build trust and credibility—essential elements for a successful rollout.\nBuild Change Ambassadors The biggest mistake in technology transformation is assuming you don’t need to \u0026ldquo;tell the story\u0026rdquo; about the change—that a few technical benefit emails and a single training session are enough.\nYou need to run an internal communications campaign for the change. Investing in two-way channels (monitored email, lunch-and-learns, community forums, Q\u0026amp;A sessions after town halls) will both build goodwill and surface practitioner insights.\nExtend influence by recruiting and supporting champions—people who can spread the change. They’re often early adopters and testers. You can incentivize participation by including these activities in annual goals, which helps expand impact across affected groups.\nTrain More Effectively with Fewer Resources As the program progresses, different groups will require different training. Initially, IT needs to learn the new environment and how to configure and manage it. Next, operations must train agents and supervisors for day-to-day use. Everything starts with a change impact assessment.\nThis assessment identifies which groups are most and most critically affected. If resources are limited and you can’t train everyone directly, focus on the business-breakers—the groups that are mission-critical. For other groups, substitute remote or self-paced learning.\nMeasure What Actually Matters Don’t let metrics become slogans. Does reporting email-open rates actually show that employees understand the change? An opened email doesn’t equal awareness. A better measure might be the number of manager-led meetings run using the toolkit you provided.\nThe mistake is measuring only project metrics. You need to demonstrate long-term improvement: reduced transfer rates, handle-time reductions, CSAT, or time comparisons for manual tasks before (scheduling, performance review, or reporting).\nProject-status reporting is important but temporary—the metrics that really matter are those from the business case. Start tracking them as early as possible, especially in phased rollouts, to quickly surface gaps in training and adoption.\nIn Summary A few key factors make Amazon Connect implementations more effective and unlock full value. Plan ahead, bring the organization along, deploy carefully, and this AI-enabled contact-center solution will exceed expectations—from omnichannel and agent experience to automation and analytics.\nContact AWS and CloudInteract for additional help unlocking the full power of Amazon Connect:\nAWS: https://pages.awscloud.com/GLOBAL-field-SP-Amazon-Connect-Contact-Us-reg.html CloudInteract: https://resources.cloudinteract.io/apollo-making-every-contact-count (Loose translation: \u0026ldquo;unlock the superpowers of Amazon Connect\u0026rdquo;)\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Report on “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)” Purpose of the Event Learn about security in GenAI and AI Agents to strengthen enterprise safety. Explore the AI-Driven Development Lifecycle (AI-DLC) and how it applies to software development. Understand how to build a unified data foundation optimized for analytics and AI. Stay updated on the latest GenAI strategies and trends on AWS. Speakers Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights Main Content Unified Data Platform on AWS for AI \u0026amp; Analytics\nBuilding an end-to-end data pipeline: ingestion → storage → processing → access → governance. Breaking down silos in data, people, and processes; enabling self-service \u0026amp; standardized governance. Key services: S3, Glue, Redshift, Lake Formation, OpenSearch, Kinesis/MSK. GenAI Strategy on AWS\nVision, trends, and enterprise adoption roadmap. Amazon Bedrock: model selection, RAG, guardrails, cost/latency optimization. AgentCore \u0026amp; Amazon Nova with support for frameworks (CrewAI, LangGraph, LlamaIndex\u0026hellip;). Securing GenAI Applications\nOWASP LLM risks; multilayered security: infrastructure → model → application. Five pillars: Compliance, Privacy, Controls, Risk Management, Resilience. Tools: Bedrock Guardrails, Human-in-the-loop, Observability (OpenTelemetry). AI Agents – Productivity Boosters\nFrom assistants to multi-agent systems, automation with less supervision. Use cases: customer support, BI with Amazon Q (QuickSight), process automation. Reliability \u0026amp; Accuracy of GenAI\nMitigating hallucination with Prompt Engineering, RAG, Fine-tuning. RAG workflow: input → embedding → context → LLM → output. AI-Driven Development Lifecycle (AI-DLC)\nLifecycle: Inception → Construction → Operation. Evolution: AI-Assisted → AI-Driven → AI-Managed. Implementation with IaC, automated testing, monitoring, and risk management. Amazon SageMaker – Unified Studio\nUnified environment for data, analytics, and AI. Supports Lakehouse, governance, Zero-ETL integration (S3 ↔ Redshift, Aurora, DynamoDB, RDS\u0026hellip;). Full MLOps: pipelines, registry, deployment, monitoring. Integrated with Bedrock \u0026amp; JumpStart to accelerate GenAI application development. Key Learnings Design Mindset\nBuild data \u0026amp; AI systems end-to-end, removing silos. Apply self-service and governance principles from the start. Technical Architecture\nIntegrate AWS services (S3, Glue, Redshift, SageMaker, Bedrock…) into a unified platform. Apply Zero-ETL, Lakehouse, MLOps for scalability, governance, and sustainable operations. Leverage AI Agents and GenAI frameworks to automate processes and boost productivity. Strategy\nDefine a GenAI adoption roadmap balancing innovation speed and cost. Focus on multilayered security: infra, model, application; combine guardrails \u0026amp; human-in-the-loop. Prioritize reliability and accuracy with RAG, prompt engineering, fine-tuning. Software Development Mindset\nTransition from AI-Assisted → AI-Driven → AI-Managed. Adopt AI-DLC to standardize development with AI involved at every stage. Application to Work In projects:\nExperiment with AI Agents for registration/login and customer support. Use validation/guardrails to safely integrate GenAI into applications. In learning \u0026amp; team projects:\nApply AI-DLC for task division: AI supports code/docs generation, team reviews \u0026amp; approves. Know when to use Lambda (serverless) vs containers (ECS/Fargate). As an intern:\nLearn to apply a business-first approach when writing documentation or gathering requirements. Realize the importance of a solid data foundation for GenAI to deliver real value. Event Experience Joining the “GenAI-powered App-DB Modernization” workshop was a highly valuable experience, giving me a holistic view of modernizing applications and databases using cutting-edge methods and tools. Some key takeaways:\nLearning from Experts AWS experts shared the latest trends in GenAI, Data Foundation, and Security. Gained a clearer understanding of building a unified data foundation for AI \u0026amp; Analytics. Impressed by the vision of AI Agents and their potential to enhance productivity. Hands-on Technical Insights Learned how to design an end-to-end data pipeline: ingestion → storage → processing → access → governance. Explored tools like Amazon Bedrock, AgentCore, and SageMaker Unified Studio. Discovered solutions to reduce hallucination (Prompt Engineering, RAG). Understood how to apply AI-DLC for balancing tasks between AI and humans in software development. Tools \u0026amp; Methods in Practice Explored Bedrock Guardrails to ensure safe GenAI implementation. Understood when to use serverless (AWS Lambda) vs containerization (ECS/Fargate). Learned how to leverage Amazon Q for BI (QuickSight) and customer support. Networking \u0026amp; Exchange The event was a great chance to interact with AWS experts and learn from real-world case studies. Realized the importance of a business-first approach in every technology decision. Key Takeaways GenAI is not just a tool, but requires the right strategy and architecture to generate value. Data and security are the foundations—without them, AI cannot thrive. AI Agents and AI-DLC are set to reshape how we design and operate systems. Event Photos (Add photos from the event here)\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Report: “AI-Driven Development Lifecycle: Reimagining Software Engineering” Event Objectives Understand how AI can automate and optimize stages of the Software Development Lifecycle (SDLC). Embrace the philosophy of AI augmenting humans rather than replacing them in the software development process. Observe how tools like Amazon Q and other AI assistants support developers from ideation and code generation to infrastructure deployment (IaC). Learn about the emerging trend of “AI-first development,” where AI becomes a natural part of future dev workflows. Speakers Toan Huynh My Nguyen Highlights Challenges of programming with AI The introduction covered the limitations and challenges of applying AI to programming:\nAI still struggles with projects that require deep domain knowledge and complex business logic. Developers can have limited control over generated code when prompts and scope are not well-defined. The quality of generated code depends heavily on the prompt and context provided to the model. This motivates the AI-DLC approach: creating a structured process to help AI and humans collaborate more effectively.\nHow AI is changing software development This section analyzed how AI is transforming the software industry:\nAI assists code generation, technical documentation, API design, and automated testing. Developers shift roles from “code writers” to “AI orchestrators” who guide, evaluate, and refine AI outputs. Tools like Amazon Q, GitHub Copilot and ChatGPT for Developers become central parts of modern development workflows. 🔹 What is AI-DLC AI-Driven Development Lifecycle (AI-DLC) is an AI-augmented software development approach where each stage is designed to provide AI with specific context and goals to produce more accurate results.\n🟧 Inception\nBuild Context on Existing Code – feed AI the current codebase so it understands project structure. Elaborate Intent with User Stories – developers describe requirements via user stories to clarify goals. Plan with Units of Work – break work into small units the AI can execute and generate code for. 🟦 Construction\nDomain Model (Component Model) – build domain models or architecture diagrams. Generate Code \u0026amp; Test – AI generates code and tests based on the plan. Add Architectural Components – add API layers, data layers, logging, and security components. Deploy with IaC \u0026amp; Tests – automate deployment using Infrastructure as Code and integration tests. 🔁 Each stage provides richer context for the next, helping AI produce increasingly accurate outputs.\nCore Concepts Context Awareness – AI needs clear context about code, requirements, and domain to work well. Collaborative Generation – humans and AI collaborate: AI generates code, humans direct and verify outputs. Continuous Refinement – iterative cycles to refine outputs and improve quality. Mob Elaboration Mob Elaboration is a collaborative method for elaborating intents:\nMultiple participants contribute user stories, questions, and additional context for the AI. It helps AI gain deeper understanding of domain, goals, and complex logic. This approach reduces the risk of misunderstandings—especially in large or cross-domain teams. The 5-Stage Sequential Process of AI-DLC AI-DLC runs through 5 phases:\nInception – understand requirements and analyze the system. Construction – create domain models and initial structure. Generation – automated code generation. Testing – automated unit and integration testing. Deployment – deploy applications with IaC and CI/CD pipelines. Each loop improves the AI\u0026rsquo;s outputs through incremental learning and feedback.\nDemo 1 – Interactive AI-DLC experience with Amazon Q The demo showcased AI-DLC in practice with a small project:\nStart from a simple idea and turn it into a user story describing business requirements. AI helps split tasks into Units of Work and plans implementation details for each module. Attendees interact with AI using questions, checkboxes, and logical conditions to clarify scope. AI generates code, tests, project structure, and executes trial deployments. The demo illustrated smooth collaboration between AI and humans: AI performs repetitive generation while humans steer and make decisions. Introducing Kiro Philosophy of Kiro\nThe workshop introduced Kiro, an intelligent development environment built around the idea of “AI-native development” where AI is a core collaborator rather than just a tool.\nKiro’s philosophy emphasizes three points:\nDeep integration with the development process – AI participates in planning, context management, and impact analysis. Comprehensive project context – Kiro maintains ongoing awareness of project structure so AI can interact with the whole project rather than single files. Intelligent control \u0026amp; collaboration – developers guide AI via contextual commands so each change has clear intent and consistency. This makes Kiro more than a code generator: it is an ecosystem for collaborative human–AI development.\nProject structure in Kiro\nUnlike traditional text editors like VSCode or JetBrains, Kiro is an AI-aware workspace with structural awareness.\nIts project model includes:\nContext Layer – stores context, domain models, and relationships among modules. Task Layer – manages Units of Work tracked and executed by AI. AI Agent Layer – agents handle specific tasks (code, tests, refactor, deploy) enabling a multi-agent collaborative model. Human-in-the-Loop Control – developers can confirm, modify, or reject AI outputs at any stage. Kiro therefore becomes an ecosystem for coordinated human–AI development rather than just a code editor.\nDemo 2: Kiro in practice In the demonstration, the presenters showed how Kiro implements AI-DLC:\nUser provides a basic business requirement like “build an event management system.” Kiro analyzes intent, creates a domain model, and splits work into user stories. AI generates modules, components, and corresponding test cases. Developers interact with a checkbox-based task control to approve each unit of work. Kiro finally deploys the completed system using IaC and automated tests. The demo proved AI-DLC is practical: AI, human operators, and processes integrate into a single coherent workflow.\nEvent experience Attending the workshop “AI DLC x Kiro: Reinventing Developer Experience with AI” was highly valuable, clarifying how AI can be deeply embedded into the developer experience and how Kiro’s design offers a fresh approach for developers.\nInsights from expert speakers Speakers presented AI-DLC as a platform that automates many SDLC tasks and supports software development using AI. The Kiro introduction gave a perspective on designing an AI-native text editor rather than adding AI plugins to legacy editors. I was particularly impressed by Kiro’s philosophy: minimalism, high performance, user-focused experience, and modular extensibility. Practical technical takeaways The demo showed how AI-DLC and Kiro can create, refactor, and optimize code efficiently. A small starter project was created and managed within Kiro, demonstrating auto-refactoring, test generation and logic analysis. Compared to editors like VSCode and Sublime, Kiro stands out for its AI-first architecture and lightweight plugin model that preserves performance. Modern tooling and potential applications Experiencing AI-DLC on Kiro highlighted the potential to automate development workflows—especially code generation, documentation, and debugging. I saw opportunities to build personal learning and productivity tools that provide smart suggestions and accelerate development. Kiro’s modular design inspires approaches to building flexible, maintainable systems. Networking and discussions The workshop offered chances to connect with developers, AI researchers, and product designers, deepening my understanding of AI-augmented development. Discussions helped me see AI as a creative collaborator, allowing developers to focus more on system logic and architecture. Key lessons AI-DLC combined with Kiro is a model for next-generation development tools—AI-first IDEs that deeply integrate AI into the workflow. Kiro’s “less is more” philosophy shows that simplicity and performance can deliver a stronger developer experience than overly complex systems. I learned that successful AI adoption depends not only on technology but also on the design philosophy and integration approach used in tooling. Sample images from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Report: “WORKSHOP: DATA SCIENCE ON AWS” Event Objectives Share AWS AI services Demonstrate deploying AI models using Amazon SageMaker Show how to deploy models and access them via APIs Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud Developer Engineer, AWS Community Builder Highlights Introduction \u0026amp; the importance of Cloud in Data Science Discussed the role of cloud computing in supporting data processing, training, and deploying AI models at scale.\nCloud vs. On-premise:\nCloud: flexible scalability, rapid deployment, lower operating costs, easy integration. On-premise: high upfront investment, harder to scale, complex maintenance. Cloud (especially AWS) provides a strong foundation for the Data Science pipeline — from collection, storage, and processing to training and deploying AI models.\nAI Layers on AWS AWS organizes the AI ecosystem into three layers, helping users choose the right level of management based on skills and goals:\n1. AI Services (Fully Managed Layer)\nFor users who want to apply AI without deep Machine Learning knowledge.\nFully managed AI services that have been pre-trained by AWS.\nUsers can call APIs to use them directly in applications.\nExamples:\nAmazon Comprehend: Natural language processing (NLP) Amazon Translate: Neural machine translation Amazon Textract: Extract data from documents and invoices Amazon Rekognition: Image and video recognition Amazon Polly: Text-to-speech Amazon Bedrock: Access to foundation models (e.g., Claude, Titan, Mistral) 👉 Benefits: Fast deployment, no model training needed, cost scales with usage.\n2. ML Services (Semi-managed Layer)\nFor Data Scientists and ML Engineers who want to build, train, and deploy ML models with more customization.\nAmazon SageMaker is at the center of this layer: it provides tools to build, train, and deploy ML models.\nKey features:\nData Wrangler: Visual data cleaning and processing. Feature Store: Manage features across models. AutoML (SageMaker Autopilot): Automated model training. Model Registry \u0026amp; Monitoring: Track and manage models after deployment. 👉 Benefits: Full control over the ML pipeline, customizable algorithms, training environments, and deployment workflows.\n3. AI Infrastructure (Self-managed Layer)\nFor organizations or experts who want to fully manage AI/ML infrastructure to optimize cost or performance.\nUsers can build training environments using core AWS infrastructure services:\nAmazon EC2 / GPU Instances (P5, G6, Inferentia): Train large custom models. Amazon EKS / ECS: Run ML workloads in containers or Kubernetes. AWS Lambda: Small-scale data processing or serverless inference. Amazon S3 / EFS: Store data and models. 👉 Benefits: Maximum flexibility and control, but requires higher technical expertise.\nPopular AWS AI Services to Support Students During Model Training 1. Amazon SageMaker\nIntegrated development environment (SageMaker Studio) for the full ML lifecycle:\nData preparation Model training Result tracking Deploying endpoints for API inference Supports AutoML, GPU training, model monitoring, and CI/CD for AI models.\n2. Amazon Comprehend\nNLP service to analyze, understand, and classify natural language.\nMain capabilities:\nSentiment analysis Entity recognition Text classification Automated labeling Language detection Use cases:\nIntelligent document processing Bulk email analysis to detect positive/negative responses Customer sentiment and behavioral analysis Contact center analytics Information extraction and validation 3. Amazon Translate\nNeural machine translation service.\nSupports over 75 languages with high accuracy and easy integration.\nApplications:\nMultilingual websites Automatic content translation in apps Multilingual chatbot support and analytics 4. Amazon Textract\nAutomatically extract text and structured data from images, documents, and forms. Used for processes like record digitization, invoice processing, and automatic data entry. AWS Data Science Pipeline Overview Data collection \u0026amp; storage: Amazon S3, AWS Data Exchange Data preprocessing: AWS Glue, Lambda, Athena Model training: SageMaker (train, tune, evaluate) Model deployment: SageMaker Endpoint / Lambda + API Gateway Monitoring \u0026amp; optimization: CloudWatch, Model Monitor Demo 1: Designing an AI Training Workflow with a Drag-and-Drop Interface (No-Code/Low-Code) Goal: Show how to build an AI training pipeline without heavy coding.\nTools: Amazon SageMaker Studio / SageMaker Canvas\nDemo steps:\nPrepare the dataset and upload it to Amazon S3.\nUse SageMaker\u0026rsquo;s drag-and-drop interface to:\nChoose data sources, training algorithms, and parameters. Design a pipeline including data cleaning, training, validation, and deployment steps. Visually monitor training progress and model results (accuracy, confusion matrix, metrics, etc.).\nKey message: Students and developers can quickly create AI workflows without complex code, speeding up research and experimentation.\nDemo 2: Deploying an AI Service and Accessing It Via API/Website Goal: Demonstrate how to deploy an AI model so users can access it in practice.\nTools: Amazon SageMaker Endpoint, API Gateway, and Lambda.\nDemo steps:\nDeploy the trained model to a SageMaker Endpoint. Integrate the endpoint with API Gateway to create a public REST API. Provide a web route or API URL for users to send requests (e.g., submit text for sentiment analysis or translation). Show how to present results visually (UI demo or Postman/API test). Key message: Demonstrates how AWS supports moving AI from research to production — easy to share, scale, and commercialize.\nDiscussion: Performance \u0026amp; Cost (Cloud vs. On-premise) Criteria Cloud (AWS) On-premise Scalability Easily scale resources as needed Limited by fixed hardware Cost Pay-as-you-go High upfront investment Deployment Automated, fast Manual, time-consuming Maintenance Managed by AWS User is responsible Suitable for students ✅ Free Tier available, easy to learn ❌ Harder to access, costly Conclusion AWS provides a comprehensive AI ecosystem from infrastructure to application layers, suitable for everyone — from students learning AI to enterprises deploying at scale. Event Experience Attending the workshop “AI Services on AWS for Data Science” was very valuable. It helped me better understand the role of cloud in Data Science and how AWS supports training, deploying, and accessing AI models.\nKey takeaways from expert speakers Speakers emphasized the importance of cloud in data processing and model training. Gained a clear understanding of the three AI layers on AWS: AI-managed services, ML services (SageMaker), and AI frameworks. Hands-on technical experience Demo 1: Designed an AI workflow using SageMaker Canvas drag-and-drop to train models without code. Demo 2: Deployed an AI model as a service accessible via API or link. Using modern tools Learned about key AI services: Amazon Comprehend, Translate, and Textract. Understood how these services support NLP, machine translation, and intelligent data extraction. Networking and discussion Interacted with experts and fellow students interested in AI \u0026amp; Cloud. Discussed cost, performance (Cloud vs On-premise), and how to optimize SageMaker usage. Lessons learned Cloud is a foundational platform for modern Data Science workflows. AWS provides tools for every AI skill level — from no-code to fully managed deployments. Gained clearer knowledge of how to bring AI models into real products using AWS services. Some photos from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Le Vu Phuong Hoa\nPhone Number: 0327 030 024\nEmail: danielleee241@gmail.com\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Information Technology\nClass: SE181951\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the fundamentals of cloud computing and AWS global infrastructure. Learn how to manage AWS services using Management Console, CLI, and SDK. Explore AWS security, IAM, and cost management through practical labs. Build foundational knowledge of VPC networking and connectivity (Subnets, Route Tables, IGW, NAT, Peering, Transit Gateway). Gain hands-on experience by performing AWS labs to reinforce theoretical knowledge with practical skills. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Read and take note of internship unit rules and regulations\n- Learning module 01:\n+ Cloud computing: Concept, benefits\n+ AWS Overview:\n+ AWS differentiation, pricing philosophy, leadership principles\n+ Cloud journey\n+ AWS infrastructure: Data center, AZ, Region, Edge Locations\n+ Managing AWS services:\n+ AWS Management Console\n+ AWS CLI\n+ AWS SDK\n- Practice:\n+ Lab 01: Create AWS account, enable MFA, set up IAM User/Group, Access keys\n+ Lab 07: Create different budget types\n+ Lab 09: Using AWS support plans and case\n- Research:\n+ AWS Well-Architected Framework 08/09/2025 08/09/2025 2 - Networking overview: Importance of networking in AWS cloud architecture\n+ Amazon VPC: Concept, difference with traditional private cloud, multi-AZ deployment\n+ VPC CIDR: IPv4 (required), IPv6 (optional), limit of 5 VPC per Region\n+ Subnet: Public vs Private, reserved IP addresses\n+ Route Table: Default route, custom route, public subnet routing\n+ Elastic Network Interface (ENI), Elastic IP address\n+ VPC Endpoint: Interface Endpoint, Gateway Endpoint (S3, DynamoDB)\n+ Internet Gateway: requirements for Internet access (public IP + route)\n+ NAT Gateway: outbound Internet access from private subnet 09/09/2025 09/09/2025 3 VPC Security:\n+ Security Group (stateful, allow rules only, applied to ENI)\n+ Network ACL (stateless, inbound/outbound rules, applied to Subnet)\n+ VPC Flow Logs (monitor traffic, detect denied requests, store in CloudWatch/S3)\n+ Multi-VPC connectivity:\n+ VPC Peering: 1:1 connection, no transitive routing, non-overlapping CIDR\n+ Transit Gateway: hub-and-spoke model, simplify routing, Transit Gateway Attachment per AZ\n+ Hybrid connectivity:\n+ VPN Site-to-Site: Virtual Private Gateway (AWS side) \u0026amp; Customer Gateway (on-premises)\n+ VPN Client-to-Site: costly, usually third-party solution\n+ Direct Connect: dedicated/hosted connection, stable latency (20–30ms), VN providers (Viettel, FPT)\n+ Elastic Load Balancing (ELB):\n+ Concepts: health check, sticky session, access logs (stored in S3)\n+ Types:\n- Application Load Balancer (Layer 7, HTTP/HTTPS, path-based routing)\n- Network Load Balancer (Layer 4, TCP/TLS, static IP, high performance)\n- Classic Load Balancer (Layer 4 \u0026amp; 7, legacy)\n- Gateway Load Balancer (Layer 3, appliance traffic forwarding) 10/09/2025 10/09/2025 4 - Review:\n- Revised knowledge from Day 2 \u0026amp; Day 3: VPC, Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, Network ACL, VPC Peering, Transit Gateway, VPN, Direct Connect, Elastic Load Balancing.\n+ Lab:\n- Lab 1:\n+ Created VPC, Subnet, Internet Gateway, Route Table, Security Group.\n+ Enabled VPC Flow Logs.\n+ Launched EC2 instance and tested SSH connectivity.\n+ Created NAT Gateway for private subnet Internet access.\n+ Used Reachability Analyzer to verify network connectivity.\n+ Managed EC2 using AWS Systems Manager Session Manager.\n+ Enabled CloudWatch Monitoring \u0026amp; Alerting for EC2 monitoring.\n- Lab 2:\n+ Repeated the same steps to reinforce knowledge and improve hands-on practice. 11/09/2025 11/09/2025 5 - Practice on AWS:\n+ Lab 02-02: Session Manager (preparation, connect to EC2, manage session logs, port forwarding)\n+ Lab 02-03: VPC Peering (prepare, update Network ACL, create peering connection, configure route tables, enable cross-peer DNS)\n+ Lab 02-04: Transit Gateway (setup infrastructure, create TGW, TGW attachments, create TGW route table, add gateways, verify result)\n+ Lab 02-05: Hybrid DNS (setup, create outbound endpoint, create Route 53 resolver rule, create inbound endpoint) 12/09/2025 12/09/2025 Week 1 Achievements: Gained solid knowledge of AWS and Cloud Computing\nUnderstood cloud computing concepts, benefits, and the cloud adoption journey. Learned AWS global infrastructure (Regions, AZs, Edge Locations, Data Centers). Hands-on with AWS management tools\nPracticed with AWS Management Console, AWS CLI, and AWS SDK. Created and managed IAM Users/Groups, enabled MFA, and configured Access Keys. Cost management \u0026amp; support\nCreated and configured different AWS Budgets. Learned AWS Support Plans and practiced opening support cases. AWS Networking \u0026amp; Security\nBuilt and configured VPC, Subnet, Route Table, Internet Gateway, NAT Gateway. Implemented Security Groups, Network ACLs, and VPC Flow Logs. Understood VPC Peering vs. Transit Gateway, and hybrid connectivity (VPN, Direct Connect). System management on AWS\nManaged EC2 instances with Session Manager instead of SSH. Practiced Port Forwarding and Session Logs. Enabled CloudWatch Monitoring \u0026amp; Alerts for EC2. Advanced solutions\nSet up VPC Peering and Transit Gateway for multi-VPC connectivity. Configured Hybrid DNS with Route 53 Resolver (Inbound/Outbound Endpoints). Practiced Elastic Load Balancer (ALB, NLB, CLB, GWLB). 👉 Outcome: After Week 1, I have built a strong foundation in AWS fundamentals and intermediate networking concepts, while successfully completing multiple hands-on labs directly on AWS to strengthen my practical skills.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn and practice AWS Compute services (EC2, AMI, EBS, Auto Scaling, ELB). Gain skills in monitoring and backup using CloudWatch and AWS Backup. Explore AWS Storage services: S3, Storage Gateway, FSx. Hands-on with deployment, scaling, and static website hosting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Theory (Module 03 – Compute VM on AWS)\n- Studied Amazon EC2: concept, architecture, instance types, Nitro hypervisor.\n- Learned about related components: AMI, Backup, Snapshot, Key Pair.\n- Understood Elastic Block Store (EBS) and Instance Store usage.\n- Explored User Data \u0026amp; Meta Data for EC2 automation.\n- Learned EC2 Auto Scaling and pricing models (On-demand, Reserved, Saving Plans, Spot).\n- Got an overview of Amazon Lightsail, Amazon EFS, Amazon FSx, and AWS Application Migration Service (MGN).\nPractice (Lab 04 – Amazon EC2)\n- Launched and explored EC2 basic features.\n- Configured Linux VPC and Windows VPC with Security Groups.\n- Connected to EC2:\n- Windows via RDP (3389) using key pair.\n- Linux via SSH using MobaXterm and PuTTY.\n- Performed key operations:\n- Change instance type.\n- Create EBS snapshot.\n- Create Custom AMI and launch instance from AMI.\n- Access EC2 without Key Pair using AWS Systems Manager (SSM).\n- Deployed a Node.js CRUD User Management application on both Amazon Linux and Windows EC2 instances. 14/09/2025 15/09/2025 2 Lab 06: EC2 Auto Scaling and Elastic Load Balancing\n- Implemented application deployment using Amazon EC2 Auto Scaling Group for scalability.\n- Deployed Elastic Load Balancing to distribute traffic across application tier.\n- Prepared network infrastructure, launched EC2 instances, and configured RDS database.\n- Created AMI and launch template for consistent instance deployment.\n- Configured Auto Scaling Group integrated with Load Balancer to ensure high availability and cost efficiency.\nLab 08: Amazon CloudWatch\n- Explored CloudWatch for monitoring AWS, hybrid, and on-premise environments.\n- Collected and analyzed logs and metrics for application and infrastructure performance.\n- Configured CloudFormation stack to set up CloudWatch environment.\n- Worked with CloudWatch Metrics: graphs, calculations, labeling data.\n- Practiced CloudWatch Logs: centralized log collection, retention policies, anomaly detection.\n- Set up CloudWatch Alarms to monitor metrics and trigger actions.\n- Built CloudWatch Dashboards for real-time visualization of system health and performance.\nLab 13: AWS Backup\n- Learned AWS Backup to create automated backup plans for EBS, RDS, DynamoDB, and EFS.\n- Deployed infrastructure with CloudFormation and created S3 bucket for storage.\n- Designed Backup Plans considering RTO (Recovery Time Objective) and RPO (Recovery Point Objective).\n- Configured automated notifications using AWS SNS for backup status, recovery operations, and errors. 16/09/2025 1609/2025 3 Theory: AWS Storage Services\n- Learned Amazon Simple Storage Service (S3) as an object storage service:\n- Objects are immutable, updates require re-upload (override).\n- Best for WORM (Write Once Read Many) data.\n- Unlimited total storage, single object ≤ 5TB.\n- Data durability: 99.999999999%, availability: 99.99%.\n- Replication across 3 AZs within a Region by default.\n- Supports multipart upload for large objects.\n- Trigger-based events (e.g., resize image after upload).\n- Explored S3 access methods:\n- REST API (HTTP PUT, GET).\n- Object URLs: https://bucket.s3.amazonaws.com/key\n- Access Point: unique hostname for access control per app/user.\n- Storage Classes:\n- S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 One Zone-IA.\n- Amazon Glacier \u0026amp; Deep Archive for long-term, rarely accessed data.\n- Lifecycle policies to transition data between storage classes.\n- S3 Static Website \u0026amp; CORS:\n- Hosting static sites on S3.\n- Cross-origin resource sharing (CORS) support.\n- Access Control:\n- S3 Access Control List (ACL).\n- S3 Bucket Policy for centralized permissions.\n- Endpoint \u0026amp; Versioning:\n- Access via AWS private network (VPC endpoint).\n- Versioning for recovery from accidental deletion/overwrite.\n- Object Key \u0026amp; Performance:\n- Partitioning for performance.\n- Use random prefixes to optimize performance at scale.\n- Amazon Glacier:\n- Low-cost archival storage with retrieval options:\n- Expedited (1–5 min), Standard (3–5 hrs), Bulk (5–12 hrs).\n- Access only via CLI/SDK.\n- 20x cheaper than S3 Standard.\n- Object Lock feature to enforce compliance retention. 17/09/2025 17/09/2025 4 Lab 24: AWS Storage Gateway (File Gateway)\n- Prepared infrastructure:\n- Created S3 Bucket for backend storage.\n- Launched EC2 instance to act as the Storage Gateway.\n- Configured AWS Storage Gateway:\n- Created Storage Gateway and set up a File Share.\n- Mounted the File Share on the local machine to enable file sharing.\n- Notes:\n- In production, Storage Gateway Appliance is deployed on-premises.\n- Supports VMware, Hyper-V, KVM, or physical appliances. 18/09/2025 18/09/2025 5 Lab 25: Amazon FSx for Windows File Server\n- Studied Amazon FSx architecture:\n- File Servers (Windows File Server instances accessed via SMB).\n- Storage (backed by Amazon S3).\n- Networking (Elastic Network Interfaces within VPC).\n- Data Replication (automatic replication across multiple AZs).\n- Managed and monitored entirely by AWS.\n- Deployed FSx for Windows File Server to provide fully managed, shared file storage.\n- Learned integration with Windows Server, management, and data access features.\nLab 57: Amazon S3 \u0026amp; Static Website Hosting\n- Reviewed Amazon S3 concepts:\n- Buckets (unique, region-specific containers).\n- Objects (files stored in buckets, up to 5TB each).\n- Prepared infrastructure:\n- Created S3 bucket with ACL and public access settings.\n- Uploaded files and organized bucket objects.\n- Configured S3 Static Website Hosting:\n- Enabled static hosting and defined index/error documents.\n- Tested DNS endpoint generated by S3.\n- Managed access and security:\n- Adjusted Block Public Access settings.\n- Applied ACLs for object-level public access.\n- Accelerated website using CloudFront:\n- Created distribution linked to S3 bucket.\n- Configured Origin Access Identity (OAI) and tested CloudFront endpoint.\n- Additional features:\n- Enabled Bucket Versioning for data recovery.\n- Practiced moving objects between folders and buckets. 19/09/2025 19/09/2025 Week 2 Achievements: Compute \u0026amp; Deployment\nGained strong understanding of Amazon EC2 concepts, architecture, and related components (AMI, Snapshot, Key Pair, EBS). Successfully launched and managed both Linux and Windows EC2 instances. Practiced deployment of a real-world Node.js CRUD application on EC2 instances. Learned automation techniques using User Data, Meta Data, and AWS Systems Manager. Scalability \u0026amp; High Availability\nImplemented EC2 Auto Scaling Group to automatically adjust capacity based on demand. Configured Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances. Integrated Auto Scaling with Load Balancing for high availability and cost optimization. Monitoring \u0026amp; Observability\nExplored Amazon CloudWatch for infrastructure and application monitoring. Created custom metrics, dashboards, and alarms for real-time system health tracking. Centralized log management using CloudWatch Logs with retention and anomaly detection. Practiced automated monitoring using CloudFormation stacks. Data Protection \u0026amp; Backup\nLearned to design AWS Backup Plans for multiple services (EBS, RDS, DynamoDB, EFS). Implemented recovery objectives (RTO/RPO) for disaster recovery strategies. Configured SNS notifications for backup success, failure, and recovery events. Storage \u0026amp; Data Management\nUnderstood Amazon S3 as an object storage service, with lifecycle policies and storage classes. Practiced S3 features: versioning, access control (ACL, Bucket Policy), and CORS. Configured S3 Static Website Hosting and tested public accessibility. Implemented CloudFront distribution for faster content delivery and secured access via OAI. Learned Amazon FSx for Windows File Server: architecture, integration with Windows, and managed file storage service. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand AWS security philosophy (“Security is Job Zero”) and the Shared Responsibility Model. Gain proficiency with Identity \u0026amp; Access Management: IAM Users, Groups, Roles, Policies, Permission Boundaries, Organizations, Identity Center, Cognito. Apply AWS KMS for data encryption, integrate with CloudTrail, and analyze logs using Athena. Learn AWS Database Services: RDS, Aurora, Redshift, ElastiCache and core concepts of SQL, NoSQL, OLTP, OLAP. Deploy Amazon RDS with Multi-AZ, subnet groups, backup and restore for high availability and resilience. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Module 05: Security Services on AWS\n- Studied AWS Security Philosophy\n- Understood the principle “Security is Job Zero” — security as the highest priority in AWS services.\n- Shared Responsibility Model\n- AWS secures infrastructure, storage, compute, and managed services.\n- Customers are responsible for configuration, encryption, authentication, OS, network, and data-level security.\n- Recognized differences in responsibility levels depending on service type (infrastructure, managed, fully managed).\n- Identity and Access Management (IAM)\n- Learned about Root Account risks and best practices (restrict usage, MFA, split credential storage).\n- Understood IAM concepts: Users, Groups, Roles, Policies.\n- Differentiated Identity-based vs Resource-based policies.\n- Explored IAM Role with temporary credentials via AWS STS (assume-role).\n- Amazon Cognito\n- User Pool: authentication and login for apps.\n- Identity Pool: provides AWS resource access after authentication.\n- Integration of authentication with social providers (Google, Facebook, etc.).\n- AWS Organizations\n- Centralized multi-account management.\n- Created Organizational Units (OU), consolidated billing, and Service Control Policies (SCPs).\n- AWS Identity Center\n- Centralized access management across AWS accounts and external applications.\n- Steps: Group users → Create Permission Sets → Assign permissions.\n- AWS Key Management Service (KMS)\n- Service for creating and managing encryption keys.\n- Customer Managed Keys (CMKs) used to generate Data Keys for encryption/decryption outside KMS.\n- AWS Security Hub\n- Continuous security check service aligned with AWS best practices and compliance standards.\n- Provides security score and identifies misconfigured resources/accounts. 22/09/2025 22/09/2025 2 Lab 02: AWS IAM Basics\n- Learned IAM fundamentals: Users, Groups, Roles, and Policies.\n- Explored best practices with least privilege principle.\n- Understood IAM authentication methods: passwords, access keys, and temporary credentials via AWS STS.\n- Practiced creating and managing IAM Users and Groups.\nLab 44: IAM Roles \u0026amp; Conditions\n- Reviewed IAM request flow: principal, action, resource, environment data.\n- Practiced authentication and role assumption process using sts:AssumeRole.\n- Created IAM Group with policies: AmazonEC2FullAccess, AmazonRDSFullAccess, DatabaseAdministrator.\n- Created multiple IAM Users: EC2-admin-user, RDS-admin-user, Group-user, and No-permission-user.\n- Configured IAM Role with Conditions:\n- Restricted role access by IP address.\n- Applied time-based conditions for additional security.\nLab 48: IAM Role for Applications\n- Compared using Access Keys vs IAM Role on EC2:\n- Highlighted security risks of hardcoding access keys in applications.\n- Showed how IAM Roles eliminate risks by assigning temporary permissions automatically.\n- Deployed an EC2 instance with IAM Role attached, enabling secure access to S3 bucket without managing access keys. 23/09/2025 23/09/2025 3 Lab 18: Identity Federation with Amazon Cognito\n- Learned about identity federation with Amazon Cognito.\n- Configured User Pool and Identity Pool to support authentication from multiple sources (Google, Facebook, SAML).\n- Practiced creating a client application, integrating, and authenticating users.\nLab 27: IAM Roles\n- Understood the concept of IAM Role and its difference from IAM User.\n- Assigned Role to EC2 instances to access AWS resources without hardcoding credentials.\n- Implemented least privilege principle with IAM Roles.\nLab 28: IAM Group and Policy\n- Created IAM Group and added multiple Users into the group.\n- Attached policy to Group for centralized permission management.\n- Practiced writing and applying IAM Policy in JSON.\nLab 30: IAM Permission Boundary\n- Learned about Permission Boundary to limit the maximum permissions for User/Group.\n- Created a policy that restricts full EC2 access to a single region.\n- Assigned both identity-based policy and permission boundary to User, observed combined permission effect.\nLab 33: Encryption at Rest with AWS KMS\n- Studied AWS Key Management Service (KMS): symmetric \u0026amp; asymmetric key usage.\n- Created CMK (Customer Master Key), set up key policy, and integrated with CloudTrail for monitoring.\n- Practiced encrypting data in Amazon S3 using KMS.\n- Used CloudTrail to log all key-related activities.\n- Queried logs with Amazon Athena for access analysis and compliance verification. 24/09/2025 24/09/2025 4 Theory: AWS Database Services\n- Reviewed fundamental database concepts:\n- Primary Key, Foreign Key, Indexing, Partitions, Execution Plan, Logs, Buffers.\n- Differentiated RDBMS (SQL) vs NoSQL databases.\n- Understood OLTP (transactional processing) vs OLAP (analytical processing).\n- Amazon RDS (Relational Database Service):\n- Learned managed database features: automated backups, read replicas, Multi-AZ failover, encryption, auto scaling.\n- Understood that RDS instances are based on EC2 but fully managed by AWS.\n- Amazon Aurora:\n- High-performance, AWS-optimized RDBMS compatible with MySQL and PostgreSQL.\n- Features: Backtrack, Cloning, Global Database, Multi-Master architecture.\n- Amazon Redshift:\n- Data warehouse solution optimized for OLAP and big data analysis.\n- Uses columnar storage, MPP architecture, leader/compute nodes.\n- Cost optimizations: Transient Clusters, Redshift Spectrum (query data in S3).\n- Amazon ElastiCache:\n- Managed caching service (Redis, Memcached).\n- Enhances application performance by offloading frequent queries from databases.\n- Requires application-level caching logic. 25/09/2025 25/09/2025 5 Lab 05: Amazon Relational Database Service (RDS)\n- Studied Amazon RDS as a managed relational database service:\n- Supports OLTP workloads, structured and relational data.\n- Benefits: automated backup, patching, scalability, replication, high availability.\n- Learned supported DB engines: Aurora, MySQL, MariaDB, Oracle, SQL Server, PostgreSQL.\n- Prepared infrastructure:\n- Created VPC with subnets for Multi-AZ deployment.\n- Configured EC2 Security Group for application server.\n- Configured RDS Security Group (separated from EC2 for better security).\n- Created DB Subnet Group with private subnets for RDS instances.\n- Deployed EC2 instance to connect with RDS.\n- Created RDS Database Instance with both Easy Create and custom options.\n- Implemented application deployment using RDS as backend database.\n- Practiced backup and restore:\n- Automatic backups and manual snapshots.\n- Point-in-time recovery for data protection. 26/09/2025 26/09/2025 Week 3 Achievements: Security Services:\nApplied Shared Responsibility Model in practice. Managed IAM with Users, Groups, Roles, Policies, Conditions, and Permission Boundaries. Used IAM Role with EC2 to remove security risks of hardcoded Access Keys. Implemented Identity Federation with Cognito (Google, Facebook, SAML). Centralized multi-account management with AWS Organizations and Identity Center. Encryption \u0026amp; Monitoring:\nCreated and managed KMS CMKs for encryption. Encrypted S3 data with KMS. Logged and monitored key activities with CloudTrail, analyzed with Athena. Database Services:\nLearned differences between RDBMS vs NoSQL, OLTP vs OLAP. Deployed Amazon RDS with Multi-AZ, subnet groups, automated backups, snapshots, and point-in-time recovery. Studied Aurora (Backtrack, Global DB, Cloning), Redshift (MPP, Spectrum), and ElastiCache (Redis, Memcached). Hands-on Practice:\nCompleted labs: IAM Basics, IAM Roles \u0026amp; Conditions, IAM Roles for Applications, Cognito Federation, IAM Permission Boundaries, KMS Encryption, RDS Deployment. Built a secure RDS environment with backup and recovery strategies. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: The goal of this week was to strengthen understanding of AWS data and AI services — from building scalable DataLake architectures to working with serverless NoSQL databases (DynamoDB) and exploring AI development lifecycle management. Additionally, this week aimed to enhance technical translation and communication skills through hands-on labs, blog translations, and participation in AWS events. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Lab 35: DataLake on AWS\n- Introduction to DataLake:\n- Understood DataLake as a centralized repository that stores raw and processed data for analytics and insights.\n- Learned its key characteristics: collect everything, multi-user accessibility, and flexible access patterns (batch, interactive, real-time, search).\n- Gained knowledge on how AWS services (Glue, Athena, QuickSight) integrate to build a complete DataLake solution.\n- Amazon Glue (ETL Service):\n- Learned about Glue Crawlers for automatic schema discovery and Data Catalog creation.\n- Understood how Glue ETL jobs generate customizable Python scripts for data transformation and loading.\n- Practiced creating a Crawler to scan S3 data and build a Data Catalog for query access.\n- Amazon Athena (Interactive Query Service):\n- Learned Athena enables querying S3 data using standard SQL without complex ETL pipelines.\n- Configured Athena to query S3 datasets and understood its cost model (pay per query).\n- Explored data formats supported by Athena (CSV, JSON, ORC, Avro, Parquet).\n- Amazon QuickSight (Data Visualization Service):\n- Studied how QuickSight connects to data sources and builds datasets for visualization.\n- Created visualizations, analyses, and dashboards to represent key business metrics.\n- Learned QuickSight’s structure: Datasource → Dataset → Analysis → Visual → Dashboard.\n- Implementation Steps:\n- Created IAM Role with S3FullAccess and AWSGlueServiceRole permissions.\n- Prepared S3 bucket structure (/data, /ref_data) and uploaded files.\n- Set up Kinesis Delivery Stream to continuously deliver data to S3.\n- Deployed CloudFormation stack for infrastructure automation.\n- Ran AWS Glue Crawler to catalog S3 data and verified it with Amazon Athena.\n- Launched SageMaker Notebook via Glue Studio for additional data transformation workflows. 29/09/2025 29/09/2025 2 Lab40:\n- Reviewed AWS Glue as a managed ETL (Extract – Transform – Load) service supporting data preparation for analytics. - Understood the workflow: 1. Upload raw data to Amazon S3. 2. Use Glue Crawler to detect schema and create a database in the Glue Data Catalog. 3. Transform data into optimized formats like Parquet. 4. Query transformed data using Amazon Athena. - Configured AWS Glue and Athena for automatic schema updates and scheduled crawlers. - Executed SQL queries in Athena for analyzing: - Top 10 costliest AWS accounts and services. - Detailed cost breakdowns by service and tag (e.g., cost_center). - Distinct billing periods and usage types. - Learned to minimize Athena query costs by: - Using compressed Parquet files. - Limiting query results with LIMIT. - Structuring data efficiently for partitioned querying. - Explored cost allocation and tagging for enterprise-level expense tracking and optimization. 30/09/2025 30/09/2025 3 Lab 60: Amazon DynamoDB\n- Overview:\n- Studied Amazon DynamoDB — a fully managed NoSQL database service that offers fast, predictable performance and seamless scalability.\n- Understood DynamoDB’s ability to automatically manage infrastructure, scaling, and data replication across multiple Availability Zones.\n- Explored features like on-demand and provisioned capacity modes, encryption at rest, point-in-time recovery, and automatic item expiration.\n- Core Components:\n- Table: Logical collection of data items (similar to a table in relational DBs).\n- Item: Individual records inside a table (like rows).\n- Attribute: Properties within items (like columns).\n- Primary Keys:\n- Learned about Partition Key and Composite Primary Key (Partition + Sort Key) for uniquely identifying items.\n- Understood how composite keys enhance query flexibility.\n- Secondary Indexes:\n- Practiced using Global Secondary Index (GSI) and Local Secondary Index (LSI) to optimize query performance.\n- Recognized DynamoDB supports up to 20 GSIs and 5 LSIs per table.\n- Read Consistency:\n- Compared Eventually Consistent Reads (faster, may show stale data) vs Strongly Consistent Reads (latest data, more costly and slower).\n- Read/Write Capacity Modes:\n- On-Demand Mode: Ideal for unpredictable workloads, automatically scales based on demand.\n- Provisioned Mode: Suitable for predictable workloads with steady or forecasted traffic.\n- Implementation:\n- Explored AWS Management Console and AWS CloudShell for DynamoDB setup and management.\n- Learned how to use AWS SDK (Boto3) for programmatically interacting with DynamoDB tables. 01/10/2025 01/10/2025 4 Activity: Translated AWS Technical Blogs\nTranslated Content:\n- “Accelerating Generative AI Development with Fully Managed MLflow 3.0 on Amazon SageMaker AI” – Learned how AWS integrates MLflow to manage the lifecycle of Generative AI models.\n- “AI-Enhanced Subsurface Infrastructure Mapping on AWS” – Understood how deep learning and HPC are applied to detect underground infrastructure.\n- “Unlocking the Full Potential of Amazon Connect” – Gained insights into best practices for deploying AI-powered contact centers on AWS.\nSkills Gained:\n- Improved technical translation skills (AI, HPC, Cloud Computing).\n- Gained deeper understanding of AWS services: SageMaker, Batch, ParallelCluster, and Amazon Connect.\n- Expanded technical vocabulary and English expression in the cloud and AI domain. 2/10/2025 2/10/2025 5 Activity: Attended AWS Event on AI Development Lifecycle and Introduction to Kiro\nDetails:\n- Participated in an AWS-hosted event focused on the AI Development Lifecycle, covering all major stages from data preparation, model training, and evaluation to deployment and continuous monitoring.\n- Learned about how AWS services such as Amazon SageMaker, Bedrock, and CodeWhisperer support the end-to-end development and optimization of AI models.\n- Attended a detailed presentation on Kiro – a new AWS solution introduced to simplify and unify AI workflow management across teams.\n- Gained insights into how Kiro integrates with other AWS tools to manage datasets, model versions, and experiment tracking more efficiently, enhancing collaboration and governance.\n- Explored real-world case studies demonstrating how organizations leverage AWS infrastructure to accelerate AI-driven automation, reduce training time, and improve model reliability in production environments.\nSkills Gained:\n- Deepened understanding of the complete AI development lifecycle within the AWS ecosystem.\n- Learned practical applications of AI lifecycle management using AWS services and Kiro.\n- Improved knowledge of model observability, experiment tracking, and deployment best practices.\n- Strengthened communication and technical vocabulary related to AI, ML, and Cloud Computing. 3/10/2025 3/10/2025 Week 4 Achievements: Built an AWS DataLake pipeline integrating Glue, Athena, and QuickSight, gaining hands-on experience in data ingestion, transformation, and visualization. Configured AWS Glue Crawlers and Athena queries for cost analysis and schema automation, applying cost optimization strategies (e.g., Parquet, partitioning, query limits). Mastered DynamoDB fundamentals, including primary/composite keys, indexes (GSI, LSI), consistency models, and capacity modes. Enhanced translation and comprehension skills by translating AWS technical blogs on AI, MLflow, and HPC, deepening understanding of SageMaker, Batch, ParallelCluster, and Connect. Participated in an AWS event focused on the AI Development Lifecycle and Kiro, gaining insights into end-to-end model management, versioning, and deployment best practices. Improved technical vocabulary and practical knowledge in cloud computing, AI model governance, and data-driven architecture design within the AWS ecosystem. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Strengthen practical knowledge and implementation skills with AWS Data Analytics services including Glue, Athena, QuickSight, S3 DataLake, and streaming data ingestion using Kinesis. Gain a deep understanding of DynamoDB’s architecture, performance models, indexing, and highly available NoSQL data storage strategies. Enhance professional English communication through translation of AWS Technical Blogs on AI, HPC, and cloud-based enterprise solutions. Stay up-to-date with the latest AWS innovations by attending an industry event focused on AI Development Lifecycle and workflow management with Kiro. Improve the ability to design, query, transform, and visualize data efficiently across AWS analytics ecosystems. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Lab 35: DataLake on AWS\n- Introduction to DataLake:\n- Understood DataLake as a centralized repository that stores raw and processed data for analytics and insights.\n- Learned its key characteristics: collect everything, multi-user accessibility, and flexible access patterns (batch, interactive, real-time, search).\n- Gained knowledge on how AWS services (Glue, Athena, QuickSight) integrate to build a complete DataLake solution.\n- Amazon Glue (ETL Service):\n- Learned about Glue Crawlers for automatic schema discovery and Data Catalog creation.\n- Understood how Glue ETL jobs generate customizable Python scripts for data transformation and loading.\n- Practiced creating a Crawler to scan S3 data and build a Data Catalog for query access.\n- Amazon Athena (Interactive Query Service):\n- Learned Athena enables querying S3 data using standard SQL without complex ETL pipelines.\n- Configured Athena to query S3 datasets and understood its cost model (pay per query).\n- Explored data formats supported by Athena (CSV, JSON, ORC, Avro, Parquet).\n- Amazon QuickSight (Data Visualization Service):\n- Studied how QuickSight connects to data sources and builds datasets for visualization.\n- Created visualizations, analyses, and dashboards to represent key business metrics.\n- Learned QuickSight’s structure: Datasource → Dataset → Analysis → Visual → Dashboard.\n- Implementation Steps:\n- Created IAM Role with S3FullAccess and AWSGlueServiceRole permissions.\n- Prepared S3 bucket structure (/data, /ref_data) and uploaded files.\n- Set up Kinesis Delivery Stream to continuously deliver data to S3.\n- Deployed CloudFormation stack for infrastructure automation.\n- Ran AWS Glue Crawler to catalog S3 data and verified it with Amazon Athena.\n- Launched SageMaker Notebook via Glue Studio for additional data transformation workflows. 29/09/2025 29/09/2025 2 Lab40:\n- Reviewed AWS Glue as a managed ETL (Extract – Transform – Load) service supporting data preparation for analytics. - Understood the workflow: 1. Upload raw data to Amazon S3. 2. Use Glue Crawler to detect schema and create a database in the Glue Data Catalog. 3. Transform data into optimized formats like Parquet. 4. Query transformed data using Amazon Athena. - Configured AWS Glue and Athena for automatic schema updates and scheduled crawlers. - Executed SQL queries in Athena for analyzing: - Top 10 costliest AWS accounts and services. - Detailed cost breakdowns by service and tag (e.g., cost_center). - Distinct billing periods and usage types. - Learned to minimize Athena query costs by: - Using compressed Parquet files. - Limiting query results with LIMIT. - Structuring data efficiently for partitioned querying. - Explored cost allocation and tagging for enterprise-level expense tracking and optimization. 30/09/2025 30/09/2025 3 Lab 60: Amazon DynamoDB\n- Overview:\n- Studied Amazon DynamoDB — a fully managed NoSQL database service that offers fast, predictable performance and seamless scalability.\n- Understood DynamoDB’s ability to automatically manage infrastructure, scaling, and data replication across multiple Availability Zones.\n- Explored features like on-demand and provisioned capacity modes, encryption at rest, point-in-time recovery, and automatic item expiration.\n- Core Components:\n- Table: Logical collection of data items (similar to a table in relational DBs).\n- Item: Individual records inside a table (like rows).\n- Attribute: Properties within items (like columns).\n- Primary Keys:\n- Learned about Partition Key and Composite Primary Key (Partition + Sort Key) for uniquely identifying items.\n- Understood how composite keys enhance query flexibility.\n- Secondary Indexes:\n- Practiced using Global Secondary Index (GSI) and Local Secondary Index (LSI) to optimize query performance.\n- Recognized DynamoDB supports up to 20 GSIs and 5 LSIs per table.\n- Read Consistency:\n- Compared Eventually Consistent Reads (faster, may show stale data) vs Strongly Consistent Reads (latest data, more costly and slower).\n- Read/Write Capacity Modes:\n- On-Demand Mode: Ideal for unpredictable workloads, automatically scales based on demand.\n- Provisioned Mode: Suitable for predictable workloads with steady or forecasted traffic.\n- Implementation:\n- Explored AWS Management Console and AWS CloudShell for DynamoDB setup and management.\n- Learned how to use AWS SDK (Boto3) for programmatically interacting with DynamoDB tables. 01/10/2025 01/10/2025 4 Activity: Translated AWS Technical Blogs\nTranslated Content:\n- “Accelerating Generative AI Development with Fully Managed MLflow 3.0 on Amazon SageMaker AI” – Learned how AWS integrates MLflow to manage the lifecycle of Generative AI models.\n- “AI-Enhanced Subsurface Infrastructure Mapping on AWS” – Understood how deep learning and HPC are applied to detect underground infrastructure.\n- “Unlocking the Full Potential of Amazon Connect” – Gained insights into best practices for deploying AI-powered contact centers on AWS.\nSkills Gained:\n- Improved technical translation skills (AI, HPC, Cloud Computing).\n- Gained deeper understanding of AWS services: SageMaker, Batch, ParallelCluster, and Amazon Connect.\n- Expanded technical vocabulary and English expression in the cloud and AI domain. 2/10/2025 2/10/2025 5 Activity: Attended AWS Event on AI Development Lifecycle and Introduction to Kiro\nDetails:\n- Participated in an AWS-hosted event focused on the AI Development Lifecycle, covering all major stages from data preparation, model training, and evaluation to deployment and continuous monitoring.\n- Learned about how AWS services such as Amazon SageMaker, Bedrock, and CodeWhisperer support the end-to-end development and optimization of AI models.\n- Attended a detailed presentation on Kiro – a new AWS solution introduced to simplify and unify AI workflow management across teams.\n- Gained insights into how Kiro integrates with other AWS tools to manage datasets, model versions, and experiment tracking more efficiently, enhancing collaboration and governance.\n- Explored real-world case studies demonstrating how organizations leverage AWS infrastructure to accelerate AI-driven automation, reduce training time, and improve model reliability in production environments.\nSkills Gained:\n- Deepened understanding of the complete AI development lifecycle within the AWS ecosystem.\n- Learned practical applications of AI lifecycle management using AWS services and Kiro.\n- Improved knowledge of model observability, experiment tracking, and deployment best practices.\n- Strengthened communication and technical vocabulary related to AI, ML, and Cloud Computing. 3/10/2025 3/10/2025 Week 5 Achievements: Successfully completed Lab 35: DataLake on AWS, integrating Glue ETL workflows, Athena SQL queries, and QuickSight dashboards for a full data analytics pipeline. Completed Lab 40 with advanced cost analytics using Glue Data Catalog and Athena, while optimizing query costs through Parquet formats and partitioning techniques. Built strong hands-on knowledge of Amazon DynamoDB including primary key design, capacity modes, GSI and LSI indexing, consistency settings, and SDK automation. Translated multiple AWS technical publications, improving professional terminology in AI, HPC, and Cloud-native services, while expanding real-world service understanding. Participated in an official AWS event, gaining insights into how SageMaker, Bedrock, CodeWhisperer, and Kiro enhance enterprise AI development efficiency and governance. Significantly improved skills across cloud data engineering, serverless analytics, NoSQL performance design, and AI operational best practices on AWS. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Strengthen practical skills in key AWS data and analytics services including Glue, Athena, QuickSight, and S3-based DataLake architecture. Build theoretical and hands-on knowledge of Amazon DynamoDB for scalable NoSQL workloads. Enhance cloud technical communication skills through translation of AWS blogs on Generative AI, HPC, and contact center solutions. Gain updated industry knowledge by attending an AWS event focused on AI development lifecycle and new tools such as Kiro. Improve the ability to design and implement analytics solutions that enable data ingestion, transformation, querying, and visualization on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Lab 35: DataLake on AWS\n- Introduction to DataLake:\n- Understood DataLake as a centralized repository that stores raw and processed data for analytics and insights.\n- Learned its key characteristics: collect everything, multi-user accessibility, and flexible access patterns (batch, interactive, real-time, search).\n- Gained knowledge on how AWS services (Glue, Athena, QuickSight) integrate to build a complete DataLake solution.\n- Amazon Glue (ETL Service):\n- Learned about Glue Crawlers for automatic schema discovery and Data Catalog creation.\n- Understood how Glue ETL jobs generate customizable Python scripts for data transformation and loading.\n- Practiced creating a Crawler to scan S3 data and build a Data Catalog for query access.\n- Amazon Athena (Interactive Query Service):\n- Learned Athena enables querying S3 data using standard SQL without complex ETL pipelines.\n- Configured Athena to query S3 datasets and understood its cost model (pay per query).\n- Explored data formats supported by Athena (CSV, JSON, ORC, Avro, Parquet).\n- Amazon QuickSight (Data Visualization Service):\n- Studied how QuickSight connects to data sources and builds datasets for visualization.\n- Created visualizations, analyses, and dashboards to represent key business metrics.\n- Learned QuickSight’s structure: Datasource → Dataset → Analysis → Visual → Dashboard.\n- Implementation Steps:\n- Created IAM Role with S3FullAccess and AWSGlueServiceRole permissions.\n- Prepared S3 bucket structure (/data, /ref_data) and uploaded files.\n- Set up Kinesis Delivery Stream to continuously deliver data to S3.\n- Deployed CloudFormation stack for infrastructure automation.\n- Ran AWS Glue Crawler to catalog S3 data and verified it with Amazon Athena.\n- Launched SageMaker Notebook via Glue Studio for additional data transformation workflows. 29/09/2025 29/09/2025 2 Lab40:\n- Reviewed AWS Glue as a managed ETL (Extract – Transform – Load) service supporting data preparation for analytics. - Understood the workflow: 1. Upload raw data to Amazon S3. 2. Use Glue Crawler to detect schema and create a database in the Glue Data Catalog. 3. Transform data into optimized formats like Parquet. 4. Query transformed data using Amazon Athena. - Configured AWS Glue and Athena for automatic schema updates and scheduled crawlers. - Executed SQL queries in Athena for analyzing: - Top 10 costliest AWS accounts and services. - Detailed cost breakdowns by service and tag (e.g., cost_center). - Distinct billing periods and usage types. - Learned to minimize Athena query costs by: - Using compressed Parquet files. - Limiting query results with LIMIT. - Structuring data efficiently for partitioned querying. - Explored cost allocation and tagging for enterprise-level expense tracking and optimization. 30/09/2025 30/09/2025 3 Lab 60: Amazon DynamoDB\n- Overview:\n- Studied Amazon DynamoDB — a fully managed NoSQL database service that offers fast, predictable performance and seamless scalability.\n- Understood DynamoDB’s ability to automatically manage infrastructure, scaling, and data replication across multiple Availability Zones.\n- Explored features like on-demand and provisioned capacity modes, encryption at rest, point-in-time recovery, and automatic item expiration.\n- Core Components:\n- Table: Logical collection of data items (similar to a table in relational DBs).\n- Item: Individual records inside a table (like rows).\n- Attribute: Properties within items (like columns).\n- Primary Keys:\n- Learned about Partition Key and Composite Primary Key (Partition + Sort Key) for uniquely identifying items.\n- Understood how composite keys enhance query flexibility.\n- Secondary Indexes:\n- Practiced using Global Secondary Index (GSI) and Local Secondary Index (LSI) to optimize query performance.\n- Recognized DynamoDB supports up to 20 GSIs and 5 LSIs per table.\n- Read Consistency:\n- Compared Eventually Consistent Reads (faster, may show stale data) vs Strongly Consistent Reads (latest data, more costly and slower).\n- Read/Write Capacity Modes:\n- On-Demand Mode: Ideal for unpredictable workloads, automatically scales based on demand.\n- Provisioned Mode: Suitable for predictable workloads with steady or forecasted traffic.\n- Implementation:\n- Explored AWS Management Console and AWS CloudShell for DynamoDB setup and management.\n- Learned how to use AWS SDK (Boto3) for programmatically interacting with DynamoDB tables. 01/10/2025 01/10/2025 4 Activity: Translated AWS Technical Blogs\nTranslated Content:\n- “Accelerating Generative AI Development with Fully Managed MLflow 3.0 on Amazon SageMaker AI” – Learned how AWS integrates MLflow to manage the lifecycle of Generative AI models.\n- “AI-Enhanced Subsurface Infrastructure Mapping on AWS” – Understood how deep learning and HPC are applied to detect underground infrastructure.\n- “Unlocking the Full Potential of Amazon Connect” – Gained insights into best practices for deploying AI-powered contact centers on AWS.\nSkills Gained:\n- Improved technical translation skills (AI, HPC, Cloud Computing).\n- Gained deeper understanding of AWS services: SageMaker, Batch, ParallelCluster, and Amazon Connect.\n- Expanded technical vocabulary and English expression in the cloud and AI domain. 2/10/2025 2/10/2025 5 Activity: Attended AWS Event on AI Development Lifecycle and Introduction to Kiro\nDetails:\n- Participated in an AWS-hosted event focused on the AI Development Lifecycle, covering all major stages from data preparation, model training, and evaluation to deployment and continuous monitoring.\n- Learned about how AWS services such as Amazon SageMaker, Bedrock, and CodeWhisperer support the end-to-end development and optimization of AI models.\n- Attended a detailed presentation on Kiro – a new AWS solution introduced to simplify and unify AI workflow management across teams.\n- Gained insights into how Kiro integrates with other AWS tools to manage datasets, model versions, and experiment tracking more efficiently, enhancing collaboration and governance.\n- Explored real-world case studies demonstrating how organizations leverage AWS infrastructure to accelerate AI-driven automation, reduce training time, and improve model reliability in production environments.\nSkills Gained:\n- Deepened understanding of the complete AI development lifecycle within the AWS ecosystem.\n- Learned practical applications of AI lifecycle management using AWS services and Kiro.\n- Improved knowledge of model observability, experiment tracking, and deployment best practices.\n- Strengthened communication and technical vocabulary related to AI, ML, and Cloud Computing. 3/10/2025 3/10/2025 Week 6 Achievements: Successfully completed Lab 35 about DataLake on AWS, gaining hands-on experience with Glue ETL pipelines, Athena interactive queries, QuickSight BI visualization, and S3 data orchestration. Completed Lab 40 to analyze AWS cost data using Glue schema discovery and Athena SQL querying, while learning cost optimization with efficient storage formats like Parquet. Fully explored Amazon DynamoDB functions in Lab 60 including capacity modes, GSI/LSI indexing, read consistency, and automation using AWS SDK. Translated multiple AWS technical articles, improving cloud computing terminology, AI lifecycle comprehension, and technical writing skills. Attended an AWS event focused on end-to-end AI development workflow, gaining deeper understanding of SageMaker, Bedrock, and the newly introduced Kiro platform. Strengthened overall practical and theoretical knowledge across Data Engineering, NoSQL Databases, AI/ML solutions, and cloud architectural best practices. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Review and reinforce theoretical knowledge of core AWS services and architectural best practices. Practice AWS Cloud Practitioner exam questions daily to improve service recognition and scenario analysis. Strengthen understanding of AWS security, network design, storage strategies, compute options, and database availability models. Build the ability to confidently select correct AWS services based on business requirements, cost considerations, and operational expectations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Lab 35: DataLake on AWS\n- Introduction to DataLake:\n- Understood DataLake as a centralized repository that stores raw and processed data for analytics and insights.\n- Learned its key characteristics: collect everything, multi-user accessibility, and flexible access patterns (batch, interactive, real-time, search).\n- Gained knowledge on how AWS services (Glue, Athena, QuickSight) integrate to build a complete DataLake solution.\n- Amazon Glue (ETL Service):\n- Learned about Glue Crawlers for automatic schema discovery and Data Catalog creation.\n- Understood how Glue ETL jobs generate customizable Python scripts for data transformation and loading.\n- Practiced creating a Crawler to scan S3 data and build a Data Catalog for query access.\n- Amazon Athena (Interactive Query Service):\n- Learned Athena enables querying S3 data using standard SQL without complex ETL pipelines.\n- Configured Athena to query S3 datasets and understood its cost model (pay per query).\n- Explored data formats supported by Athena (CSV, JSON, ORC, Avro, Parquet).\n- Amazon QuickSight (Data Visualization Service):\n- Studied how QuickSight connects to data sources and builds datasets for visualization.\n- Created visualizations, analyses, and dashboards to represent key business metrics.\n- Learned QuickSight’s structure: Datasource → Dataset → Analysis → Visual → Dashboard.\n- Implementation Steps:\n- Created IAM Role with S3FullAccess and AWSGlueServiceRole permissions.\n- Prepared S3 bucket structure (/data, /ref_data) and uploaded files.\n- Set up Kinesis Delivery Stream to continuously deliver data to S3.\n- Deployed CloudFormation stack for infrastructure automation.\n- Ran AWS Glue Crawler to catalog S3 data and verified it with Amazon Athena.\n- Launched SageMaker Notebook via Glue Studio for additional data transformation workflows. 29/09/2025 29/09/2025 2 Lab40:\n- Reviewed AWS Glue as a managed ETL (Extract – Transform – Load) service supporting data preparation for analytics. - Understood the workflow: 1. Upload raw data to Amazon S3. 2. Use Glue Crawler to detect schema and create a database in the Glue Data Catalog. 3. Transform data into optimized formats like Parquet. 4. Query transformed data using Amazon Athena. - Configured AWS Glue and Athena for automatic schema updates and scheduled crawlers. - Executed SQL queries in Athena for analyzing: - Top 10 costliest AWS accounts and services. - Detailed cost breakdowns by service and tag (e.g., cost_center). - Distinct billing periods and usage types. - Learned to minimize Athena query costs by: - Using compressed Parquet files. - Limiting query results with LIMIT. - Structuring data efficiently for partitioned querying. - Explored cost allocation and tagging for enterprise-level expense tracking and optimization. 30/09/2025 30/09/2025 3 Lab 60: Amazon DynamoDB\n- Overview:\n- Studied Amazon DynamoDB — a fully managed NoSQL database service that offers fast, predictable performance and seamless scalability.\n- Understood DynamoDB’s ability to automatically manage infrastructure, scaling, and data replication across multiple Availability Zones.\n- Explored features like on-demand and provisioned capacity modes, encryption at rest, point-in-time recovery, and automatic item expiration.\n- Core Components:\n- Table: Logical collection of data items (similar to a table in relational DBs).\n- Item: Individual records inside a table (like rows).\n- Attribute: Properties within items (like columns).\n- Primary Keys:\n- Learned about Partition Key and Composite Primary Key (Partition + Sort Key) for uniquely identifying items.\n- Understood how composite keys enhance query flexibility.\n- Secondary Indexes:\n- Practiced using Global Secondary Index (GSI) and Local Secondary Index (LSI) to optimize query performance.\n- Recognized DynamoDB supports up to 20 GSIs and 5 LSIs per table.\n- Read Consistency:\n- Compared Eventually Consistent Reads (faster, may show stale data) vs Strongly Consistent Reads (latest data, more costly and slower).\n- Read/Write Capacity Modes:\n- On-Demand Mode: Ideal for unpredictable workloads, automatically scales based on demand.\n- Provisioned Mode: Suitable for predictable workloads with steady or forecasted traffic.\n- Implementation:\n- Explored AWS Management Console and AWS CloudShell for DynamoDB setup and management.\n- Learned how to use AWS SDK (Boto3) for programmatically interacting with DynamoDB tables. 01/10/2025 01/10/2025 4 Activity: Translated AWS Technical Blogs\nTranslated Content:\n- “Accelerating Generative AI Development with Fully Managed MLflow 3.0 on Amazon SageMaker AI” – Learned how AWS integrates MLflow to manage the lifecycle of Generative AI models.\n- “AI-Enhanced Subsurface Infrastructure Mapping on AWS” – Understood how deep learning and HPC are applied to detect underground infrastructure.\n- “Unlocking the Full Potential of Amazon Connect” – Gained insights into best practices for deploying AI-powered contact centers on AWS.\nSkills Gained:\n- Improved technical translation skills (AI, HPC, Cloud Computing).\n- Gained deeper understanding of AWS services: SageMaker, Batch, ParallelCluster, and Amazon Connect.\n- Expanded technical vocabulary and English expression in the cloud and AI domain. 2/10/2025 2/10/2025 5 Activity: Attended AWS Event on AI Development Lifecycle and Introduction to Kiro\nDetails:\n- Participated in an AWS-hosted event focused on the AI Development Lifecycle, covering all major stages from data preparation, model training, and evaluation to deployment and continuous monitoring.\n- Learned about how AWS services such as Amazon SageMaker, Bedrock, and CodeWhisperer support the end-to-end development and optimization of AI models.\n- Attended a detailed presentation on Kiro – a new AWS solution introduced to simplify and unify AI workflow management across teams.\n- Gained insights into how Kiro integrates with other AWS tools to manage datasets, model versions, and experiment tracking more efficiently, enhancing collaboration and governance.\n- Explored real-world case studies demonstrating how organizations leverage AWS infrastructure to accelerate AI-driven automation, reduce training time, and improve model reliability in production environments.\nSkills Gained:\n- Deepened understanding of the complete AI development lifecycle within the AWS ecosystem.\n- Learned practical applications of AI lifecycle management using AWS services and Kiro.\n- Improved knowledge of model observability, experiment tracking, and deployment best practices.\n- Strengthened communication and technical vocabulary related to AI, ML, and Cloud Computing. 3/10/2025 3/10/2025 Week 7 Achievements: Completed 2–3 AWS Cloud Practitioner practice exam sets every day with consistent improvement in performance. Successfully reviewed theory and real-business scenarios involving key AWS domains including Compute, Storage, Networking, Security, and Databases. Improved ability to differentiate AWS services with similar purposes such as: Elastic Load Balancing vs Route 53 vs Global Accelerator, S3 storage classes vs EBS vs EFS, Multi-AZ vs Read Replicas in RDS. Fully understood the theoretical foundation behind cost optimization options (On-Demand, Reserved Instances, Savings Plans, Spot Instances). Strengthened the theoretical knowledge of identity and access control: IAM users, roles, policies, MFA, least privilege principle. Enhanced comprehension of the AWS Well-Architected Framework theory and its six pillars for exam requirement questions. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: AWS Foundations, Networking, Security, and Hands-on Labs\nWeek 2: Hands-on with AWS Compute (EC2, Auto Scaling, Load Balancing), Monitoring (CloudWatch), Backup, and Storage Services (S3, FSx, Storage Gateway)\nWeek 3: AWS Security \u0026amp; Database Services (IAM, Cognito, KMS, RDS, Aurora, Redshift, ElastiCache)\nWeek 4: AWS Data Analytics, NoSQL, and AI Lifecycle Exploration Week\nWeek 5: AWS Data Engineering, NoSQL Databases, and AI Development Practices\nWeek 6: AWS Data Analytics, Database Services, AI Development Workflow \u0026amp; Technical Translation Practice\nWeek 7: Review basic AWS services and practice quizzes\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://danielleit241.github.io/aws-fcj-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Personal Finance Management App 1. Executive Summary The Personal Finance Management App project aims to provide an intelligent, modern, and highly automated personal financial management platform. The application allows users to record income and expenses, create and manage multiple money jars for different purposes, create spending plans, receive smart alerts, and generate visual analytical reports.\nThe application is built with a microservices architecture on .NET and FastAPI platform, deployed on AWS Cloud, ensuring flexibility, scalability, and data security. The development process follows the Agile/Scrum model (2 weeks/sprint), with MVP completion time within 2 months.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nSolution\nThe solution uses AWS Cloud combined with microservices architecture to build an automated personal financial management platform, integrating AI in voice processing and bill recognition. The system is deployed on AWS ECS Fargate for backend services (.NET), FastAPI for AI processing, and Next.js for frontend. Compared to popular financial platforms like Money Lover or Misa Money Keeper, this application focuses on complete automation of financial data entry through detailed Vietnamese AI voice-to-text and bill scanning, helping reduce manual operations and errors. The system is suitable for individual users and small groups, and can be expanded when needed for enterprise scale or digital banking applications.\nBenefits and Return on Investment (ROI)\nThe solution brings many practical benefits both technically and in business value:\nData Entry Automation: Reduces over 70% manual operations through AI voice and bill recognition. Increased Accuracy: Limits input errors, ensures financial data integrity (\u0026gt;90% accuracy). Improved User Performance: Record and categorize transactions in just seconds, optimizing user experience. Cost Savings: Low infrastructure costs thanks to AWS Free Tier utilization until 2026; only estimated ~$60 USD/month for AWS and ~$30 USD for AI compute. Fast ROI: Expected payback in 6–12 months, thanks to time savings in data entry and increased operational efficiency. Scalability \u0026amp; Integration: Microservices architecture on AWS allows easy addition of features (mobile app, advanced analytics, banking integration). 3. Solution Architecture The system is built on a microservices architecture hosted on AWS Cloud, combining serverless components, containerized services, and managed databases for scalability and performance.\nUsers access the Next.js web application through Amazon CloudFront, with static content hosted in Amazon S3 and routed via Amazon Route 53. The first layer of security is provided by AWS WAF, which protects the system from common web attacks such as SQL injection and XSS.\nWhen users log in, authentication is handled by Amazon Cognito, which issues access tokens used by the frontend to call APIs through Amazon API Gateway. The API Gateway routes requests to an Application Load Balancer (ALB) via AWS PrivateLink, and the ALB forwards them to Amazon ECS (Fargate) — where the backend containers are deployed, including:\nBackend Service: Handles the core business logic of the system. AI Service (FastAPI): Handles invoice processing, voice recognition, and other AI tasks. The AI Service can access files from Amazon S3 for data processing, then return the results to the Backend Service through internal APIs.\nContainer images are stored in Amazon ECR, and the deployment process is fully automated using GitLab CI/CD pipelines — including build, push to ECR, and ECS task definition updates.\nAll logs, metrics, and alerts from ECS, API Gateway, and ALB are collected in Amazon CloudWatch for centralized monitoring, while Amazon SNS is configured to send automatic notifications when incidents occur.\nAWS Services Used\nAmazon Route 53: Domain name and DNS management. AWS WAF: Protects the system from common web attacks. Amazon CloudFront: Global content delivery and frontend acceleration. Amazon S3: Stores static website content and user files (invoices, audio files). Amazon Cognito: User authentication and access management. Amazon API Gateway: Entry point for frontend requests, routing traffic to backend services. AWS PrivateLink: Provides secure, private connectivity between API Gateway and ALB within the VPC. Application Load Balancer (ALB): Distributes traffic across backend containers on ECS. Amazon ECS (Fargate): Runs containerized backend and FastAPI (AI) services. Amazon ECR: Container image registry for ECS deployment. Amazon CloudWatch: Centralized logging, monitoring, and alerting. Amazon SNS: Sends alerts and notifications during incidents. GitLab CI/CD: Automates the container build, push, and deploy pipeline to ECS. 4. Technical Deployment Implementation Phases\nResearch and Architecture Design: Study microservices models and design the overall architecture on AWS (including CloudFront, ECS Fargate, RDS, S3, API Gateway, Cognito) — (January). Cost Estimation and Optimization: Use AWS Pricing Calculator to estimate cost and optimize service selection for affordability and ease of deployment — (January–February). Development, Testing, and Deployment: Build frontend (Next.js), backend (.NET), and AI service (FastAPI); perform microservice integration testing; deploy the system to AWS via ECS Fargate; and set up monitoring with CloudWatch — (February–March). Technical Requirements\nFrontend: The Next.js web application is hosted on Amazon S3 and distributed via CloudFront, communicating securely with the backend through API Gateway. User authentication and session management are handled by Amazon Cognito, which provides tokens for secured API calls. Backend: Developed in .NET (or a similar framework) and deployed on ECS Fargate. Handles business operations, user interactions, and service orchestration. Container images are stored in ECR and automatically deployed via GitLab CI/CD. Load balancing between backend containers is managed by ALB. AI Service: Developed in FastAPI, responsible for processing invoice images and voice inputs. Accesses raw data from S3, performs AI inference, and returns results to the Backend Service via internal APIs. Cloud Infrastructure: Runs inside a multi-AZ Amazon VPC, using Application Load Balancer for traffic distribution and CloudWatch for observability. Containers are stored in ECR and deployed through ECS Fargate. The deployment pipeline is automated using GitLab CI/CD. Security: User authentication managed by Amazon Cognito. Access permissions defined through IAM Roles for ECS, S3, CloudWatch, and API Gateway. Security Groups are tightly configured between ECS, ALB, and other services. AWS WAF provides web-layer protection against common exploits. 5. Timeline \u0026amp; Milestones Pre-internship (Month 0): 1 month for planning. Internship (Month 1–3): Month 1: Learn AWS and upgrade programming skills. Month 2: Design and adjust architecture. Month 3: Implement, test, and deploy. Post-deployment: Research mobile development and deploy after month 4. 6. Budget Estimation You can view costs on AWS Pricing Calculator.\nService In Free Tier After Free Tier Amazon ECS (Fargate) $0.00 / month $8.00 / month Amazon API Gateway $0.00 / month $2.00 / month Amazon S3 $0.00 / month $1.00 / month Amazon CloudWatch $0.00 / month $2.00 / month Amazon Cognito $0.00 / month $0.00 / month Amazon ECR $0.00 / month $0.20 / month Amazon Route 53 $1.00 / month $1.00 / month AWS WAF $0.00 / month $1.00 / month Amazon SNS $0.00 / month $0.50 / month GitLab CI/CD $0.00 / month $2.00 / month Estimated Total ≈ $1.00 / month (≈ $12.00 / year) ≈ $17.70 / month (≈ $212.40 / year) 7. Risk Assessment Risk Matrix\nAI model misrecognition (voice/bill): Medium impact, medium probability. AWS connection loss or regional service errors: High impact, low probability. Exceeding AWS usage budget: Medium impact, low probability. Data synchronization errors between microservices: Medium impact, medium probability. User information leakage (Cognito/Database): High impact, low probability. Mitigation Strategies\nAI: Improve OCR and voice-to-text models through additional training, regular testing with real data. AWS Region: Set up multi-AZ deployment and regular RDS database backup. Cost: Configure AWS Budget Alert and optimize ECS, S3 based on actual usage. Microservices: Use SQS/RabbitMQ to ensure asynchronous processing and retry on errors. Security: Data encryption (AES-256, HTTPS), IAM control following \u0026ldquo;Least Privilege\u0026rdquo; principle. Contingency Plans\nIf AWS encounters issues: Temporarily switch to local transaction data storage and sync after recovery. Restore infrastructure using AWS CloudFormation or pre-saved IaC (Infrastructure as Code). Keep regular database copies (RDS snapshots) for data loss recovery. 8. Expected results of the project Automated financial data entry: The application helps users avoid manual entry, just take a photo of the invoice or record a voice for the system to automatically classify spending. Intuitive financial management: Users can view spending charts, monthly reports, and receive savings suggestions based on consumer behavior. Minimal user experience: Friendly web interface, modern design, optimized for mobile devices and suitable for people new to financial management. Stable, scalable system: Microservices architecture makes it easy to add new features such as spending reminders, AI predictive analysis, or expand to a mobile app. Low operating costs: Take advantage of Free Tier AWS and the serverless model to maintain the system at an average cost of \u0026lt; 50 USD/month. Improving development team skills: Project members have practical access to DevOps processes, CI/CD implementation, and cloud-based application optimization. 9. Project limitations Vietnamese AI model is still limited: The ability to recognize regional voices or handwritten invoices has not yet achieved high accuracy. No separate mobile application: The MVP version only supports the web platform, there is no native mobile app. User limit: The current architecture is only optimized for 50–100 active users; when expanding the scale, the infrastructure needs to be restructured. Internet connection dependent: All processing and storage operations are via the cloud, cannot operate offline. Advanced security system has not been deployed: Only stops at Cognito authentication and basic encryption, no MFA (Multi-Factor Authentication) or in-depth security logs. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerating generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI This blog introduces fully managed MLflow 3.0 on Amazon SageMaker AI, which accelerates Generative AI development by unifying experiment tracking, behavior monitoring, and model lifecycle management into a single platform. This version adds tracing and version tracking features, enabling developers to capture inputs, outputs, and metadata for easier debugging and performance optimization. With deep integration into Amazon Bedrock and SageMaker HyperPod, MLflow 3.0 enhances observability, speeds up troubleshooting, and shortens the time to production for AI models.\nBlog 2 - AI-Enhanced Subsurface Infrastructure Mapping on AWS This blog describes how S2 Labs, Empact AI, and Kraken Robotics use physics-informed AI on AWS HPC to enhance subsurface infrastructure mapping. This approach combines magnetic imaging and deep learning (using a U-Net model) to accurately reconstruct underground or underwater structures such as pipes and tanks. Leveraging AWS Batch, EC2, and S3 for parallel computing, the system achieves high precision in detecting subsurface features up to 40 meters deep—significantly outperforming traditional mapping methods.\nBlog 3 - Unlocking the full potential of Amazon Connect This blog explores how to unlock the full potential of Amazon Connect, an AI- and AWS-powered contact center platform. It highlights the importance of effective change management, including identifying the right stakeholders, securing an executive sponsor, building change ambassadors, and tracking meaningful performance metrics. Success relies on understanding business needs, targeted training, and strong internal communication. When properly implemented, Amazon Connect can enhance operations, enable automation, and deliver superior customer experiences—maximizing ROI and long-term value for organizations.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in three events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DATA SCIENCE WORKSHOP ON AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University, D1 Street, High-Tech Park, Tang Nhon Phu Ward, Ho Chi Minh City.\nRole in the event: Attendees\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://danielleit241.github.io/aws-fcj-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://danielleit241.github.io/aws-fcj-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://danielleit241.github.io/aws-fcj-report/tags/","title":"Tags","tags":[],"description":"","content":""}]