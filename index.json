[{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerate Generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI By Ram Vittal, Amit Modi, Rahul Easwar, and Sandeep Raveesh-Babu on 10 JUL 2025 in Amazon SageMaker AI, Announcements, Technical How-to\nAmazon SageMaker now offers fully managed support for MLflow 3.0, simplifying AI experimentation and accelerating your Generative AI journey from idea to production. This release turns managed MLflow from an experiment-tracking tool into an end-to-end observability solution, reducing time to market for Generative AI.\nAs customers across industries accelerate Generative AI development, they need experiment tracking, behavior observability, and performance evaluation for models and AI applications. Data scientists and developers often struggle to analyze model and application performance from experimentation to production, making root-cause analysis and remediation difficult. Teams spend significant time integrating tools instead of improving the quality of their models or generative AI applications.\nWith the launch of fully managed MLflow 3.0 on Amazon SageMaker AI, you can speed up Generative AI development by tracking experiments and observing model and application behavior with a single tool. MLflow 3.0‚Äôs tracing capabilities let customers record inputs, outputs, and metadata at every step of a Generative AI application, helping developers quickly identify the origin of errors or unexpected behavior. By keeping a history of each model and application version, MLflow 3.0 provides traceability, connecting AI feedback back to the underlying components. This lets developers trace issues back to the exact code, data, or parameters that caused them.\nCustomers using Amazon SageMaker HyperPod to train and deploy foundation models (FMs) can now use managed MLflow to track experiments, monitor training, gain deeper insights into model and application behavior, and manage the ML lifecycle at scale. This reduces troubleshooting time and allows teams to focus more on innovation.\nThis post introduces core concepts of fully managed MLflow 3.0 on SageMaker and provides technical guidance so you can leverage the new features to accelerate your next Generative AI application.\nGet started You can get started with fully managed MLflow 3.0 on Amazon SageMaker to track experiments, manage models, and optimize the Generative AI/ML lifecycle via the AWS Management Console, the AWS CLI, or the API.\nPrerequisites To get started you need:\nAn AWS account with billing enabled\nAn Amazon SageMaker Studio AI domain (see the guide: Guide to getting set up with Amazon SageMaker AI)\nConfigure your environment to use the SageMaker-managed MLflow tracking server Configuration steps: In the SageMaker Studio UI, open the Applications panel, choose MLflow, then select Create. Enter a unique name for the tracking server and specify the Amazon S3 URI where experiment artifacts will be stored. Choose Create. (SageMaker defaults to MLflow v3.0.)\nOptional: choose Update to adjust settings such as server size, tags, and IAM role.\nThe server is provisioned and started automatically and typically takes about 25 minutes. After setup completes, you can launch the MLflow UI from SageMaker Studio to begin tracking ML and Generative AI experiments. For more details on tracking server configuration, see Machine learning experiments using Amazon SageMaker AI with MLflow in the SageMaker Developer Guide: https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html\nTo start logging experiments to the SageMaker managed MLflow tracking server you created, install both MLflow and the SageMaker MLflow Python package in your environment. You can use SageMaker Studio managed Jupyter Lab, SageMaker Studio Code Editor, a local IDE, or any environment that supports AI workloads to connect to the SageMaker managed MLflow tracking server.\nTo install both Python packages with pip: pip install mlflow==3.0 sagemaker-mlflow==0.1.0\nTo connect and start logging your AI experiments, parameters, and models directly to the managed MLflow on SageMaker, replace the Amazon Resource Name (ARN) of your SageMaker MLflow tracking server:\nimport mlflow # SageMaker MLflow ARN tracking_server_arn = \u0026#34;arn:aws:sagemaker:\u0026lt;Region\u0026gt;:\u0026lt;Account_id\u0026gt;:mlflow-tracking-server/\u0026lt;Name\u0026gt;\u0026#34; # Enter ARN mlflow.set_tracking_uri(tracking_server_arn) mlflow.set_experiment(\u0026#34;customer_support_genai_app\u0026#34;) Your environment is now configured and ready to track experiments with the SageMaker managed MLflow tracking server.\nImplement tracing and version tracking for Generative AI applications Generative AI applications have many components ‚Äî code, configuration, and data ‚Äî which can become hard to manage without systematic versioning. A LoggedModel entity in managed MLflow 3.0 represents your AI model, agent, or Generative AI application within an experiment. It provides unified tracking of model artifacts, execution traces, evaluation metrics, and metadata across the development lifecycle. A trace is a recorded log of inputs, outputs, and intermediate steps from a single application execution. Traces offer deep visibility into application performance, execution flow, and response quality, aiding debugging and evaluation. With LoggedModel, you can track and compare different versions of an application, making it easy to identify issues, deploy the best version, and maintain a clear record of what was deployed and when.\nTo implement version tracking and tracing with managed MLflow 3.0 on SageMaker, you can establish a model identity with a version using the Git commit hash, set it as the active model context so subsequent traces automatically link to that specific version, enable automatic logging for interactions with Amazon Bedrock, and then make an API call to Anthropic‚Äôs Claude 3.5 Sonnet ‚Äî that call will be fully traced, with inputs, outputs, and metadata automatically recorded in the established model context. Managed MLflow 3.0 tracing integrates with a number of Generative AI libraries and provides one-line automatic tracing for all supported libraries. For details on supported libraries, see Supported Integrations in the MLflow documentation.\n# 1. Define your application version using the git commit logged_model= \u0026#34;customer_support_agent\u0026#34; logged_model_name = f\u0026#34;{logged_model}-{git_commit}\u0026#34; # 2.Set the active model context - traces will be linked to this mlflow.set_active_model(name=logged_model_name) # 3.Set auto logging for your model provider mlflow.bedrock.autolog() # 4. Chat with your LLM provider # Ensure that your boto3 client has the necessary auth information bedrock = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;\u0026lt;REPLACE_WITH_YOUR_AWS_REGION\u0026gt;\u0026#34;, ) model = \u0026#34;anthropic.claude-3-5-sonnet-20241022-v2:0\u0026#34; messages = [{ \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello!\u0026#34;}]}] # All intermediate executions within the chat session will be logged bedrock.converse(modelId=model, messages=messages) After recording this information you can view Generative AI experiments and the LoggedModel for the agent in the Managed MLflow 3.0 tracking server UI, as shown in the screenshot below.\nIn addition to one-line auto tracing, MLflow provides a Python SDK so you can manually instrument your code and work with traces. See the sample notebook sagemaker_mlflow_strands.ipynb in the aws-samples GitHub repository, where we use MLflow manual instrumentation to trace Strands Agents. With tracing in fully managed MLflow 3.0 you can capture inputs, outputs, and metadata for each intermediate step of a request to help identify the root cause of errors and unexpected behavior.\nThese capabilities provide observability for your AI workloads by collecting detailed information about the execution of workload services, nodes, and tools, and you can inspect them under the Traces tab.\nYou can inspect an individual trace, as illustrated below, by selecting the request ID for the trace you want to view in the Traces tab.\nFully managed MLflow 3.0 on Amazon SageMaker also introduces tagging for traces. Tags are mutable key‚Äìvalue pairs you can attach to traces to add metadata and contextual information. Trace tags make it easy to organize, search, and filter traces by criteria such as user session, environment, model version, or performance characteristics. You can add, update, or remove tags at any time ‚Äî while a trace is executing using mlflow.update_current_trace() or after a trace has been logged using the MLflow APIs or UI.\nManaged MLflow 3.0 makes searching and analyzing traces seamless, helping teams quickly identify issues, compare agent behavior, and optimize performance. Both the tracing UI and the Python API support powerful filtering, so you can dive into traces based on attributes such as status, tags, user, environment, or execution time, as shown in the screenshot below.\nFor example, you can immediately find all traces with errors, filter by production environment, or find traces for a specific request. This capability is important for debugging, cost analysis, and continuous improvement of Generative AI applications.\nThe screenshot below shows the traces returned when searching for the tag \u0026lsquo;Production\u0026rsquo;.\nThe following code shows how you can search for all traces in the production environment with a successful status:\n# Search for traces in production environment with successful status\ntraces = mlflow.search_traces(filter_string=\u0026ldquo;attributes.status = \u0026lsquo;OK\u0026rsquo; AND tags.environment = \u0026lsquo;production\u0026rsquo;\u0026rdquo;)\nUsing MLflow tracing for Generative AI Building and deploying generative AI agents ‚Äî such as chat-based assistants, code generators, or customer support assistants ‚Äî requires deep observability into how those agents interact with large language models (LLMs) and external tools. In a typical agentic workflow, the agent iterates through reasoning steps, calls LLMs, and uses tools or subsystems like search APIs or Model Context Protocol (MCP) servers until the user task is completed. These complex, multi-step interactions make debugging, optimization, and cost tracking particularly challenging.\nTraditional observability tools fall short for generative AI because agent decisions, tool calls, and LLM responses are dynamic and context-dependent. Managed MLflow 3.0 tracing provides comprehensive observability by recording every LLM call, tool invocation, and decision point in the agent workflow. You can use this end-to-end trace data to:\nDebug agent behavior ‚Äî pinpoint where an agent‚Äôs reasoning went off track or why it produced an unexpected output.\nMonitor tool usage ‚Äî discover when and how external tools are invoked and analyze their impact on quality and cost.\nTrack performance and cost ‚Äî measure latency, token usage, and API cost at each step of the agentic loop.\nAudit and govern ‚Äî maintain detailed logs for compliance and analysis.\nImagine a practical scenario using the managed MLflow 3.0 tracing UI for a sample finance customer support agent equipped with a tool for retrieving financial data from a datastore. While developing a generative AI customer support agent or analyzing agent behavior in production, you can observe whether the agent‚Äôs response and execution invoke the product database tool to provide more accurate recommendations.\nExample: the first trace (shown below) captures the agent handling a user query without calling any tools. The trace records the prompt, agent response, and decision points. The agent‚Äôs response lacks product-specific detail. The trace clearly shows that no external tool was invoked, which helps you quickly identify this behavior in the agent‚Äôs reasoning chain.\nThe second trace (shown below) records the same agent but this time the agent decides to call the product database tool. This trace logs the tool invocation, the returned product data, and how the agent integrates that information into the final response. Here, you can observe improved answer quality, slightly increased latency, and additional API cost due to higher token usage.\nBy comparing these traces side-by-side you can debug why the agent sometimes skips using the tool, optimize when and how tools are called, and balance quality with latency and cost. MLflow‚Äôs Tracing UI makes agentic loops transparent, actionable, and straightforward to analyze at scale. The sample agent used in this post and the full source code are available in the aws-samples GitHub repository so you can reproduce and adapt it for your own applications.\nClean up resources After creation, a SageMaker managed MLflow tracking server incurs charges until you stop or delete it. Tracking server charges are based on server uptime, selected size, and the amount of data logged to the tracking server. You can stop the tracking server when not in use to save costs, or delete it via the API or the SageMaker Studio UI. For pricing details, see Amazon SageMaker pricing.\nConclusion Fully managed MLflow 3.0 on Amazon SageMaker AI is now available. Get started with the sample code in the aws-samples GitHub repository. We invite you to explore the new features and experience the increased productivity and control they provide for your ML projects. To learn more, visit Machine Learning Experiments using Amazon SageMaker with MLflow.\nFor more information, see the SageMaker Developer Guide and send feedback to AWS re:Post for SageMaker or through your usual AWS Support channels.\nAbout the authors Ram Vittal Ram Vittal is a Principal ML Solutions Architect at AWS. He has over three decades of experience in architecting and building distributed, hybrid, and cloud-native applications. He is passionate about creating secure, scalable, and reliable AI/ML and big data solutions that help enterprise customers accelerate their cloud adoption and optimization journey to improve business outcomes. In his spare time, he enjoys motorcycle riding and walking his three-year-old sheep-a-doodle dog. Sandeep Raveesh Sandeep Raveesh is a GenAI Specialist Solutions Architect at AWS. He works with customers throughout their AIOps journey, including model training, Retrieval-Augmented Generation (RAG), GenAI Agents, and scaling Generative AI use cases. He also focuses on Go-To-Market strategies to help AWS design and tailor offerings that address industry challenges in the Generative AI domain. You can find Sandeep on LinkedIn. Amit Modi Amit Modi is the Head of Product for SageMaker AIOps and Governance, as well as Responsible AI at AWS. With over a decade of B2B experience, he has built scalable products and teams that drive innovation and deliver customer value globally. Rahul Easwar Rahul Easwar is a Senior Product Manager at AWS, leading Managed MLflow and Partner AI Apps within the SageMaker AIOps group. With over 15 years of experience spanning startups to enterprise technology, he leverages his entrepreneurial background and an MBA from Chicago Booth to build scalable ML platforms that simplify AI adoption for organizations worldwide. Connect with Rahul on LinkedIn to learn more about his work in ML platforms and enterprise AI solutions. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AI-enhanced Subsurface Infrastructure Mapping on AWS By: Santi Adavani, Jacques Guigne, Ryan Qi, Souvik Mukherjee, Srinivas Tadepalli, and Vidyasagar Ananthan ‚Äî 13 May 2025 ‚Äî AWS Batch, AWS ParallelCluster, Customer Solutions, High Performance Computing, Thought Leadership\nSubsurface infrastructure mapping is the process of identifying and visualizing buried structures such as pipes, cables, tanks, and foundations located beneath the surface without excavation. This technology is critical for urban planning, utility maintenance, oil \u0026amp; gas operations, construction safety, and environmental protection. Without accurate subsurface maps, construction projects risk costly delays, dangerous utility strikes, and environmental damage. For example, when Hurricane Ivan damaged an offshore oil platform in 2004, important infrastructure was buried under 35‚Äì45 meters of sediment, creating an invisible hazard that traditional mapping techniques could not fully detect.\nThrough a collaboration between S2 Labs, Empact AI, and Kraken Robotics, a breakthrough in subsurface infrastructure mapping has emerged on AWS. The approach combines advanced magnetic imaging with physics-informed AI to deliver unprecedentedly clear images of subsurface structures ‚Äî especially in scenarios where traditional methods fail. The combination of cloud computing and AI is changing how the industry visualizes and understands critical buried infrastructure.\nDetection methods and limitations Traditional subsurface imaging uses a range of geophysical techniques, each suited to specific materials and conditions. For example, electromagnetic methods detect metallic pipes and cables via conductivity, while magnetometers measure variations in Earth‚Äôs magnetic field to identify ferrous materials such as steel pipes. Ground-penetrating radar (GPR) is particularly effective at imaging concrete structures and geological layers; specialized frequencies can reveal plastic pipes or water-bearing assets because of dielectric contrasts.\nManual interpretation of survey data typically involves 2D signal analysis and basic depth estimation ‚Äî a fast but approximate approach. Two main challenges arise: first, different subsurface configurations can produce nearly identical sensor responses, making it ambiguous which configuration is correct without additional data. Second, real-world environments contain heterogeneous soils, moisture levels, and material densities that vary over short distances, producing complex signals that traditional algorithms struggle to interpret accurately.\nExamining a magnetic survey result, surface measurements show magnetic intensity variations across a 50 m area as illustrated in Figure 1(a). When processed with conventional methods, we obtain a blurred image suggesting a pipe-like structure down to about 5 m, but the result lacks detail (Figure 1(b)). This is where an AI-based approach shows its strength ‚Äî producing a much sharper image that reveals a pipe-like structure at 1‚Äì1.5 m depth (Figure 1(c)). The AI-based result is far more precise while remaining consistent with the original magnetic measurements.\nFigure 1. (a) Map view over a 50 m survey patch. (b) Conventional small-square inversion. (c) Deep-learning-based inversion.\nPhysics-informed deep learning solution S2 Labs applies physics-informed AI and AWS high performance computing (HPC) to solve hard engineering problems across oil \u0026amp; gas, manufacturing, healthcare, and biotech ‚Äî delivering scientifically accurate results with reduced compute time. S2 Labs partnered with two domain specialists: Empact AI, which provides 3D subsurface pipe mapping, and Kraken Robotics, which contributes high-resolution underwater imagery via Synthetic Aperture Sonar. This collaboration integrates advanced sonar, 3D subsurface analysis, and AI-driven pattern recognition on AWS Cloud to locate and characterize pipeline leaks with higher accuracy and speed.\nOur AI approach combines the physics of magnetic fields with deep learning to better interpret what lies underground. By training AI on simulated data that models real-world structures like tanks and pipes, we teach it to ‚Äúread‚Äù magnetic measurements like a map. Using a specialized neural network architecture called U-Net, the model learns to translate magnetic signals into crisp images of subsurface structures, identifying not only location but also composition and shape. For technical details, see the recent research paper published by S2 Labs.\nModel training The physics-informed deep learning model was trained on AWS by combining high performance compute, scalable data storage, and parallel processing services, as illustrated in the architecture diagram in Figure 2.\nUsing Amazon EC2 instances, we generated 202,000 3D susceptibility models (each with 226,000 cells) representing many different subsurface scenarios ‚Äî including pipes at multiple orientations, multiple-pipe configurations, and tanks.\nThe models were parameterized by domain knowledge and stored as NumPy files in Amazon S3 buckets. S2 Labs‚Äô proprietary magnetostatic solver was containerized and stored in Amazon ECR for consistent deployment across compute resources. The solver processed models sequentially from S3 and wrote the response data back to S3.\nWe also employed distributed computing using AWS Batch to generate synthetic data, utilizing Spot Instances to optimize cost. We used P4d instances, each providing eight NVIDIA A100 GPUs, to compute field responses at 1,800 measurement points spaced 2 meters apart. The pipeline synchronized data between Amazon S3 and local storage, trained a 2D U-Net architecture (500M parameters) for 110 epochs, achieving training loss 0.0018 and validation loss 0.0019. The full computation required 100,000 CPU hours.\nFigure 2. Architecture diagram for synthetic data generation and model training on AWS.\nScalable inference workflow for large magnetic surveys Our magnetic survey pipeline applies a systematic four-stage workflow to process large surveys efficiently while preserving high-quality, reproducible reconstructions of subsurface infrastructure, illustrated in Figure 3.\nStage 1 ‚Äî Data Acquisition: Field data collection uses magnetometer systems customized to the survey environment ‚Äî drone-mounted for airborne surveys, ground systems for land surveys, or underwater systems for marine applications. Surveys follow a systematic grid sampling pattern with consistent sensor heights and transect spacing to ensure uniform coverage of the target area.\nStage 2 ‚Äî Survey Domain Preparation: Rather than processing the entire survey area at once, we adopt a modular approach by dividing the survey domain into smaller tiles sized to the AI training input. Adjacent tiles include overlap regions that are critical to ensuring smooth transitions in the final reconstruction and avoiding edge artifacts.\nStage 3 ‚Äî Parallel Processing Architecture: The workflow leverages parallel computing to process many tiles simultaneously, dramatically reducing compute time while preserving consistency with the trained model parameters. This distributed approach efficiently uses compute resources by processing tiles independently. For example, our deployment can process a survey volume of 400 m x 400 m x 60 m in under 5 seconds.\nStage 4 ‚Äî AI-Based Inference: The trained AI model performs inference on each tile independently, reconstructing subsurface magnetic susceptibility distributions from field magnetic measurements. The reconstructions are then blended smoothly using weighted blending across overlap regions to ensure seamless transitions between neighboring tiles. This modular pipeline enables scalable surveying at any size while maintaining consistent resolution and optimizing memory usage via effective parallel processing, making it practical for real-world applications from infrastructure mapping to geological surveys.\nFigure 3. Modular processing workflow for large-scale magnetic surveys.\nCase study: mapping buried offshore well conductors in the Gulf of Mexico Hurricane Ivan (2004) damaged an offshore oil platform in the Gulf of Mexico, burying well conductors under 35‚Äì45 meters of sediment. Initial acoustic imaging in 2022 had some success but remained limited where gas-bearing sediments obscured critical areas. A high-resolution magnetometer array was deployed 3.5 m above the seafloor to detect iron-rich conductors through hydrocarbon-saturated sediment.\nThe model described earlier successfully mapped conductors buried at 35‚Äì45 m depths, revealing a primary conductor bundle and a secondary fragment 40 m northeast of the well bay (see Figure 4). The results show strong discrimination of magnetic signatures even in complex debris fields, verified where possible with borehole locations and acoustic imagery. This demonstrates the power of deep learning in cases where traditional acoustic methods fail.\nFigure 4. Plan view (a) and oblique view (b) of relative susceptibility distribution.\nConclusion Our work demonstrates how AI-enhanced magnetic imaging is transforming subsurface infrastructure mapping across industries ‚Äî from onshore utilities to deep offshore well conductors. Physics-informed deep learning trained on AWS combines HPC resources, scalable storage, and parallel processing services to overcome limitations of traditional magnetic data interpretation.\nThrough real-world case studies, we demonstrated that deep learning can exceed traditional magnetic interpretation limits and map structures successfully at 40 m depth beneath the seafloor ‚Äî structures that remained ‚Äúinvisible‚Äù to acoustic methods for 18 years.\nThe impact of this technology spans oil \u0026amp; gas decommissioning, urban utility mapping, environmental protection, and marine operations. While results are promising, opportunities remain in multi-physics integration, real-time processing, and higher-resolution imaging.\nTo collaborate or learn more about deployment, please contact us at santi@s2labs.co or ryanqi@amazon.com.\nAuthors Santi Adavani Dr. Santi Adavani is the founder and CEO of S2 Labs, a deep-tech startup building AI products to accelerate scientific discovery. Prior to S2 Labs, Santi founded and served as CTO of RocketML, where he built an MLOps platform backed by HPC. He also served as Head of Product and AI at PostgresML, leading development of an in-memory Postgres-based vector database, and earlier held senior product leadership roles at Intel. Santi holds a PhD in Computational Science and Engineering from University of Pennsylvania. Jacques Guign√© Professor Jacques Yves Guign√© is a Senior Advisor to Kraken Robotics in Newfoundland, Canada. He serves as Chief Scientific Officer of Subsea Micropiles Ltd., which operates in Ireland and the U.K., and is CEO of Acoustic Zoom Inc., a leading geophysical research company. Jacques brings deep experience in acoustic imaging and has made significant contributions to mapping complex seabed features. His scientific achievements include over 80 patents and 70 publications with strong citation counts on ResearchGate. He has been honored in physics with the Deryck Chesterman and Rayleigh Medals, holds degrees including a DSc and a PhD, and is recognized as a Geoscience Canada member and a director of PEGNL (Professional Engineers and Geoscientists Newfoundland and Labrador). Ryan Qi Ryan has 19 years of experience in multiphysics modeling and simulation, strategy, and business development across industrial and digital domains. At AWS, Ryan is a Principal Worldwide BD/GTM Leader focused on simulation technologies and autonomous systems. Souvik Mukherjee Dr. Souvik Mukherjee is a founding member of EmPact-AI and Principal Technical Advisor. His 15+ year career spans energy and technology roles as a geophysicist, data scientist, and product leader. He has received industry recognition such as the Shell GameChanger (2015) award and the URTeC 2019 Best Paper \u0026 Innovation award, among others. He has led and delivered multi-million-dollar projects including commercialization of the award-winning QUANTUM hydraulic-fracture delineation technology for Carbo Ceramics and coordinated the Shell Frontier Exploration Study, managing a cross-disciplinary team of 15 technical specialists that influenced a $100M lease acquisition strategy. Srinivas Tadepalli Srinivas is the Global HPC Go-to-Market lead at AWS, responsible for building a comprehensive GTM strategy across HPC and accelerated computing workloads serving commercial and public-sector customers. Previously he worked at Dassault Systems and holds a PhD in biomedical engineering. Vidyasagar Ananthan Vidyasagar specializes in high performance computing, numerical simulations, optimization engineering, and software development across industry and academia. At AWS, Vidyasagar is a Senior Solutions Architect building predictive models and simulation technologies. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Unlocking the Full Potential of Amazon Connect By Puneet Badlani and Eliza Taylor ‚Äì May 12, 2025; categories: Amazon Connect, Best Practices, Foundational (100), Thought Leadership\nConsumers today have very high expectations‚Äîand your customers are no exception. Every business is racing to adopt the latest technology innovations that can improve service, reduce costs, and support strategic growth. Amazon Connect is one of those solutions‚Äîpowered by AWS and AI, it‚Äôs a modern, scalable contact-center platform. However, cutting corners during implementation can undermine the benefits.\nHere we look at how change-management best practices can not only protect but accelerate your investment‚Äîrisks to watch for, which metrics really matter, and how to maximize limited time and resources to deliver high-impact change.\nDon‚Äôt \u0026ldquo;Trip Up\u0026rdquo; on Stakeholders \u0026ldquo;Stakeholders\u0026rdquo; are often misunderstood. Many organizations focus too much on either internal or external groups. For a customer-service technology and process transformation, it‚Äôs critical to focus on external stakeholders because they have direct impact.\nExternal stakeholders include customers, partners, and vendors‚Äîthe people who contact your service center. Their ability to receive information, resolve issues, and feel heard matters greatly to program success and whether they‚Äôll remain engaged with you long term.\nInternally, large programs require cross-organizational coordination. An executive sponsor can marshal resources and help ensure success. Beyond IT, operations and marketing, you‚Äôll need finance, HR, and other teams involved. Not all stakeholders are equal‚Äîthe level of engagement and oversight depends on role and influence.\nUnderstand What You Actually Need Amazon Connect is a powerful, customizable platform serving many use cases‚Äîfrom omnichannel contact, interaction automation, agent assist with generative AI, dynamic reporting, automated quality evaluation, and more.\nBut without a clear understanding of current processes, pain points, and business requirements, you risk scope drift and losing benefits. Do you need an intelligent IVR to reduce call volume and shorten transfers to agents? Do you need Connect integrated with a legacy CRM? Are SLAs slipping because of sudden call spikes or scheduling issues?\nIdentify these factors early and a deployment roadmap will emerge, backed by a clear business case. You‚Äôll have a compelling story with metrics to prove ROI and clear goals for the teams. This is also a strength of AWS Partner CloudInteract‚Äîusing AI to uncover insights and bring precision to improvement planning.\nEnsure Executive Sponsorship A committed executive sponsor plays a vital role in driving momentum and raising organizational awareness for any transformation program. They need a convincing vision and by actively promoting the project they can help surface and address potential risks.\nThis approach helps remove blockers early while fostering a collaborative environment where stakeholders feel encouraged to contribute. Executive presence and support also build trust and credibility‚Äîessential elements for a successful rollout.\nBuild Change Ambassadors The biggest mistake in technology transformation is assuming you don‚Äôt need to \u0026ldquo;tell the story\u0026rdquo; about the change‚Äîthat a few technical benefit emails and a single training session are enough.\nYou need to run an internal communications campaign for the change. Investing in two-way channels (monitored email, lunch-and-learns, community forums, Q\u0026amp;A sessions after town halls) will both build goodwill and surface practitioner insights.\nExtend influence by recruiting and supporting champions‚Äîpeople who can spread the change. They‚Äôre often early adopters and testers. You can incentivize participation by including these activities in annual goals, which helps expand impact across affected groups.\nTrain More Effectively with Fewer Resources As the program progresses, different groups will require different training. Initially, IT needs to learn the new environment and how to configure and manage it. Next, operations must train agents and supervisors for day-to-day use. Everything starts with a change impact assessment.\nThis assessment identifies which groups are most and most critically affected. If resources are limited and you can‚Äôt train everyone directly, focus on the business-breakers‚Äîthe groups that are mission-critical. For other groups, substitute remote or self-paced learning.\nMeasure What Actually Matters Don‚Äôt let metrics become slogans. Does reporting email-open rates actually show that employees understand the change? An opened email doesn‚Äôt equal awareness. A better measure might be the number of manager-led meetings run using the toolkit you provided.\nThe mistake is measuring only project metrics. You need to demonstrate long-term improvement: reduced transfer rates, handle-time reductions, CSAT, or time comparisons for manual tasks before (scheduling, performance review, or reporting).\nProject-status reporting is important but temporary‚Äîthe metrics that really matter are those from the business case. Start tracking them as early as possible, especially in phased rollouts, to quickly surface gaps in training and adoption.\nIn Summary A few key factors make Amazon Connect implementations more effective and unlock full value. Plan ahead, bring the organization along, deploy carefully, and this AI-enabled contact-center solution will exceed expectations‚Äîfrom omnichannel and agent experience to automation and analytics.\nContact AWS and CloudInteract for additional help unlocking the full power of Amazon Connect:\nAWS: https://pages.awscloud.com/GLOBAL-field-SP-Amazon-Connect-Contact-Us-reg.html CloudInteract: https://resources.cloudinteract.io/apollo-making-every-contact-count (Loose translation: \u0026ldquo;unlock the superpowers of Amazon Connect\u0026rdquo;)\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Report on ‚ÄúVietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)‚Äù Purpose of the Event Learn about security in GenAI and AI Agents to strengthen enterprise safety. Explore the AI-Driven Development Lifecycle (AI-DLC) and how it applies to software development. Understand how to build a unified data foundation optimized for analytics and AI. Stay updated on the latest GenAI strategies and trends on AWS. Speakers Jun Kai Loke ‚Äì AI/ML Specialist SA, AWS Kien Nguyen ‚Äì Solutions Architect, AWS Tamelly Lim ‚Äì Storage Specialist SA, AWS Binh Tran ‚Äì Senior Solutions Architect, AWS Taiki Dang ‚Äì Solutions Architect, AWS Michael Armentano ‚Äì Principal WW GTM Specialist, AWS Key Highlights Main Content Unified Data Platform on AWS for AI \u0026amp; Analytics\nBuilding an end-to-end data pipeline: ingestion ‚Üí storage ‚Üí processing ‚Üí access ‚Üí governance. Breaking down silos in data, people, and processes; enabling self-service \u0026amp; standardized governance. Key services: S3, Glue, Redshift, Lake Formation, OpenSearch, Kinesis/MSK. GenAI Strategy on AWS\nVision, trends, and enterprise adoption roadmap. Amazon Bedrock: model selection, RAG, guardrails, cost/latency optimization. AgentCore \u0026amp; Amazon Nova with support for frameworks (CrewAI, LangGraph, LlamaIndex\u0026hellip;). Securing GenAI Applications\nOWASP LLM risks; multilayered security: infrastructure ‚Üí model ‚Üí application. Five pillars: Compliance, Privacy, Controls, Risk Management, Resilience. Tools: Bedrock Guardrails, Human-in-the-loop, Observability (OpenTelemetry). AI Agents ‚Äì Productivity Boosters\nFrom assistants to multi-agent systems, automation with less supervision. Use cases: customer support, BI with Amazon Q (QuickSight), process automation. Reliability \u0026amp; Accuracy of GenAI\nMitigating hallucination with Prompt Engineering, RAG, Fine-tuning. RAG workflow: input ‚Üí embedding ‚Üí context ‚Üí LLM ‚Üí output. AI-Driven Development Lifecycle (AI-DLC)\nLifecycle: Inception ‚Üí Construction ‚Üí Operation. Evolution: AI-Assisted ‚Üí AI-Driven ‚Üí AI-Managed. Implementation with IaC, automated testing, monitoring, and risk management. Amazon SageMaker ‚Äì Unified Studio\nUnified environment for data, analytics, and AI. Supports Lakehouse, governance, Zero-ETL integration (S3 ‚Üî Redshift, Aurora, DynamoDB, RDS\u0026hellip;). Full MLOps: pipelines, registry, deployment, monitoring. Integrated with Bedrock \u0026amp; JumpStart to accelerate GenAI application development. Key Learnings Design Mindset\nBuild data \u0026amp; AI systems end-to-end, removing silos. Apply self-service and governance principles from the start. Technical Architecture\nIntegrate AWS services (S3, Glue, Redshift, SageMaker, Bedrock‚Ä¶) into a unified platform. Apply Zero-ETL, Lakehouse, MLOps for scalability, governance, and sustainable operations. Leverage AI Agents and GenAI frameworks to automate processes and boost productivity. Strategy\nDefine a GenAI adoption roadmap balancing innovation speed and cost. Focus on multilayered security: infra, model, application; combine guardrails \u0026amp; human-in-the-loop. Prioritize reliability and accuracy with RAG, prompt engineering, fine-tuning. Software Development Mindset\nTransition from AI-Assisted ‚Üí AI-Driven ‚Üí AI-Managed. Adopt AI-DLC to standardize development with AI involved at every stage. Application to Work In projects:\nExperiment with AI Agents for registration/login and customer support. Use validation/guardrails to safely integrate GenAI into applications. In learning \u0026amp; team projects:\nApply AI-DLC for task division: AI supports code/docs generation, team reviews \u0026amp; approves. Know when to use Lambda (serverless) vs containers (ECS/Fargate). As an intern:\nLearn to apply a business-first approach when writing documentation or gathering requirements. Realize the importance of a solid data foundation for GenAI to deliver real value. Event Experience Joining the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was a highly valuable experience, giving me a holistic view of modernizing applications and databases using cutting-edge methods and tools. Some key takeaways:\nLearning from Experts AWS experts shared the latest trends in GenAI, Data Foundation, and Security. Gained a clearer understanding of building a unified data foundation for AI \u0026amp; Analytics. Impressed by the vision of AI Agents and their potential to enhance productivity. Hands-on Technical Insights Learned how to design an end-to-end data pipeline: ingestion ‚Üí storage ‚Üí processing ‚Üí access ‚Üí governance. Explored tools like Amazon Bedrock, AgentCore, and SageMaker Unified Studio. Discovered solutions to reduce hallucination (Prompt Engineering, RAG). Understood how to apply AI-DLC for balancing tasks between AI and humans in software development. Tools \u0026amp; Methods in Practice Explored Bedrock Guardrails to ensure safe GenAI implementation. Understood when to use serverless (AWS Lambda) vs containerization (ECS/Fargate). Learned how to leverage Amazon Q for BI (QuickSight) and customer support. Networking \u0026amp; Exchange The event was a great chance to interact with AWS experts and learn from real-world case studies. Realized the importance of a business-first approach in every technology decision. Key Takeaways GenAI is not just a tool, but requires the right strategy and architecture to generate value. Data and security are the foundations‚Äîwithout them, AI cannot thrive. AI Agents and AI-DLC are set to reshape how we design and operate systems. Event Photos (Add photos from the event here)\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Report: ‚ÄúAI-Driven Development Lifecycle: Reimagining Software Engineering‚Äù Event Objectives Understand how AI can automate and optimize stages of the Software Development Lifecycle (SDLC). Embrace the philosophy of AI augmenting humans rather than replacing them in the software development process. Observe how tools like Amazon Q and other AI assistants support developers from ideation and code generation to infrastructure deployment (IaC). Learn about the emerging trend of ‚ÄúAI-first development,‚Äù where AI becomes a natural part of future dev workflows. Speakers Toan Huynh My Nguyen Highlights Challenges of programming with AI The introduction covered the limitations and challenges of applying AI to programming:\nAI still struggles with projects that require deep domain knowledge and complex business logic. Developers can have limited control over generated code when prompts and scope are not well-defined. The quality of generated code depends heavily on the prompt and context provided to the model. This motivates the AI-DLC approach: creating a structured process to help AI and humans collaborate more effectively.\nHow AI is changing software development This section analyzed how AI is transforming the software industry:\nAI assists code generation, technical documentation, API design, and automated testing. Developers shift roles from ‚Äúcode writers‚Äù to ‚ÄúAI orchestrators‚Äù who guide, evaluate, and refine AI outputs. Tools like Amazon Q, GitHub Copilot and ChatGPT for Developers become central parts of modern development workflows. üîπ What is AI-DLC AI-Driven Development Lifecycle (AI-DLC) is an AI-augmented software development approach where each stage is designed to provide AI with specific context and goals to produce more accurate results.\nüüß Inception\nBuild Context on Existing Code ‚Äì feed AI the current codebase so it understands project structure. Elaborate Intent with User Stories ‚Äì developers describe requirements via user stories to clarify goals. Plan with Units of Work ‚Äì break work into small units the AI can execute and generate code for. üü¶ Construction\nDomain Model (Component Model) ‚Äì build domain models or architecture diagrams. Generate Code \u0026amp; Test ‚Äì AI generates code and tests based on the plan. Add Architectural Components ‚Äì add API layers, data layers, logging, and security components. Deploy with IaC \u0026amp; Tests ‚Äì automate deployment using Infrastructure as Code and integration tests. üîÅ Each stage provides richer context for the next, helping AI produce increasingly accurate outputs.\nCore Concepts Context Awareness ‚Äì AI needs clear context about code, requirements, and domain to work well. Collaborative Generation ‚Äì humans and AI collaborate: AI generates code, humans direct and verify outputs. Continuous Refinement ‚Äì iterative cycles to refine outputs and improve quality. Mob Elaboration Mob Elaboration is a collaborative method for elaborating intents:\nMultiple participants contribute user stories, questions, and additional context for the AI. It helps AI gain deeper understanding of domain, goals, and complex logic. This approach reduces the risk of misunderstandings‚Äîespecially in large or cross-domain teams. The 5-Stage Sequential Process of AI-DLC AI-DLC runs through 5 phases:\nInception ‚Äì understand requirements and analyze the system. Construction ‚Äì create domain models and initial structure. Generation ‚Äì automated code generation. Testing ‚Äì automated unit and integration testing. Deployment ‚Äì deploy applications with IaC and CI/CD pipelines. Each loop improves the AI\u0026rsquo;s outputs through incremental learning and feedback.\nDemo 1 ‚Äì Interactive AI-DLC experience with Amazon Q The demo showcased AI-DLC in practice with a small project:\nStart from a simple idea and turn it into a user story describing business requirements. AI helps split tasks into Units of Work and plans implementation details for each module. Attendees interact with AI using questions, checkboxes, and logical conditions to clarify scope. AI generates code, tests, project structure, and executes trial deployments. The demo illustrated smooth collaboration between AI and humans: AI performs repetitive generation while humans steer and make decisions. Introducing Kiro Philosophy of Kiro\nThe workshop introduced Kiro, an intelligent development environment built around the idea of ‚ÄúAI-native development‚Äù where AI is a core collaborator rather than just a tool.\nKiro‚Äôs philosophy emphasizes three points:\nDeep integration with the development process ‚Äì AI participates in planning, context management, and impact analysis. Comprehensive project context ‚Äì Kiro maintains ongoing awareness of project structure so AI can interact with the whole project rather than single files. Intelligent control \u0026amp; collaboration ‚Äì developers guide AI via contextual commands so each change has clear intent and consistency. This makes Kiro more than a code generator: it is an ecosystem for collaborative human‚ÄìAI development.\nProject structure in Kiro\nUnlike traditional text editors like VSCode or JetBrains, Kiro is an AI-aware workspace with structural awareness.\nIts project model includes:\nContext Layer ‚Äì stores context, domain models, and relationships among modules. Task Layer ‚Äì manages Units of Work tracked and executed by AI. AI Agent Layer ‚Äì agents handle specific tasks (code, tests, refactor, deploy) enabling a multi-agent collaborative model. Human-in-the-Loop Control ‚Äì developers can confirm, modify, or reject AI outputs at any stage. Kiro therefore becomes an ecosystem for coordinated human‚ÄìAI development rather than just a code editor.\nDemo 2: Kiro in practice In the demonstration, the presenters showed how Kiro implements AI-DLC:\nUser provides a basic business requirement like ‚Äúbuild an event management system.‚Äù Kiro analyzes intent, creates a domain model, and splits work into user stories. AI generates modules, components, and corresponding test cases. Developers interact with a checkbox-based task control to approve each unit of work. Kiro finally deploys the completed system using IaC and automated tests. The demo proved AI-DLC is practical: AI, human operators, and processes integrate into a single coherent workflow.\nEvent experience Attending the workshop ‚ÄúAI DLC x Kiro: Reinventing Developer Experience with AI‚Äù was highly valuable, clarifying how AI can be deeply embedded into the developer experience and how Kiro‚Äôs design offers a fresh approach for developers.\nInsights from expert speakers Speakers presented AI-DLC as a platform that automates many SDLC tasks and supports software development using AI. The Kiro introduction gave a perspective on designing an AI-native text editor rather than adding AI plugins to legacy editors. I was particularly impressed by Kiro‚Äôs philosophy: minimalism, high performance, user-focused experience, and modular extensibility. Practical technical takeaways The demo showed how AI-DLC and Kiro can create, refactor, and optimize code efficiently. A small starter project was created and managed within Kiro, demonstrating auto-refactoring, test generation and logic analysis. Compared to editors like VSCode and Sublime, Kiro stands out for its AI-first architecture and lightweight plugin model that preserves performance. Modern tooling and potential applications Experiencing AI-DLC on Kiro highlighted the potential to automate development workflows‚Äîespecially code generation, documentation, and debugging. I saw opportunities to build personal learning and productivity tools that provide smart suggestions and accelerate development. Kiro‚Äôs modular design inspires approaches to building flexible, maintainable systems. Networking and discussions The workshop offered chances to connect with developers, AI researchers, and product designers, deepening my understanding of AI-augmented development. Discussions helped me see AI as a creative collaborator, allowing developers to focus more on system logic and architecture. Key lessons AI-DLC combined with Kiro is a model for next-generation development tools‚ÄîAI-first IDEs that deeply integrate AI into the workflow. Kiro‚Äôs ‚Äúless is more‚Äù philosophy shows that simplicity and performance can deliver a stronger developer experience than overly complex systems. I learned that successful AI adoption depends not only on technology but also on the design philosophy and integration approach used in tooling. Sample images from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Report: ‚ÄúWORKSHOP: DATA SCIENCE ON AWS‚Äù Event Objectives Share AWS AI services Demonstrate deploying AI models using Amazon SageMaker Show how to deploy models and access them via APIs Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud Developer Engineer, AWS Community Builder Highlights Introduction \u0026amp; the importance of Cloud in Data Science Discussed the role of cloud computing in supporting data processing, training, and deploying AI models at scale.\nCloud vs. On-premise:\nCloud: flexible scalability, rapid deployment, lower operating costs, easy integration. On-premise: high upfront investment, harder to scale, complex maintenance. Cloud (especially AWS) provides a strong foundation for the Data Science pipeline ‚Äî from collection, storage, and processing to training and deploying AI models.\nAI Layers on AWS AWS organizes the AI ecosystem into three layers, helping users choose the right level of management based on skills and goals:\n1. AI Services (Fully Managed Layer)\nFor users who want to apply AI without deep Machine Learning knowledge.\nFully managed AI services that have been pre-trained by AWS.\nUsers can call APIs to use them directly in applications.\nExamples:\nAmazon Comprehend: Natural language processing (NLP) Amazon Translate: Neural machine translation Amazon Textract: Extract data from documents and invoices Amazon Rekognition: Image and video recognition Amazon Polly: Text-to-speech Amazon Bedrock: Access to foundation models (e.g., Claude, Titan, Mistral) üëâ Benefits: Fast deployment, no model training needed, cost scales with usage.\n2. ML Services (Semi-managed Layer)\nFor Data Scientists and ML Engineers who want to build, train, and deploy ML models with more customization.\nAmazon SageMaker is at the center of this layer: it provides tools to build, train, and deploy ML models.\nKey features:\nData Wrangler: Visual data cleaning and processing. Feature Store: Manage features across models. AutoML (SageMaker Autopilot): Automated model training. Model Registry \u0026amp; Monitoring: Track and manage models after deployment. üëâ Benefits: Full control over the ML pipeline, customizable algorithms, training environments, and deployment workflows.\n3. AI Infrastructure (Self-managed Layer)\nFor organizations or experts who want to fully manage AI/ML infrastructure to optimize cost or performance.\nUsers can build training environments using core AWS infrastructure services:\nAmazon EC2 / GPU Instances (P5, G6, Inferentia): Train large custom models. Amazon EKS / ECS: Run ML workloads in containers or Kubernetes. AWS Lambda: Small-scale data processing or serverless inference. Amazon S3 / EFS: Store data and models. üëâ Benefits: Maximum flexibility and control, but requires higher technical expertise.\nPopular AWS AI Services to Support Students During Model Training 1. Amazon SageMaker\nIntegrated development environment (SageMaker Studio) for the full ML lifecycle:\nData preparation Model training Result tracking Deploying endpoints for API inference Supports AutoML, GPU training, model monitoring, and CI/CD for AI models.\n2. Amazon Comprehend\nNLP service to analyze, understand, and classify natural language.\nMain capabilities:\nSentiment analysis Entity recognition Text classification Automated labeling Language detection Use cases:\nIntelligent document processing Bulk email analysis to detect positive/negative responses Customer sentiment and behavioral analysis Contact center analytics Information extraction and validation 3. Amazon Translate\nNeural machine translation service.\nSupports over 75 languages with high accuracy and easy integration.\nApplications:\nMultilingual websites Automatic content translation in apps Multilingual chatbot support and analytics 4. Amazon Textract\nAutomatically extract text and structured data from images, documents, and forms. Used for processes like record digitization, invoice processing, and automatic data entry. AWS Data Science Pipeline Overview Data collection \u0026amp; storage: Amazon S3, AWS Data Exchange Data preprocessing: AWS Glue, Lambda, Athena Model training: SageMaker (train, tune, evaluate) Model deployment: SageMaker Endpoint / Lambda + API Gateway Monitoring \u0026amp; optimization: CloudWatch, Model Monitor Demo 1: Designing an AI Training Workflow with a Drag-and-Drop Interface (No-Code/Low-Code) Goal: Show how to build an AI training pipeline without heavy coding.\nTools: Amazon SageMaker Studio / SageMaker Canvas\nDemo steps:\nPrepare the dataset and upload it to Amazon S3.\nUse SageMaker\u0026rsquo;s drag-and-drop interface to:\nChoose data sources, training algorithms, and parameters. Design a pipeline including data cleaning, training, validation, and deployment steps. Visually monitor training progress and model results (accuracy, confusion matrix, metrics, etc.).\nKey message: Students and developers can quickly create AI workflows without complex code, speeding up research and experimentation.\nDemo 2: Deploying an AI Service and Accessing It Via API/Website Goal: Demonstrate how to deploy an AI model so users can access it in practice.\nTools: Amazon SageMaker Endpoint, API Gateway, and Lambda.\nDemo steps:\nDeploy the trained model to a SageMaker Endpoint. Integrate the endpoint with API Gateway to create a public REST API. Provide a web route or API URL for users to send requests (e.g., submit text for sentiment analysis or translation). Show how to present results visually (UI demo or Postman/API test). Key message: Demonstrates how AWS supports moving AI from research to production ‚Äî easy to share, scale, and commercialize.\nDiscussion: Performance \u0026amp; Cost (Cloud vs. On-premise) Criteria Cloud (AWS) On-premise Scalability Easily scale resources as needed Limited by fixed hardware Cost Pay-as-you-go High upfront investment Deployment Automated, fast Manual, time-consuming Maintenance Managed by AWS User is responsible Suitable for students ‚úÖ Free Tier available, easy to learn ‚ùå Harder to access, costly Conclusion AWS provides a comprehensive AI ecosystem from infrastructure to application layers, suitable for everyone ‚Äî from students learning AI to enterprises deploying at scale. Event Experience Attending the workshop ‚ÄúAI Services on AWS for Data Science‚Äù was very valuable. It helped me better understand the role of cloud in Data Science and how AWS supports training, deploying, and accessing AI models.\nKey takeaways from expert speakers Speakers emphasized the importance of cloud in data processing and model training. Gained a clear understanding of the three AI layers on AWS: AI-managed services, ML services (SageMaker), and AI frameworks. Hands-on technical experience Demo 1: Designed an AI workflow using SageMaker Canvas drag-and-drop to train models without code. Demo 2: Deployed an AI model as a service accessible via API or link. Using modern tools Learned about key AI services: Amazon Comprehend, Translate, and Textract. Understood how these services support NLP, machine translation, and intelligent data extraction. Networking and discussion Interacted with experts and fellow students interested in AI \u0026amp; Cloud. Discussed cost, performance (Cloud vs On-premise), and how to optimize SageMaker usage. Lessons learned Cloud is a foundational platform for modern Data Science workflows. AWS provides tools for every AI skill level ‚Äî from no-code to fully managed deployments. Gained clearer knowledge of how to bring AI models into real products using AWS services. Some photos from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Workshop Harvest Report: ‚ÄúAWS Cloud Mastery Series #1: GENERATIVE AI, RAG \u0026amp; AWS AGENTIC AI‚Äù Event Objectives Master the art of Prompt Engineering to effectively control AI models. Explore the ecosystem of Pretrained AI Services available on AWS. Gain a deep understanding of building AI applications using RAG (Retrieval-Augmented Generation). Update on the latest trends in Agentic AI and how to move AI Agents from prototype (POC) to production using Amazon Bedrock AgentCore. Explore the Pipecat Framework for building real-time voice-based virtual assistants. Speakers Lam Tuan Kiet - Sr. DevOps Engineer (FPT Software) Danh Hoang Hieu Nghi - AI Engineer (Renova Cloud) Dinh Le Hoang Anh - Cloud Engineer Trainee (First Cloud AI Journey) Key Highlights 1. Prompt Engineering \u0026amp; Foundation Models (The Core Foundation) Before diving into complex services, the event emphasized the importance of understanding and communicating with Foundation Models via Amazon Bedrock.\nZero-shot / Few-shot Prompting: Techniques involving direct instructions or providing examples to guide the model\u0026rsquo;s output format. Chain of Thought (CoT): A crucial technique requiring the model to \u0026ldquo;think step-by-step,\u0026rdquo; significantly improving accuracy for complex logical problems. 2. Pretrained AWS AI Services (Ready-to-Use APIs) Introduction to \u0026ldquo;ready-to-use\u0026rdquo; APIs that integrate intelligent features without model training:\nImage/Video: Amazon Rekognition. Language: Amazon Translate, Comprehend, Textract (OCR). Audio: Amazon Polly (Text-to-Speech), Transcribe (Speech-to-Text). 3. RAG - Retrieval Augmented Generation A process helping AI answer based on enterprise data, reducing hallucinations:\nEmbeddings: Using Amazon Titan Text Embeddings V2 to vectorise text for semantic search. Knowledge Bases for Amazon Bedrock: Fully managed process handling Chunking -\u0026gt; Vector Store -\u0026gt; Retrieval -\u0026gt; Generation. 4. The Evolution to Agentic AI The event introduced the next evolution of GenAI:\nGenAI Assistants: Follow rules, automate repetitive tasks. GenAI Agents: Goal-oriented, handling a broader range of tasks. Agentic AI Systems: Multi-agent systems acting fully autonomously with minimal human oversight. The \u0026ldquo;Prototype to Production Chasm\u0026rdquo;: Moving Agents from POC to Production faces major hurdles regarding:\nPerformance \u0026amp; Scalability. Security \u0026amp; Governance. Complexity: Difficulties in managing Memory, access controls, and auditing Agent interactions. 5. Amazon Bedrock AgentCore: Bridging the Gap To solve these challenges, AWS introduced AgentCore - a comprehensive platform for building and operating Agents:\nKey Components: Runtime \u0026amp; Memory: Execution environment and the ability to \u0026ldquo;remember\u0026rdquo; interaction history/learning. Identity \u0026amp; Gateway: Identity management and secure connection gateways. Code Interpreter: Allows Agents to write and execute code to process complex data. Observability: Tools to monitor and audit agent activities. Benefit: Allows developers to focus on business logic rather than infrastructure security or context management. 6. Pipecat: Framework for Real-time Voice AI An interesting Open Source framework introduced for building Multimodal Virtual Assistants:\nFocus: Optimized for Real-time interactions and conversational streaming. Pipeline Mechanism: WebRTC Input: Receives audio signals from the user. STT (Speech-to-Text): Converts voice to text. LLM Processing: Processes natural language to generate a response. TTS (Text-to-Speech): Converts text back to voice. Output: Streams audio back to the user with ultra-low latency. Event Experience \u0026amp; Reflection Participating in this workshop expanded my perspective from basic concepts to the cutting-edge technologies shaping the future of AI.\n1. The Shift from \u0026ldquo;Q\u0026amp;A\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) The most impressive concept for me was Agentic AI. Previously, I viewed AI primarily for chatting or summarization. However, through the AgentCore presentation, I see a future of \u0026ldquo;virtual employees\u0026rdquo; capable of planning, using tools (like web browsers or code interpreters), and solving complex workflows without constant human hand-holding.\n2. Solving the \u0026ldquo;Production\u0026rdquo; Puzzle I resonated deeply with the discussion on the \u0026ldquo;Chasm\u0026rdquo; between POC and Production. Tools like Amazon Bedrock AgentCore are essentially the key to building enterprise trust. They provide the necessary security layers (Identity) and control mechanisms (Observability) that allow businesses to confidently delegate tasks to AI.\n3. The Potential of Voice AI with Pipecat The Pipecat demo was fascinating. Combining WebRTC with AI models to create fluid, low-latency conversations opens up endless practical applications, such as intelligent virtual call centers, AI interview assistants, or real-time language tutors.\nConclusion The ‚ÄúGenerative AI \u0026amp; Agentic AI on AWS‚Äù workshop provided a valuable panoramic view:\nPresent: We rely on RAG and Prompt Engineering to work effectively with data. Future: We are entering the era of Agentic AI, where Autonomous Agents will transform business operations. Tools: With the AWS ecosystem (Bedrock, AgentCore) and Frameworks (Pipecat, LangChain), technical barriers are being removed, empowering engineers to turn breakthrough ideas into reality. Some photos from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Report ‚ÄúAWS Cloud Mastery Series #2: From DevOps, IaC to Containers \u0026amp; Observability‚Äù Event Objectives Standardize Mindset: Deeply understand the Value Cycle and the core role of DevOps in continuous, reliable software delivery. Modernize Infrastructure (IaC): Shift from manual operations (ClickOps) to managing infrastructure as code using CloudFormation, Terraform, and CDK. Optimize Applications (Containerization): Master the architecture and strategy for selecting the appropriate container platform: App Runner, ECS, or EKS. Comprehensive Monitoring (Observability): Build proactive monitoring systems to detect errors and optimize performance using CloudWatch and X-Ray. Speakers AWS Experts \u0026amp; Cloud Engineers: Shared insights on system architecture, Platform Engineering strategies, and deep-dive technical demos. Key Content Details 1. DevOps Mindset \u0026amp; CI/CD Pipeline (The Foundation) The event began by redefining DevOps not just as a set of tools, but as a culture of optimizing the value stream.\nThe Value Cycle:\nA closed-loop 5-step process: Insights \u0026amp; Analysis -\u0026gt; Portfolio \u0026amp; Backlog -\u0026gt; Continuous Integration -\u0026gt; Continuous Testing -\u0026gt; Continuous Delivery. Core Goal: Increase delivery speed (Speed) to meet market demands faster, while ensuring system stability (Stability) and quality. Redefining CI/CD Concepts:\nContinuous Integration (CI): Developers merge code frequently (daily). The system automatically builds and runs Unit Tests. The goal is to detect errors early (Fail fast). Continuous Delivery: Automates the deployment process to Staging/Pre-prod environments. Deployment to Production requires human approval (Manual Trigger). Continuous Deployment: Fully automated 100% from Code Commit to running in Production (no manual intervention). Effective Pipeline Strategy:\nCentralized CI: Build a centralized CI system for security and resource management, but ensure Self-service capabilities for Developers to avoid bottlenecks. Artifact Management: Apply the \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo; principle. Source code is built only once into a Binary package (Artifact). Subsequent environments (Staging, Prod) use this exact Artifact for deployment, ensuring absolute consistency. Fail Fast Conditions: The pipeline must be configured to fail immediately if violations occur: Compilation errors, Code Style violations, Security scans finding vulnerabilities, or Tests running too slow. Measuring Efficiency (Metrics):\nUse Heatmaps to monitor the Pipeline health of the entire organization. Golden Metrics: Deployment Frequency, Change Failure Rate, and MTTR (Mean Time To Recovery). 2. Infrastructure as Code (IaC) - From ClickOps to Code This section delved into eliminating manual habits (ClickOps) and moving towards full infrastructure automation.\nThe Problem with \u0026ldquo;ClickOps\u0026rdquo;: Manual operations on the AWS Console are prone to Human Error, slow, hard to scale, and cause inconsistency between Dev/Prod environments. IaC Solutions: Provide Automation, Scalability, Reproducibility, and Collaboration. Deep Dive into Top 3 IaC Tools:\n1. AWS CloudFormation (Native Tool):\nUses text files (YAML or JSON) to describe the desired state. Template Anatomy: Structure includes Parameters (Dynamic inputs), Mappings (Handling regional differences - e.g., different AMI IDs per Region), and Resources (The actual assets to create). Stack: The unit for managing resource lifecycles. Deleting a Stack deletes all associated resources. 2. Terraform (Multi-Cloud Powerhouse):\nOpen-source tool, uses HCL (HashiCorp Configuration Language). Strength: Multi-platform support (Multi-cloud: AWS, Azure, GCP\u0026hellip;). Workflow: Write (Code) -\u0026gt; Plan (Preview changes) -\u0026gt; Apply (Execute). The Plan step is critical for safety checks. State File: Stores the actual state of the infrastructure for synchronization. 3. AWS CDK (Cloud Development Kit):\nAllows defining infrastructure using programming languages (Python, TypeScript, Java\u0026hellip;). Constructs: L1 (Cfn Resources): Detailed configuration for every line (like CloudFormation). L2 (Curated): Automatically applies Best Practices and secure default configurations. L3 (Patterns): Builds complex architectures (e.g., VPC + ALB + ECS) in just a few lines of code. Drift Detection: A crucial feature to detect discrepancies between Code and Reality (caused by manual \u0026ldquo;ClickOps\u0026rdquo; changes), helping maintain operational discipline.\n3. Containerization - Application Strategy Deep analysis of container orchestration platforms:\nKubernetes (K8s):\nArchitecture includes Control Plane (API Server, etcd, Scheduler) and Worker Nodes (Kubelet, Pods). Powerful and flexible but complex to operate. Comparison: Amazon ECS vs. Amazon EKS:\nAmazon ECS: Simple, deeply integrated with AWS (ALB, IAM). Suitable for teams wanting to reduce operational overhead and deploy fast. Amazon EKS: Based on standard Kubernetes. Powerful, massive ecosystem. Suitable for Enterprises, complex systems, or Hybrid-cloud. Compute Options:\nEC2 Launch Type: You manage the servers (Patching, Scaling). Highest control but high operational effort. AWS Fargate (Serverless): No server management required. AWS handles the infrastructure; users only define CPU/RAM for Tasks. Secure and convenient. AWS App Runner:\n\u0026ldquo;Zero-ops\u0026rdquo; solution for Web Apps/APIs. Fully automated from Source Code/Image -\u0026gt; Public URL (HTTPS) without configuring networking or servers. 4. Observability - Monitoring \u0026amp; Optimization Closing the development lifecycle loop with deep observability to ensure stable system operation.\nAmazon CloudWatch (System Eyes \u0026amp; Ears):\nMetrics: Collect performance data (CPU, Memory, Disk). Logs: Centralized application log collection. Use Logs Insights to query errors. Alarms: Automatically trigger actions (Auto Scaling, Restart Server, Send Notifications) when thresholds are breached. AWS X-Ray (Distributed Tracing):\nSolves the \u0026ldquo;needle in a haystack\u0026rdquo; problem in Microservices. Distributed Tracing: Tracks the journey of a request across multiple services to identify bottlenecks and root causes. AWS Observability Best Practices:\nUtilize AWS resources to reference standard Patterns and Recipes. Clear distinction: Logs (Discrete events) vs. Traces (Connected journeys). Event Experience \u0026amp; Reflection Participating in this series brought significant changes to my perception and technical skills:\n1. The Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized the role of modern DevOps is not running after Developers to manually deploy code. DevOps is about architecting a \u0026ldquo;Highway\u0026rdquo; (Pipeline \u0026amp; Platform). A good platform allows Developers to Self-service environment creation and code deployment quickly, while staying within the safety boundaries (Governance) established by the DevOps team.\n2. Operational Discipline Lessons on Artifact Management and Drift Detection are golden rules. In an Enterprise environment, consistency is vital. Differences in build processes across environments (Dev/Test/Prod) are strictly prohibited, and manual changes to code-managed systems must be forbidden.\n3. Smart Tool Selection Strategy There is no \u0026ldquo;best\u0026rdquo; tool, only the \u0026ldquo;most suitable\u0026rdquo; one:\nNeed absolute stability and deepest support for new AWS services: Choose CloudFormation. Enterprise using Multi-cloud or Hybrid-cloud: Terraform is the optimal choice. Strong Programming Development Team needing to build complex architectures fast with high code reuse: AWS CDK is the strongest weapon. Simple Web Applications: Use App Runner instead of wasting resources operating a Kubernetes cluster. Conclusion The \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; series provided a complete roadmap for the Cloud journey:\nMindset: Transitioning from manual work to automation and data-driven measurement. Infrastructure: Mastering IaC for scalable, reproducible systems with drift control. Operations: Combining flexible Containerization and deep Observability to ensure system stability, high performance, and self-healing capabilities. This is a solid knowledge foundation for building large-scale, modern software systems on AWS.\nSome photos from the event "},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Report: ‚ÄúAWS Cloud Mastery Series #3‚Äù Event Purpose This series was not just about individual services but a journey in System Thinking, helping to transition from traditional infrastructure management to a Cloud-Native Security model. The core objectives included:\nCommunity Connection: Spreading the spirit of learning and skill development through AWS Cloud Clubs. Governance Foundation: Managing scale with hundreds of AWS accounts while ensuring compliance. Defense in Depth: Combining Identity, Network, and Data protection to eliminate Single Points of Failure. Automated Response: Removing human latency from the incident response process. Speakers List The event gathered top experts from the AWS community, including AWS Community Builders, Cloud Engineers, and core members of the First Cloud Journey program:\nAWS Cloud Clubs Representatives: Captains from HCMUTE, SGU, PTIT, HUFLIT (Le Vu Xuan An, Tran Duc Anh, Tran Doan Cong Ly, Danh Hoang Hieu Nghi). Identity \u0026amp; Governance: Huynh Hoang Long, Dinh Le Hoang Anh (AWS Community Builders). Detection \u0026amp; Monitoring: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat. Network Security: Kha Van (Cloud Security Engineer | AWS Community Builder). Data Protection: Thinh Lam, Viet Nguyen. Incident Response: Mendel Grabski (Long) - ex Head of Security \u0026amp; DevOps, Tinh Truong - Platform Engineer. Detailed Content PART 1: KICK-OFF - AWS CLOUD CLUBS \u0026amp; OPPORTUNITIES The journey began with the introduction of AWS Cloud Clubs, a place to nurture future Cloud talents.\n1. Vision:\nEmpower students to explore and grow cloud computing skills. Develop technical leadership and build global connections. 2. Core Benefits:\nBuild Skills: Learn through hands-on projects, access AWS exam vouchers and Udemy accounts. Build Community: Connect with AWS experts and industry speakers. Build Opportunities: Enhance personal portfolios, receive AWS credits, and get career support. 3. The Badging Journey:\nGamified development roadmap for Core Team members and Captains. Levels ranging from Bronze, Silver, Gold, Platinum to Diamond. Rewards: AWS Credits ($200+), Certification Vouchers, Exclusive Swag kits, and pre-approval for Student Community Day. PART 2: IDENTITY \u0026amp; GOVERNANCE FOUNDATION Security in the Cloud starts with controlling \u0026ldquo;Who can do what\u0026rdquo;.\n1. Modern IAM Mindset:\nIdentity First: In the Cloud environment, Identity is the new firewall. Credential Spectrum: Absolute shift from Long-term Credentials (Permanent Access Keys - high risk) to Short-term Credentials (STS tokens - secure, auto-expire). Least Privilege: Apply minimum necessary permissions. Avoid using * in Policies unless absolutely necessary. 2. Governance at Scale with AWS Organizations:\nHierarchical Structure: Divide the organization into Organizational Units (OUs) like Security, Shared Services, Workloads (Prod/Dev) to isolate risks. Service Control Policies (SCPs): This is the \u0026ldquo;Constitution\u0026rdquo; of the organization. SCPs establish Guardrails that block dangerous actions (e.g., prohibiting CloudTrail disablement, restricting Regions) that even Admin accounts cannot bypass. PART 3: VISIBILITY \u0026amp; DETECTION You cannot protect what you cannot see.\n1. Amazon GuardDuty - Intelligent Scout:\nUses Machine Learning to detect anomalies from 3 foundational data sources: CloudTrail (management events), VPC Flow Logs (network traffic), and DNS Logs (domain queries). Runtime Monitoring: Advanced feature that looks \u0026ldquo;deep\u0026rdquo; inside the operating system (via a lightweight Agent) to detect strange processes, file modifications, or privilege escalation behaviors. 2. AWS Security Hub - Command Center:\nSolves the \u0026ldquo;alert fatigue\u0026rdquo; problem using ASFF (AWS Security Finding Format). It normalizes alerts from GuardDuty, Inspector, and Macie into a single JSON language. Acts as a Cloud Security Posture Management (CSPM) tool, automatically checking if the system complies with CIS, PCI-DSS standards. PART 4: NETWORK SECURITY Building a \u0026ldquo;Digital Fortress\u0026rdquo; with a defense-in-depth strategy from the edge to the core.\n1. Fundamental Controls (VPC Fundamentals):\nSecurity Groups (Stateful): Apply Micro-segmentation. Instead of whitelisting IP addresses (which change easily), use Security Group Referencing (e.g., SG-DB only allows traffic from SG-App). NACLs (Stateless): Act as a coarse filtering layer at the Subnet boundary, used to block blacklisted IPs or untrusted subnets. 2. Advanced Defense (Advanced Filtering):\nDNS Firewall (Route 53 Resolver): Blocks connections to Command \u0026amp; Control (C2) servers right at the domain resolution step. This is a crucial choke point against malware (like the M√©lof√©e case study). AWS Network Firewall: Next-gen firewall with Deep Packet Inspection (DPI) capabilities. Stateless Engine: Fast filtering based on 5-tuple (IP/Port). Stateful Engine: Uses Suricata-compatible rules for Intrusion Prevention (IPS) and Domain filtering (FQDN) for Egress traffic. 3. Modern Network Architecture:\nUses AWS Transit Gateway with Native Network Firewall integration to simplify the network model, removing the complexity of routing through an \u0026ldquo;Inspection VPC\u0026rdquo;. Applies Active Threat Defense: Automatically syncs malicious IP lists from GuardDuty to Network Firewall for immediate blocking without manual intervention. PART 5: DATA PROTECTION Data is the ultimate asset that must be protected by encryption.\n1. Envelope Encryption:\nUnderstanding the AWS KMS mechanism: Master Key (resides in HSM) encrypts the Data Key, and the Data Key is what encrypts the actual data. This mechanism ensures high performance and absolute security. 2. Secrets Management:\nProblem: Hardcoding passwords in source code is a basic but common error. Solution: Use AWS Secrets Manager for storage and, more importantly, Automatic Rotation of Database passwords using Lambda. Applications always retrieve the latest password via API. 3. Infrastructure Encryption:\nUses AWS Nitro System: Encryption tasks are offloaded to specialized hardware (Nitro Cards), enabling data encryption without compromising the host server\u0026rsquo;s CPU performance (Zero Performance Impact). PART 6: INCIDENT RESPONSE When defense layers are breached, the response process determines the extent of the damage.\n1. Prevention Strategy (Sleep Better):\nGolden Rules: Eliminate long-lived SSH/Keys, Block Public S3 access, Default to Private Subnets. Infrastructure as Code (IaC): Mandate all infrastructure changes via Code (Terraform/CDK) and approval processes (PR Review), completely eliminating manual changes (ClickOps) that cause configuration drift. 2. Standard 5-Step Process:\nPreparation: Have tools and Playbooks ready. Detection: Rely on CloudTrail and GuardDuty. Containment: \u0026ldquo;Jail\u0026rdquo; infected resources by changing Security Groups or revoking IAM permissions. Eradication \u0026amp; Recovery: Remove malware, restore from clean backups. Post-Incident: Learn lessons. 3. Automation is King:\nHumans cannot race against machine speed. Hands-on labs demonstrated the necessity of using EventBridge + Lambda to automatically isolate compromised EC2 instances or auto-remediate public S3 buckets in seconds. Conclusion The \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; series has provided a comprehensive overview of building secure systems on AWS through key pillars:\nGovernance \u0026amp; Identity: The foundation of every security system starts with strict user management and organizational policies. Network \u0026amp; Monitoring: Establishing defense-in-depth layers and comprehensive visibility to detect potential threats. Data \u0026amp; Response: Protecting digital assets with encryption and readying automated incident response processes to ensure service continuity. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/5.3.1-create-kb/","title":"Initialize Knowledge Base","tags":[],"description":"","content":"Target We will use the Amazon Bedrock Wizard to set up the entire RAG architecture. This process will connect the S3 data source, the Embedding model, and automatically initialize the Vector storage (OpenSearch Serverless).\nImplementation Steps Log in to the AWS Management Console and access the Amazon Bedrock service. In the left-hand menu, select Knowledge bases. Click the Create knowledge base button in the top right corner of the screen. Step 1: Configure Knowledge Base\nOn the first configuration screen:\nKnowledge base name: Enter knowledge-base-demo Knowledge Base description - optional: Enter Knowledge Base from AWS Overview (This section requires you to describe the data you have previously uploaded to S3). IAM permissions: Select the option Create and use a new service role. Service role name: Keep the default value suggested by AWS (starting with AmazonBedrockExecutionRoleForKnowledgeBase_...). Click Next. Step 2: Configure Data Source\nConnect to the S3 Bucket containing the documents:\nData source name: Enterknowledge-base-demo S3 URI:\nClick the Browse S3 button. In the pop-up window, select the bucket rag-workshop-demo you created in the previous section. Click Choose. Keep Default configurations. Click Next.\nStep 3: Storage \u0026amp; Processing\nThis is the most critical step to define the AI model and vector storage location:\nEmbeddings model:\nClick Select model. Select model: Titan Embeddings G1 - Text v2. Vector Store:\nVector store creation method: Choose Quick create a new vector store - Recommended Vector store type - new: Choose Amazon OpenSearch Serverless Note: This option allows AWS to automatically create an Amazon OpenSearch Serverless cluster to store data, saving you from manual infrastructure management. Click Next. Step 4: Review and Create Knowledge Base\nReview all configuration information on the Review page. Ensure the S3 URI and Model items are correct. Scroll to the bottom of the page and click the Create knowledge base button. Step 5: Wait for Initialization\nAfter clicking Create, the system will begin the background infrastructure initialization process for the Vector Store.\nWait time: Approximately 2 - 5 minutes. Note: Please do not close the browser during this time. Success: When the screen displays a green notification \u0026ldquo;Knowledge base created successfully\u0026rdquo;, you have completed this step and are ready for the next section. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Le Vu Phuong Hoa\nPhone Number: 0327 030 024\nEmail: danielleee241@gmail.com\nUniversity: FPT University Campus Ho Chi Minh\nMajor: Information Technology\nClass: SE181951\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Overview In this workshop, we will focus on building an intelligent AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; and answering questions based on proprietary enterprise data (RAG technique).\nThe main objective is to establish a fully automated and serverless data processing workflow, consisting of the following steps:\nIngestion: Loading source documents into the system. Indexing: Converting text into vectors and storing them for retrieval. Retrieval \u0026amp; Generation: Configuring the AI model to search for relevant information and generate answers to user questions. üí° Highlight: This solution allows you to eliminate the need to manage any server infrastructure, optimizing costs and operational time.\nImplementation Steps RAG Explanation Service Introduction "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/5.2.1-model-access/","title":"Verify Model Access","tags":[],"description":"","content":"Overview According to AWS\u0026rsquo;s new policy, Foundation Models are often automatically enabled. However, for third-party partner models like Anthropic (Claude), first-time users in a new Region must declare usage information (Use Case) to be able to invoke the model.\nEnsure your AWS account has permission to access and use the Anthropic Claude 3 Sonnet model. This is a mandatory step to avoid AccessDenied errors when the Chatbot operates later. If this is the first time you are using this model in a new Region, you need to declare the intended use (Use Case).\nAccess Check We will perform a quick test (Test Run) to ensure your account is ready.\nIn the search bar, access the Amazon Bedrock.\nStep 1. Access Chat Playground\nIn the left menu of the Bedrock Console, find Playgrounds. Click Chat. Step 2. Select Test Model\nClick Select model (above the chat box). Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet). Throughput: Select On-demand. Click Apply. Step 3. Send Activation Message\nIn the chat box: Enter Hello. Click Run. Observe result: If AI answers: Success (Proceed immediately to section 5.2.2). If a red error or \u0026ldquo;Submit use case details\u0026rdquo; popup appears: Information declaration required (Continue to step 4 below). Step 4. Submit Use Case (Only perform if error occurred in step 3)\nClick Submit use case details (in the error message). Fill in the form: Company Name: Enter Personal Learning. Industry: Select Technology. Intended Use: Select Research \u0026amp; Development. Click Submit. Wait 1 minute, return to the chat box, Click Run on the Hello message again to confirm success. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the fundamentals of cloud computing and AWS global infrastructure. Learn how to manage AWS services using Management Console, CLI, and SDK. Explore AWS security, IAM, and cost management through practical labs. Build foundational knowledge of VPC networking and connectivity (Subnets, Route Tables, IGW, NAT, Peering, Transit Gateway). Gain hands-on experience by performing AWS labs to reinforce theoretical knowledge with practical skills. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 AWS Cloud Foundation \u0026amp; IAM Security - Cloud Essentials: Researched core cloud computing concepts and AWS Global Infrastructure (Regions, AZs, Edge Locations). - Account Management: Explored AWS Pricing models and Support Plans. - Hands-on Lab: Secured Root User, configured IAM (Users, Groups, MFA), and set up cost alerts using AWS Budgets. 08/09/2025 08/09/2025 Module 01 2 Networking Fundamentals (VPC Architecture) - VPC Design: Analyzed Amazon VPC architecture, IP addressing (CIDR Blocks), and Subnet segmentation (Public/Private). - Routing Components: Studied routing mechanisms via Route Tables, Internet Gateway (IGW), and NAT Gateway. - Network Interfaces: Researched Elastic IP (EIP), ENI, and VPC Endpoints (Gateway \u0026amp; Interface) for optimized internal connectivity. 09/09/2025 09/09/2025 Module 02 3 Network Security \u0026amp; Advanced Connectivity - Network Security: Compared Instance-level security (Security Groups) vs. Subnet-level security (Network ACLs); monitored traffic with VPC Flow Logs. - Inter-Connectivity: Explored multi-VPC connection models (VPC Peering, Transit Gateway) and hybrid connectivity (VPN, Direct Connect). - Load Balancing: Studied application traffic distribution using Elastic Load Balancing (ELB) (ALB, NLB, GWLB). 10/09/2025 10/09/2025 Module 02 4 Hands-on Lab: VPC Infrastructure Implementation - Infrastructure Setup: Deployed a standard VPC model: Created Subnets, IGW, Route Tables, and Security Groups. - Compute \u0026amp; Connectivity: Launched EC2 Instances and configured NAT Gateway for secure Private Subnet internet access. - Operations: Practiced resource monitoring via CloudWatch and secure session management using SSM Session Manager. 11/09/2025 11/09/2025 Module 02 5 Hands-on Lab: Advanced Networking Scenarios - Secure Access: Practiced Port Forwarding techniques and centralized log management with Session Manager. - VPC Peering: Configured cross-Region/Account VPC peering connections and updated route tables. - Transit Gateway: Implemented a centralized Hub-and-Spoke network model to simplify routing. - Hybrid DNS: Configured Route 53 Resolver (Inbound/Outbound Endpoints) for hybrid domain resolution. 12/09/2025 12/09/2025 Module 02 Week 1 Achievements: Gained solid knowledge of AWS and Cloud Computing\nUnderstood cloud computing concepts, benefits, and the cloud adoption journey. Learned AWS global infrastructure (Regions, AZs, Edge Locations, Data Centers). Hands-on with AWS management tools\nPracticed with AWS Management Console, AWS CLI, and AWS SDK. Created and managed IAM Users/Groups, enabled MFA, and configured Access Keys. Cost management \u0026amp; support\nCreated and configured different AWS Budgets. Learned AWS Support Plans and practiced opening support cases. AWS Networking \u0026amp; Security\nBuilt and configured VPC, Subnet, Route Table, Internet Gateway, NAT Gateway. Implemented Security Groups, Network ACLs, and VPC Flow Logs. Understood VPC Peering vs. Transit Gateway, and hybrid connectivity (VPN, Direct Connect). System management on AWS\nManaged EC2 instances with Session Manager instead of SSH. Practiced Port Forwarding and Session Logs. Enabled CloudWatch Monitoring \u0026amp; Alerts for EC2. Advanced solutions\nSet up VPC Peering and Transit Gateway for multi-VPC connectivity. Configured Hybrid DNS with Route 53 Resolver (Inbound/Outbound Endpoints). Practiced Elastic Load Balancer (ALB, NLB, CLB, GWLB). üëâ Outcome: After Week 1, I have built a strong foundation in AWS fundamentals and intermediate networking concepts, while successfully completing multiple hands-on labs directly on AWS to strengthen my practical skills.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn and practice AWS Compute services (EC2, AMI, EBS, Auto Scaling, ELB). Gain skills in monitoring and backup using CloudWatch and AWS Backup. Explore AWS Storage services: S3, Storage Gateway, FSx. Hands-on with deployment, scaling, and static website hosting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 AWS Compute \u0026amp; EC2 Administration - Compute Theory: Researched Amazon EC2 architecture, Nitro Hypervisor, Instance families, and pricing strategies (Spot, Reserved, Savings Plans). - Storage for Compute: Differentiated between EBS (Block) and Instance Store (Ephemeral); managed AMIs and Snapshots. - Hands-on Lab 04: + Provisioned EC2 infrastructure (Linux \u0026amp; Windows) within a VPC. + Established secure connections via SSH (Linux) and RDP (Windows). + Deployed a Node.js CRUD User Management application on EC2. 14/09/2025 15/09/2025 Module 03 2 High Availability \u0026amp; Monitoring Operations - Scaling \u0026amp; Load Balancing (Lab 06): + Configured Auto Scaling Groups for dynamic scalability. + Deployed Elastic Load Balancer (ELB) for traffic distribution. - Observability (Lab 08): Configured Amazon CloudWatch to collect Metrics, Logs, and set up system anomaly Alarms. - Data Protection (Lab 13): Implemented automated backup strategies using AWS Backup for EC2, RDS, and EFS based on RPO/RTO targets. 16/09/2025 16/09/2025 Module 03\nModule 06 3 AWS Storage Fundamentals (Amazon S3) - Object Storage Architecture: Studied S3 architecture, object immutability, and data durability standards. - Storage Classes: Analyzed storage tiers (Standard, Intelligent-Tiering, Glacier) to optimize costs based on access patterns. - Security \u0026amp; Management: Configured Bucket Policies, ACLs, Versioning, and Lifecycle Policies. - Archive Strategy: Explored long-term archival solutions with S3 Glacier and Deep Archive. 17/09/2025 17/09/2025 Module 04 4 Hybrid Cloud Storage (Storage Gateway) - Hybrid Architecture: Explored on-premises storage extension to Cloud via AWS Storage Gateway. - Hands-on Lab 24 (File Gateway): + Deployed an EC2 instance acting as a Gateway Appliance. + Configured NFS File Shares mapping to backend S3 Buckets. + Practiced mounting network drives from Cloud to local workstations. 18/09/2025 18/09/2025 Module 04 5 Managed File Systems \u0026amp; Static Hosting - Amazon FSx (Lab 25): Deployed fully managed Windows native file shares (SMB) using Amazon FSx for Windows File Server. - Static Website Hosting (Lab 57): + Hosted a static website directly on an S3 Bucket. + Optimized performance and security via Amazon CloudFront (CDN) and OAI (Origin Access Identity). + Configured DNS and Public Access settings. 19/09/2025 19/09/2025 Module 04 Week 2 Achievements: Compute \u0026amp; Deployment\nGained strong understanding of Amazon EC2 concepts, architecture, and related components (AMI, Snapshot, Key Pair, EBS). Successfully launched and managed both Linux and Windows EC2 instances. Practiced deployment of a real-world Node.js CRUD application on EC2 instances. Learned automation techniques using User Data, Meta Data, and AWS Systems Manager. Scalability \u0026amp; High Availability\nImplemented EC2 Auto Scaling Group to automatically adjust capacity based on demand. Configured Elastic Load Balancer (ELB) to distribute traffic across multiple EC2 instances. Integrated Auto Scaling with Load Balancing for high availability and cost optimization. Monitoring \u0026amp; Observability\nExplored Amazon CloudWatch for infrastructure and application monitoring. Created custom metrics, dashboards, and alarms for real-time system health tracking. Centralized log management using CloudWatch Logs with retention and anomaly detection. Practiced automated monitoring using CloudFormation stacks. Data Protection \u0026amp; Backup\nLearned to design AWS Backup Plans for multiple services (EBS, RDS, DynamoDB, EFS). Implemented recovery objectives (RTO/RPO) for disaster recovery strategies. Configured SNS notifications for backup success, failure, and recovery events. Storage \u0026amp; Data Management\nUnderstood Amazon S3 as an object storage service, with lifecycle policies and storage classes. Practiced S3 features: versioning, access control (ACL, Bucket Policy), and CORS. Configured S3 Static Website Hosting and tested public accessibility. Implemented CloudFront distribution for faster content delivery and secured access via OAI. Learned Amazon FSx for Windows File Server: architecture, integration with Windows, and managed file storage service. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand AWS security philosophy (‚ÄúSecurity is Job Zero‚Äù) and the Shared Responsibility Model. Gain proficiency with Identity \u0026amp; Access Management: IAM Users, Groups, Roles, Policies, Permission Boundaries, Organizations, Identity Center, Cognito. Apply AWS KMS for data encryption, integrate with CloudTrail, and analyze logs using Athena. Learn AWS Database Services: RDS, Aurora, Redshift, ElastiCache and core concepts of SQL, NoSQL, OLTP, OLAP. Deploy Amazon RDS with Multi-AZ, subnet groups, backup and restore for high availability and resilience. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 AWS Security \u0026amp; Identity Management - Security Fundamentals: Studied the Shared Responsibility Model and the \u0026ldquo;Security is Job Zero\u0026rdquo; philosophy. - Identity Management: Deep dived into IAM (Users, Groups, Roles, Policies) and centralized access management via AWS Organizations \u0026amp; Identity Center. - App Security: Explored Amazon Cognito (User/Identity Pools) for application authentication. - Compliance: Researched data encryption using AWS KMS and compliance monitoring with Security Hub. 22/09/2025 22/09/2025 Module 05 2 Hands-on Lab: IAM Policy \u0026amp; Role Configuration - IAM Basics (Lab 02): Practiced creating IAM Users/Groups and applying the Least Privilege principle. - Advanced Roles (Lab 44): Configured IAM Roles with advanced conditions (IP, Date/Time) for granular access control. - Instance Profile (Lab 48): Attached IAM Roles to EC2 Instances to enable secure S3 access without hardcoding credentials in the application. 23/09/2025 23/09/2025 Module 05 3 Hands-on Lab: Advanced Security Implementation - Federated Identity (Lab 18): Configured Amazon Cognito for identity federation and user authentication. - Permission Boundaries (Lab 30): Implemented Permissions Boundaries to limit the maximum permissions available to IAM entities. - Data Encryption (Lab 33): Managed Customer Master Keys (CMKs) with AWS KMS and integrated CloudTrail for auditing key usage. 24/09/2025 24/09/2025 Module 05 4 AWS Database Services Fundamentals - Database Concepts: Differentiated between Relational (RDBMS) vs Non-Relational (NoSQL) databases, and OLTP vs OLAP workloads. - Managed RDBMS: Analyzed Amazon RDS (Multi-AZ, Read Replicas) and Amazon Aurora (Cloud-native architecture). - Specialized DBs: Explored Amazon Redshift for Data Warehousing and Amazon ElastiCache for in-memory caching strategies. 25/09/2025 25/09/2025 Module 06 5 Hands-on Lab: Deploying \u0026amp; Managing Amazon RDS - Infrastructure Setup: Configured VPC, DB Subnet Groups, and isolated Security Groups for the database layer (Lab 05). - Deployment: Provisioned a Multi-AZ Amazon RDS instance and established secure connectivity from an EC2 Web Server. - Operations: Practiced Backup \u0026amp; Restore procedures, managed Snapshots, and executed Point-in-Time Recovery (PITR). 26/09/2025 26/09/2025 Module 06 Week 3 Achievements: Security Services:\nApplied Shared Responsibility Model in practice. Managed IAM with Users, Groups, Roles, Policies, Conditions, and Permission Boundaries. Used IAM Role with EC2 to remove security risks of hardcoded Access Keys. Implemented Identity Federation with Cognito (Google, Facebook, SAML). Centralized multi-account management with AWS Organizations and Identity Center. Encryption \u0026amp; Monitoring:\nCreated and managed KMS CMKs for encryption. Encrypted S3 data with KMS. Logged and monitored key activities with CloudTrail, analyzed with Athena. Database Services:\nLearned differences between RDBMS vs NoSQL, OLTP vs OLAP. Deployed Amazon RDS with Multi-AZ, subnet groups, automated backups, snapshots, and point-in-time recovery. Studied Aurora (Backtrack, Global DB, Cloning), Redshift (MPP, Spectrum), and ElastiCache (Redis, Memcached). Hands-on Practice:\nCompleted labs: IAM Basics, IAM Roles \u0026amp; Conditions, IAM Roles for Applications, Cognito Federation, IAM Permission Boundaries, KMS Encryption, RDS Deployment. Built a secure RDS environment with backup and recovery strategies. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: The goal of this week was to strengthen understanding of AWS data and AI services ‚Äî from building scalable DataLake architectures to working with serverless NoSQL databases (DynamoDB) and exploring AI development lifecycle management. Additionally, this week aimed to enhance technical translation and communication skills through hands-on labs, blog translations, and participation in AWS events. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Building a Data Lake on AWS - Data Lake Concepts: Explored centralized repository architectures for varied data analytics workloads. - AWS Glue (ETL): Studied Glue Crawlers for automated schema discovery and Data Catalog generation. - Amazon Athena: Performed serverless SQL queries directly on S3 data. - Amazon QuickSight: Visualized data insights via interactive Dashboards. - Hands-on Lab 35: Deployed Data Lake infrastructure, ingested data via Kinesis Firehose, and queried cataloged data using Athena. 29/09/2025 29/09/2025 Module 07 2 Query Optimization \u0026amp; Cost Analysis (Lab 40) - ETL Workflow: Executed end-to-end pipeline: Raw Data Ingestion -\u0026gt; Glue Crawler -\u0026gt; Transformation (Parquet) -\u0026gt; Athena Query. - Cost Analysis: Analyzed AWS billing data using SQL in Athena (Service breakdown, Tagging). - Optimization Techniques: Applied cost-saving strategies: Parquet compression, query LIMITs, and data partitioning. 30/09/2025 30/09/2025 Module 07 3 NoSQL Databases with Amazon DynamoDB (Lab 60) - DynamoDB Fundamentals: Researched Serverless NoSQL architecture, auto-scaling, and Capacity Modes (On-demand vs Provisioned). - Data Modeling: Deep dived into Primary Keys (Partition + Sort) and Secondary Indexes (GSI/LSI). - Consistency Models: Compared Eventual Consistency vs. Strong Consistency trade-offs. - Implementation: Practiced table creation, item manipulation, and querying via AWS Console and SDK (Boto3). 01/10/2025 01/10/2025 Module 06 4 AWS Technical Blog Research \u0026amp; Translation - Generative AI: Translated and studied GenAI lifecycle management using MLflow on SageMaker. - Industry Application: Researched Deep Learning applications for subsurface infrastructure mapping. - Contact Center AI: Explored Amazon Connect optimization using AI capabilities. - Skill Development: Enhanced technical vocabulary and domain knowledge in SageMaker, Batch, and ParallelCluster. 02/10/2025 02/10/2025 5 AWS Event: AI Development Lifecycle \u0026amp; Kiro - AI Lifecycle: Gained insights into the full AI development pipeline: Data Prep -\u0026gt; Training -\u0026gt; Deployment -\u0026gt; Monitoring. - AWS AI Ecosystem: Understood the role of SageMaker, Bedrock, and CodeWhisperer in AI DevSecOps. - New Tooling (Kiro): Explored Kiro for unified AI workflow management. - Real-world Use Cases: Analyzed case studies on AI automation and model optimization in production. 03/10/2025 03/10/2025 Week 4 Achievements: Built an AWS DataLake pipeline integrating Glue, Athena, and QuickSight, gaining hands-on experience in data ingestion, transformation, and visualization. Configured AWS Glue Crawlers and Athena queries for cost analysis and schema automation, applying cost optimization strategies (e.g., Parquet, partitioning, query limits). Mastered DynamoDB fundamentals, including primary/composite keys, indexes (GSI, LSI), consistency models, and capacity modes. Enhanced translation and comprehension skills by translating AWS technical blogs on AI, MLflow, and HPC, deepening understanding of SageMaker, Batch, ParallelCluster, and Connect. Participated in an AWS event focused on the AI Development Lifecycle and Kiro, gaining insights into end-to-end model management, versioning, and deployment best practices. Improved technical vocabulary and practical knowledge in cloud computing, AI model governance, and data-driven architecture design within the AWS ecosystem. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Project Initiation: Define project scope, analyze requirements, and develop a development plan for the Personal Finance Management App v2 focusing on Microservices. .NET \u0026amp; AWS Integration: Research and implement AWS SDK integration within a .NET Microservices architecture. Networking Mastery: Gain deep understanding of VPC architecture, network security, and hybrid connectivity (VPN, DNS). Compute Fundamentals: Master core Amazon EC2 concepts for optimal compute resource selection and management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Proposal Research \u0026amp; AWS SDK with .NET - Project Proposal: Conducted group discussions to evaluate feasibility and select a Microservices-focused project topic. - Tech Stack Integration: Researched AWS SDK integration for .NET applications. - Coding Practice: Wrote C# code snippets for basic AWS operations: uploading data to S3, querying DynamoDB, and invoking Lambda functions. 06/10/2025 06/10/2025 2 Project Planning \u0026amp; Cost Estimation - Requirements Analysis: Analyzed requirements for the Personal Finance Management App v2. - Agile Planning: Defined 20 User Stories across 8 Epics (Wallet, Transaction, AI Voice, OCR, etc.) and planned 2 Sprints. - Cost Estimation: Selected necessary AWS services (Cognito, Transcribe, SQS\u0026hellip;) and estimated operational costs (Free Tier vs. Paid) for budget optimization. 07/10/2025 07/10/2025 Project Documentation 3 AWS Networking: VPC \u0026amp; Site-to-Site VPN (Lab 03) - VPC Architecture: Configured Custom VPC, Subnets, Route Tables, Internet Gateway (IGW), and NAT Gateway. - Network Security: Differentiated and configured Security Groups (Stateful) vs. Network ACLs (Stateless). - Hybrid Connectivity: Set up AWS Site-to-Site VPN (VGW, CGW) and monitored connectivity using VPC Flow Logs and Reachability Analyzer. 08/10/2025 08/10/2025 Module Re-hand-on 4 Amazon EC2 \u0026amp; Compute Infrastructure Management - EC2 Fundamentals: Researched Instance types (T, M, R series) and selected appropriate hardware configurations (Intel/AMD/Graviton). - Image Management: Learned about AMI (Amazon Machine Images) for standardized server environments. - Backup Strategy: Designed data backup strategies using EBS Snapshots. 09/10/2025 09/10/2025 Module Review 5 Hybrid DNS with Route 53 Resolver - Hybrid DNS Concepts: Studied unified domain resolution models between On-premise and AWS environments. - Route 53 Resolver: Configured Outbound Endpoints (forwarding queries from AWS) and Inbound Endpoints (receiving queries from on-premise). - Resolver Rules: Defined DNS forwarding rules for specific domains. 10/10/2025 10/10/2025 Module Review Week 5 Achievements: Project Plan Finalized: Completed User Stories (20 stories/8 Epics), Sprint planning, and AWS infrastructure cost estimation for the FinTech project. Technical PoC: Successfully wrote C# sample code integrating AWS SDK (S3 data upload, DynamoDB retrieval). Advanced Networking Config: Successfully configured Custom VPC with Public/Private subnets and Site-to-Site VPN connectivity. Hybrid DNS Architecture: Designed and understood the hybrid DNS resolution flow between On-premise and AWS using Route 53 Resolver. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Containerization Mastery: Master the process of packaging and operating applications on Container platforms (Docker) and orchestrating at scale with Amazon ECS. Automation \u0026amp; Optimization: Automate the software development lifecycle (CI/CD) and optimize global user experience via CDN. Decoupled Architecture: Understand and implement Event-driven/Decoupled architectures using AWS messaging and queuing services. Modern Backend Development: Build high-performance, secure, and maintainable APIs using the modern FastAPI framework (Python). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Containerization \u0026amp; Orchestration (Docker \u0026amp; ECS) - Docker Deployment: Packaged full-stack applications (Frontend/Backend/DB) into containers. Managed Images with Docker Hub and Amazon ECR. Configured Nginx as a Reverse Proxy. - Amazon ECS: Orchestrated containers at scale. Differentiated between EC2 Launch Type and AWS Fargate (Serverless). - Deployment Strategies: Practiced deployment strategies on ECS: Rolling Update (minimize downtime) and Blue/Green (safe traffic shifting). 13/10/2025 13/10/2025 Module Re-hand-on 2 Content Delivery \u0026amp; CI/CD Automation - CDN Optimization (Lab 9.4): Integrated Amazon CloudFront with S3 Static Hosting, reducing global latency (from ~200ms to ~30ms) and setting up SSL/Custom Domain security. - Serverless CI/CD (Lab 8.4): Built automated workflows with AWS CodePipeline. + CI: Automated code build and test (CodeBuild). + CD: Automated artifact deployment to Staging/Production environments (CodeDeploy). 14/10/2025 14/10/2025 Module Re-hand-on 3 Decoupled Architecture (SQS \u0026amp; SNS) - Amazon SQS (Message Queue): Designed a distributed order processing system using Standard Queues (high throughput) and FIFO Queues (guaranteed ordering) to decouple system components. - Amazon SNS (Pub/Sub): Configured asynchronous notification mechanisms (Fan-out pattern) to multiple endpoints (Lambda, SQS, Email/SMS). - Integration: Combined SNS and SQS to build a reliable and highly scalable Event-driven architecture. 15/10/2025 15/10/2025 Module Re-hand-on 4 Modern API Development with FastAPI (Part 1) - Framework Fundamentals: Got started with FastAPI (Python), leveraging ASGI architecture for high performance and automatic documentation (Swagger UI/ReDoc). - Modular Design: Structured the project using APIRouter to separate modules (Users, Items, Auth) for easier maintenance. - Data Validation: Used Pydantic schemas to define and validate input/output data automatically. - Database Setup: Integrated SQLAlchemy, established database connections, and managed Sessions. 16/10/2025 16/10/2025 5 Advanced FastAPI Implementation (Part 2) - Security \u0026amp; Auth: Implemented secure authentication mechanisms using OAuth2 with Password Flow and JWT Tokens (Access/Refresh tokens). - Database Operations: Built full CRUD APIs, handling data relationships in SQLAlchemy. - Testing \u0026amp; CI/CD: Wrote Unit/Integration tests with pytest. Set up pipelines (GitHub Actions) to automatically test and deploy the application (Zero-downtime). 17/10/2025 17/10/2025 Week 6 Achievements: ECS Proficiency: Successfully containerized and deployed a Full-stack application on Amazon ECS, mastering deployment strategies (Rolling/Blue-Green). DevOps Optimization: Successfully built a serverless CI/CD pipeline and reduced application access latency from 200ms to 30ms using CloudFront. Reliable Architecture: Designed an asynchronous message processing system, ensuring decoupling and scalability between components. Secure API Construction: Completed a robust Backend with FastAPI including full features: JWT Authentication, ORM Database connection, and automated testing. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Review and reinforce theoretical knowledge of core AWS services and architectural best practices. Practice AWS Cloud Practitioner exam questions daily to improve service recognition and scenario analysis. Strengthen understanding of AWS security, network design, storage strategies, compute options, and database availability models. Build the ability to confidently select correct AWS services based on business requirements, cost considerations, and operational expectations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Cloud Concepts \u0026amp; Well-Architected Framework - Cloud Value: Studied cloud benefits: CapEx to OpEx shift, Elasticity, and rapid innovation. - Performance Services: Differentiated use cases: CloudFront (Content caching), Global Accelerator (Routing via AWS backbone), and Route 53 (DNS Management). - Well-Architected Framework: Reviewed 6 pillars (Operational Excellence, Security, Reliability, Performance, Cost, Sustainability) and related keywords. - Infrastructure as Code: Understood CloudFormation (Automated deployment) and AWS Service Catalog (Service governance). 20/10/2025 20/10/2025 Module Review 2 Networking, VPC Components \u0026amp; Hybrid Connectivity - VPC Structure: Reviewed CIDR, Subnet isolation, and Public/Private network planning. - Hybrid Connectivity: Compared AWS Site-to-Site VPN (Internet, encrypted) vs. AWS Direct Connect (Dedicated, low latency). - Gateways: Differentiated Internet Gateway (Inbound/Outbound for Public subnet) and NAT Gateway (Outbound only for Private subnet). - Security Layers: Compared Security Groups (Stateful, Instance-level) and Network ACLs (Stateless, Subnet-level). - Routing: Studied Route 53 policies: Failover, Weighted, Geolocation. 21/10/2025 21/10/2025 Module Review 3 Databases, High Availability \u0026amp; Migration - Amazon RDS: Differentiated Multi-AZ (High Availability/Disaster Recovery) vs. Read Replicas (Scaling read workloads). - Security: Learned Database encryption (KMS) and IAM Auth for MySQL/PostgreSQL. - Migration Tools: Used DMS for continuous data replication and SCT for schema conversion between different engines. - Use Cases: Differentiated DynamoDB (NoSQL, micro-second latency) and Redshift (Analytics, Petabyte-scale). 22/10/2025 22/10/2025 Module Review 4 S3 Storage Architecture \u0026amp; Security Controls - Storage Classes: Selected optimal storage classes: Standard, Intelligent-Tiering (auto cost-opt), Glacier/Deep Archive (long-term). - Storage Comparison: Differentiated S3 (Object), EFS/FSx (Shared File), and EBS (Block storage for EC2). - Access Control: Differentiated IAM Policies (User-based) and S3 Bucket Policies (Resource-based). Configured Block Public Access. - Disaster Recovery: Explored Cross-Region Replication (CRR) for regional data resilience. 23/10/2025 23/10/2025 Module Review 5 Compute, Pricing \u0026amp; Operational Efficiency - EC2 Pricing Models: Selected pricing models: On-Demand (short-term, unpredictable), Reserved/Savings Plans (long-term, steady), Spot (flexible, cheap). - Architecture: Designed HA architecture with Elastic Load Balancer combined with Auto Scaling Group. - Serverless: Reviewed AWS Lambda for event-driven tasks and operational cost optimization. - Operational Tools: Used EC2 Image Builder for standardized AMIs and Compute Optimizer for resource recommendations. 24/10/2025 24/10/2025 Module Review Week 7 Achievements: Completed 2‚Äì3 AWS Cloud Practitioner practice exam sets every day with consistent improvement in performance. Successfully reviewed theory and real-business scenarios involving key AWS domains including Compute, Storage, Networking, Security, and Databases. Improved ability to differentiate AWS services with similar purposes such as: Elastic Load Balancing vs Route 53 vs Global Accelerator, S3 storage classes vs EBS vs EFS, Multi-AZ vs Read Replicas in RDS. Fully understood the theoretical foundation behind cost optimization options (On-Demand, Reserved Instances, Savings Plans, Spot Instances). Strengthened the theoretical knowledge of identity and access control: IAM users, roles, policies, MFA, least privilege principle. Enhanced comprehension of the AWS Well-Architected Framework theory and its six pillars for exam requirement questions. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Master Serverless Concepts: Deep dive into AWS Lambda theory, event-driven mechanisms, and the importance of Lambda Layers. Explore Generative AI: Gain a theoretical overview of Amazon Bedrock, Foundation Models (FMs), and basic use cases. API Architecture: Understand Amazon API Gateway theory and its critical role in Microservices architecture. Hands-on Lab (Weekend): Successfully build an automated image processing pipeline: Upload to S3 -\u0026gt; Trigger Lambda (w/ Pillow Layer) -\u0026gt; Resize Image -\u0026gt; Save to destination S3 bucket. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 AWS Lambda Core Concepts (Research) - Execution Model: Studied the Lambda lifecycle (Init, Invoke, Shutdown) and the \u0026ldquo;Cold Start\u0026rdquo; phenomenon. - Resource Model: Understood the relationship between Memory and CPU (higher RAM = proportional CPU power). - Billing: Researched pricing models: Request count + Compute duration (GB-seconds). - Concurrency: Differentiated between Reserved Concurrency (guaranteed resources) and Provisioned Concurrency (eliminated cold starts). 27/10/2025 27/10/2025 AWS Lambda Docs 2 Amazon Bedrock \u0026amp; GenAI Overview (Research) - Key Concepts: Explored Foundation Models (Claude, Titan, Stable Diffusion), Tokens, and Inference parameters (Temperature, Top P). - Bedrock Features: Researched theoretical concepts of Knowledge Bases (RAG) for custom data and Agents for task execution. - Security: Reviewed Bedrock\u0026rsquo;s data privacy commitments (customer data is not used to retrain AWS base models). - Use Cases: Read case studies on text summarization and image generation. 28/10/2025 28/10/2025 Bedrock User Guide 3 API Gateway Fundamentals (Research) - Architecture: Studied the role of API Gateway: Authentication, Throttling, and Caching. - Types: Compared REST API (Feature-rich) vs. HTTP API (Low-cost/Low-latency) vs. WebSocket API (Real-time). - Integration: Deep dived into Lambda Proxy Integration (passing the raw event object directly to Lambda). - Endpoint Types: Differentiated between Edge-optimized (Global), Regional (Region-specific), and Private endpoints (VPC internal). 29/10/2025 29/10/2025 API Gateway Concepts 4 S3 Event Notifications \u0026amp; IAM Preparation (Pre-Lab) - Event Patterns: Investigated S3 Event Notifications mechanism (specifically s3:ObjectCreated:*) to trigger Lambda functions. - IAM Permissions: Drafted the necessary IAM Policy: Lambda requires s3:GetObject (Source bucket), s3:PutObject (Destination bucket), and logs:CreateLogGroup. - Library Research: Researched the Pillow (PIL) Python library for image manipulation and the process of packaging it into a Lambda Layer (since it\u0026rsquo;s not in the standard runtime). 30/10/2025 30/10/2025 S3 Event Notifications 5 Lab: Serverless Image Resizer (Implementation) - Lambda Layer: Created and uploaded a Layer containing the Pillow library (compatible with Python 3.x runtime). - Coding: Developed the Lambda function (Python/Boto3) to retrieve the image from the S3 event, resize it to a thumbnail (e.g., 128x128), and save it to the target bucket. - Configuration: Configured S3 Bucket Triggers to automatically invoke the function upon file upload to the /raw folder. - Testing \u0026amp; Debug: Uploaded test images, verified results in the destination bucket, and analyzed CloudWatch Logs to resolve import/permission errors. 31/10/2025 01/11/2025 Serverless Image Resizing Week 8 Achievements: Solidified Serverless \u0026amp; AI Theory: Gained a comprehensive understanding of the Serverless ecosystem (Lambda, API Gateway) and Generative AI trends (Bedrock) through deep documentation review. Mastered Lambda Environment: Recognized the critical importance of Lambda Layers when working with external dependencies like Pillow or Pandas, optimizing code efficiency. Implemented Event-Driven Architecture: Successfully built a functional, real-world application: a serverless Image Processing Pipeline. IAM \u0026amp; Debugging Skills: Applied \u0026ldquo;Least Privilege\u0026rdquo; principles for Lambda security and utilized CloudWatch Logs effectively for troubleshooting execution errors. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Complete the setup of the Microservice architecture using .NET Aspire and build all core Shared Libraries. Fully implement the User Service with AWS Cognito authentication integration and automatic user/profile creation logic. Fully implement the Wallet Service with balance management logic, Money Jar management, Goals Tracking, and Jar Expiry Processing mechanisms. Set up the API Gateway, configure Polly, and finalize Integration Testing for the End-to-End flow (User-Wallet). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Microservice Architecture \u0026amp; Shared Libraries Setup - Initialize the .NET Aspire solution and structure 6 Microservices (User, Wallet, Transaction, Report, Notification, AI Integration Service). - Configure Service Discovery and Docker Compose for the local development environment. - Build core Shared Libraries: Common.DTOs (with ApiResponse\u0026lt;T\u0026gt;), Common.Authentication (JWT), Common.Messaging (MassTransit/RabbitMQ), Common.Repositories (UoW Pattern), and Common.Middlewares (Rate Limiting, Correlation ID). 03/11/2025 03/11/2025 Day 1 Docs 2 Implement User Service \u0026amp; Authentication Integration - Set up PostgreSQL Database (user_service_db) and design the User Entity (using Guid.CreateVersion7(), Soft Delete Filter). - Integrate AWS Cognito authentication (JWT Bearer Authentication) and create middleware to extract CognitoSub. - Implement automatic user record creation logic upon first login. - Implement Core APIs: GET /api/users/me (Get/Create profile), PUT /api/users/me (Update), and POST /api/users/complete-profile. 04/11/2025 04/11/2025 Day 2 Docs 3 Wallet Service - Part 1: Wallet Management \u0026amp; Allocation Logic - Set up PostgreSQL Database (wallet_service_db) and design Wallet, MoneyJar, BudgetPeriod Entities. - Implement Message Consumer to automatically create Wallet upon receiving the UserCreated event, and simultaneously create the default \u0026ldquo;Income\u0026rdquo; jar. - Implement core Wallet APIs: Deposit, Withdraw (from Available Balance only), Reconcile (Synchronize total balance), and Soft Delete/Archive Wallet. - Implement basic Money Jar CRUD APIs. 05/11/2025 05/11/2025 Day 3 Docs 4 Wallet Service - Part 2: Money Jar, Goals \u0026amp; Budget - Complete CRUD for Money Jar, including logic to recalculate AvailableBalance upon updating AllocatedAmount. - Integrate Goals Tracking (calculate ProgressPercent) and GoalAchieved event publishing logic. - Implement Jar Operations: Deposit, Withdraw, Transfer between jars, and Reconcile Jar Balance. - Implement Budget Threshold \u0026amp; Over-Budget Logic (publish warning events) and Jar Expiry Processing Service (handle Income Jar auto-reset, Expense Jar Grace Period/Auto-Return). 06/11/2025 06/11/2025 Day 4 Docs 5 End-to-End Integration \u0026amp; Testing - Set up API Gateway (Routing, CORS, Rate Limiting). - Configure Service-to-Service communication with HTTP Client Factory and Polly (Retry Policy, Circuit Breaker). - Perform Integration Testing for the End-to-End flow: New User Onboarding, Jar Creation, Wallet/Jar Operations (Deposit/Withdraw/Transfer/Reconcile), and Goal Achievement. - Test Security (JWT Expiration, Authorization) and Concurrency (Balance Updates). 07/11/2025 07/11/2025 Day 5 Docs Week 9 Achievements: Microservice Platform Architecture Established: Used .NET Aspire to shape the Microservice architecture for 6 services. Completed the core set of Shared Libraries, including MassTransit/RabbitMQ configuration. User Service Completed: Successfully integrated AWS Cognito for the JWT authentication mechanism. Implemented automatic user profile creation logic and the profile completion API. Wallet Service Completed: Implemented three-component balance management logic: TotalBalance, AllocatedBalance, AvailableBalance. Completed Money Jar Management with Goals Tracking (Target, Progress) and GoalAchieved event publishing. Complex Business Logic Implemented: Finalized operations such as Wallet/Jar Reconciliation (Balance Synchronization) and Jar Expiry Processing (allowing Income Jar auto-reset and Expense Jar with a grace period). Stable and Integrated System: API Gateway and the fault tolerance mechanism Polly (Retry, Circuit Breaker) have been configured. The entire main business flow has been successfully Integration Tested (from Cognito -\u0026gt; User Service -\u0026gt; Wallet Service). "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/5.1.1-whatisrag/","title":"What is Retrieval-Augmented Generation (RAG)","tags":[],"description":"","content":"Brief Definition RAG (short for Retrieval-Augmented Generation) is a technique or software architecture in the field of Artificial Intelligence (AI), designed to optimize the output of a Large Language Model (LLM).\nIn essence, RAG is a combination of two mechanisms:\nInformation Retrieval Mechanism: Searching for data from a highly reliable External Knowledge Base. Text Generation Mechanism: Using the LLM\u0026rsquo;s language understanding and synthesis capabilities to generate natural responses. The goal of RAG is to provide the LLM with accurate, up-to-date, and specific context, helping the model overcome the limitations of static training data.\nWhy is RAG needed? Traditional LLM models often face 3 major problems that RAG can solve:\nInformation Updates (Freshness): The LLM does not need Re-training or Fine-tuning yet can still answer the latest information, simply by updating the search database. Data Ownership (Proprietary Data): Allows AI to answer questions regarding private enterprise data (internal documents, code base, customer information) that the original model does not know. Authenticity (Grounding): Minimizes \u0026ldquo;Hallucination\u0026rdquo; (AI fabricating information) by forcing the AI to cite or rely on actual text passages found. Operational Architecture The process of handling a question in RAG proceeds as follows:\nStep Name Action Description 1 Retrieval (Truy xu·∫•t) The system searches for text segments most relevant to the question in the data repository (usually using a Vector Database). 2 Augmentation (TƒÉng c∆∞·ªùng) Combine the user\u0026rsquo;s question + The data just found into a complete \u0026ldquo;prompt\u0026rdquo;. 3 Generation (T·∫°o sinh) Send that prompt to the AI (LLM) for it to synthesize and write out the final answer for the user. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: AWS Foundations, Networking, Security, and Hands-on Labs\nWeek 2: Hands-on with AWS Compute (EC2, Auto Scaling, Load Balancing), Monitoring (CloudWatch), Backup, and Storage Services\nWeek 3: AWS Security \u0026amp; Database Services (IAM, Cognito, KMS, RDS, Aurora, Redshift, ElastiCache)\nWeek 4: AWS Data Analytics, NoSQL, and AI Lifecycle Exploration Week\nWeek 5: Project Initialization, .NET Integration \u0026amp; Hybrid Network Architecture (VPC/DNS)\nWeek 6: Deploy Decoupled Architecture, ECS Orchestration \u0026amp; Secure API\nWeek 7: Strengthen Infrastructure Architecture: Network, Storage, Database \u0026amp; Compute\nWeek 8: Serverless Architecture, Generative AI Concepts, and Automated Image Processing Lab\nWeek 9: Building .NET Aspire Platform, Integrating AWS Cognito \u0026amp; Wallet Management Logic\nWeek 10: Developing Advanced Transaction Logic \u0026amp; Integrating RabbitMQ/MassTransit\nWeek 11: Reporting System, Alerts \u0026amp; Automated Processing\nWeek 12: System Optimization, Enhanced Security \u0026amp; Deployment Readiness\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/5.3.2-sync-data/","title":"Check Vector Store and Data Synchronization","tags":[],"description":"","content":"Target Before the AI can answer, data must be ingested into the vector storage (Vector Store). We will perform a \u0026ldquo;Before and After\u0026rdquo; check to clearly see how Bedrock automatically encodes and stores data into OpenSearch.\nImplementation Steps Step 1: Check Vector Store (Empty State)\nWe will directly access Amazon OpenSearch Serverless to confirm that no data exists yet.\nIn the AWS Console search bar, type Amazon OpenSearch Service and select Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. Click on the Collection name newly created by Bedrock (usually named like bedrock-knowledge-data...). On the Collection details page, click the Open Dashboard button (located at the top right of the screen).\nNote: If asked to log in, use your current AWS credentials. In the OpenSearch Dashboard interface: Click the Menu (3 horizontal lines) icon in the top left corner. Select Dev Tools (usually located at the bottom of the menu list). In the Console pane (on the left), enter the following command to check data: GET _search { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } } Click the Play (Run) button (small triangle next to the command line).\nResult: Observe the right pane, hits -\u0026gt; total -\u0026gt; value is 0.\nStep 2: Sync Data\nNow we will trigger Bedrock to read files from S3 and load them into OpenSearch.\nReturn to the Amazon Bedrock tab on the browser. Select Knowledge bases in the left menu and click on the KB name you just created. Scroll down to the Data source section, check the box (tick) next to the data source name (s3-datasource). Click the Sync button (Orange). Wait: This process will take 5 - 10 minutes depending on the sample document size. Wait until the Sync status column changes from Syncing to Available. Step 3: Re-check Vector Store (Populated)\nAfter Bedrock reports Sync completion, we return to the repository to verify the data has been successfully ingested.\nSwitch to the OpenSearch Dashboard tab (still open from Step 1). In Dev Tools, click the Play (Run) button again with the old command: GET _search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } } Result: The hits -\u0026gt; total -\u0026gt; value section will be greater than 0 (e.g., 10, 20\u0026hellip; depending on the number of text chunks). You will see details of the vectors (number arrays) and text content stored in the _source field. Congratulations! You have completed building the \u0026ldquo;brain\u0026rdquo; for the AI. The data has been encoded and sits safely in the Vector Database, ready for retrieval.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/5.2.2-prepare-data/","title":"Prepare source data","tags":[],"description":"","content":"Overview Initialize an object storage (S3 Bucket) to hold original documents (PDF, Word, Text). This acts as the \u0026ldquo;Source of Truth\u0026rdquo; that the Knowledge Base will access to read, analyze, and synchronize knowledge for the AI. You can store knowledge related to your field to use in creating your own personal assistant or Chatbot.\nData Preparation We will create an S3 Bucket to store original documents, acting as the knowledge source for the Chatbot.\nStep 1. Create S3 Bucket\nAccess the S3 service from the search bar. AWS Region: Select United States (N. Virginia us-east-1). Click Create bucket. Configure Bucket information: Bucket Type: Click General purpose Bucket name: Enter rag-workshop-demo Object Ownership: Keep default ACLs disabled. Block Public Access settings: Keep default (Selected Block all public access). Scroll to the bottom of the page, Click Create bucket. Check that the S3 Bucket has been created successfully. Step 2. Upload sample documents\nThis is a sample document, relevant to overview of AWS cloud computing knowledge. You can use it to run demos or upload your data. PDF format file\nIn the Buckets list, Click on the bucket name you just created. Click Upload. In the Upload interface:\nClick Add files. Select the sample document file attached above or a file from your computer (PDF or Word file with a lot of text is recommended). Scroll to the bottom of the page, Click Upload.\nWhen you see the green \u0026ldquo;Upload succeeded\u0026rdquo; notification, Click Close. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Objectives Before building the application, we need to establish a solid foundation. Similar to preparing ingredients before cooking, this section ensures your AWS account is ready with the necessary permissions and data.\nIn this section, we will complete 3 key initialization objectives:\nSelect Region: Set up the working environment in the United States N. Virginia (us-east-1) region to optimize connection speed and ensure service availability. Enable Model (Model Access): Check and ensure the account has permission to invoke the Anthropic Claude 3 model ‚Äì the main linguistic \u0026ldquo;brain\u0026rdquo; of the system. Prepare Data (Data Setup): Initialize storage (S3 Bucket) and upload source documents to serve the knowledge ingestion process later. Key Components In this preparation section, we will interact with the following components:\nAWS Management Console (Region Selector): The general management interface to switch the working Region to United States N. Virginia. Amazon Bedrock (Model Access \u0026amp; Playground): The place to manage access to Foundation Models and the chat tool to quickly test AI response capabilities. Amazon S3 (Simple Storage Service): Object storage service where we will create a Bucket to hold original document files (PDF, Word, Text). Implementation Steps Check Model Access Prepare Source Data "},{"uri":"https://danielleit241.github.io/aws-fcj-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Personal Finance Management App (Vicobi) You can read the full proposal here: Vicobi Proposal 1. Executive Summary The Vicobi (Personal Finance Management App) project aims to provide an intelligent, modern, and highly automated personal financial management platform. Vicobi simplifies financial management through 4 main pillars:\nSmart Recording: Voice input and invoice scanning, eliminating manual data entry barriers. Goal-based Budgeting: Automated creation and flexible management of money jars. Analysis \u0026amp; Control: Provides visual reports and intelligent alert systems. AI Financial Assistant (Chatbot): Integrates AI Chatbot acting as an advisor, supporting inquiries and enhancing financial knowledge. From a technical perspective, Vicobi is built on a Microservices architecture using .NET Aspire and FastAPI, deployed on AWS Cloud, ensuring flexibility and data security. The development process follows the Agile/Scrum model (2 weeks/sprint during the main development phase), with the goal of completing MVP within 2 months of execution.\n2. Problem Statement Current Problem In today\u0026rsquo;s dynamic market, users face difficulties in controlling finances due to \u0026ldquo;behavioral inertia\u0026rdquo; ‚Äî reluctance to manually record each transaction. Existing applications (like Money Lover, Misa Money Keeper) still rely heavily on manual input, causing \u0026ldquo;input fatigue\u0026rdquo; and high abandonment rates.\nSolution Vicobi solves the problem through high automation of the data entry process using AWS Cloud and Microservices:\nCore Technology: Integrates AI for Vietnamese voice processing (Voice-to-Text) and detailed invoice recognition (OCR). Optimized Architecture: Uses AWS ECS Fargate running Multi-container Task model (combining .NET Backend and AI Service) to reduce infrastructure costs while ensuring seamless communication. Modern Frontend: Uses Next.js hosted on Amazon S3 and distributed globally via Amazon CloudFront. Benefits and Return on Investment (ROI) The solution provides clear competitive advantages:\nUser Value: Reduces over 70% manual operations. Voice recognition accuracy reaches 90% and invoice extraction reaches 80%. Economic Efficiency: Maximizes AWS Free Tier usage (S3, CloudFront, Cognito). Lean operating budget around ~$60/month for infrastructure and ~$15/month for AI compute. ROI: Expected to achieve ROI within 6‚Äì12 months thanks to time savings and increased efficiency. Scalability: Microservices architecture ready for Mobile App integration or Open Banking. 3. Solution Architecture The system is designed with a distributed Microservices model, using API Gateway as the single entry point.\nTech Stack Details: Component Technology Details Frontend Next.js 16 App Router, TypeScript, Tailwind CSS, Zustand, React Query. Backend Core .NET Aspire Orchestrates Microservices (User, Wallet, Transaction, Report, Notification). AI Service FastAPI (Python) Handles Voice (PhoWhisper), OCR (Bedrock), Chatbot (RAG). Database Polyglot PostgreSQL, MongoDB, Elasticsearch, Qdrant (Vector DB). Messaging RabbitMQ Asynchronous communication between services. AWS Workflow: Access: Users access via Route 53, protected by AWS WAF and accelerated by CloudFront. Authentication: Amazon Cognito manages identity and issues JWT Tokens. API Processing: Requests go through API Gateway, connecting securely via AWS PrivateLink to Application Load Balancer (ALB). Compute: ALB distributes load to containers in ECS Fargate (located in Private Subnet). DevOps: CI/CD process fully automated by GitLab, builds images pushed to Amazon ECR and updates tasks on ECS. 4. Technical Deployment Implementation Phases The project spans 4 months (including internship):\nMonth 0 (Pre-internship): Ideation and overall planning. Month 1 (Foundation): Learn AWS, upgrade .NET/Next.js/AI skills. Set up VPC, IAM. Month 2 (Design): Design High-level \u0026amp; Detailed architecture on AWS. Month 3-4 (Realization): Coding, Integration Testing, Deploy to AWS Production, set up Monitoring. After Month 5: Research and develop Mobile App. Detailed Technical Requirements: Frontend: Deploy Next.js 16 on S3 + CloudFront. Use Origin Access Control (OAC) to secure bucket. Backend: Use .NET Aspire to manage Cloud-native configuration. Database-per-service: PostgreSQL \u0026amp; MongoDB. Elasticsearch for complex transaction search. Background Jobs: Use Hangfire. AI Service Pipelines: Voice: Preprocessing with Pydub, PhoWhisper-small Model (VinAI) for Vietnamese. OCR: Amazon Bedrock (Claude 3.5 Sonnet Multimodal) to accurately extract invoice information. Chatbot (RAG): Knowledge Base stored in Qdrant, generates responses via Amazon Bedrock (Claude 3.5 Sonnet). Security: Data encryption in transit (HTTPS/TLS 1.2+) and at rest (AES-256). Secrets management not deeply integrated (currently at MVP level), will upgrade to AWS Secrets Manager in the future. 5. Timeline \u0026amp; Milestones (Sprints) The main execution phase is divided into 4 Sprints:\nSprint 1: Core Foundation Authentication (Cognito), Wallet Management, Spending Jars. Sprint 2: Core Features Transactions (CRUD), AI Voice Processing. Sprint 3: Analytics Reports/Charts, Notification System (SES), Message Broker. Sprint 4: Stabilization Integration Testing, UI Refinement, Deploy to AWS ECS \u0026amp; CloudFront. Testing \u0026amp; Go-live: Domain Configuration, SSL, Monitoring Dashboard, UAT and project defense. 6. Budget Estimation Based on detailed cost estimates for the MVP phase.\nYou can review the detailed cost estimation by downloading the following files: üìä CSV Pricing File üíæ JSON Pricing File\nAWS Service Component / Usage Cost (USD/month) Elastic Load Balancing Application Load Balancer $18.98 Amazon ECS Fargate (vCPU \u0026amp; Memory) $17.30 Amazon VPC VPC Endpoints \u0026amp; NAT $10.49 AWS WAF Web ACL \u0026amp; Requests $7.20 Amazon API Gateway API Calls \u0026amp; Data Transfer $2.50 Amazon CloudFront Data Transfer Out $2.00 Amazon ECR Storage $1.00 Amazon Route 53 Hosted Zones $0.54 Amazon S3 Standard Storage $0.34 TOTAL AWS COST ~$60.35 Other Costs:\nCategory Details Cost (USD/month) AI Compute / Tooling Gemini API, Amazon Bedrock ~$15.00 PROJECT TOTAL ~$75.35 / month (Based on On-Demand pricing in Singapore region - ap-southeast-1)\n7. Risk Assessment Main Risks: User information leakage (Impact: High), AWS Region connection loss (Impact: High), AI misrecognition (Impact: Medium). Mitigation Strategies: Security: AES-256 encryption, HTTPS, IAM Least Privilege, AWS WAF. High Availability: Multi-AZ deployment for ECS and ALB. AI: Continuously improve model with real data. Resilience: Use internal RabbitMQ for asynchronous processing and retry. Disaster Recovery Plan: Use IaC (Infrastructure as Code) for rapid infrastructure restoration. 8. Expected Results \u0026amp; Team Expected Results of the Project Automated financial data entry: The application helps users avoid manual entry, just take a photo of the invoice or record a voice for the system to automatically classify spending. Intuitive financial management: Users can view spending charts, monthly reports, and receive savings suggestions based on consumer behavior. Minimal user experience: Friendly web interface, modern design, optimized for mobile devices and suitable for people new to financial management. Stable, scalable system: Microservices architecture makes it easy to add new features such as spending reminders, AI predictive analysis, or expand to a mobile app. Improving development team skills: Project members have practical access to DevOps processes, CI/CD implementation, and cloud-based application optimization. Project Limitations Vietnamese AI model is still limited: The ability to recognize regional voices or handwritten invoices has not yet achieved high accuracy. No separate mobile application: The MVP version only supports the web platform, there is no native mobile app. Implementation Team: Name Role Email Le Vu Phuong Hoa Backend Developer (Leader) hoalvpse181951@fpt.edu.vn Nguyen Van Anh Duy AI Developer (Member) duynvase181823@fpt.edu.vn Uong Tuan Vu Frontend Developer (Member) vuutse180241@fpt.edu.vn Tran Huynh Bao Minh AI Developer (Member) baominhbrthcs@gmail.com Mentor Support:\nNguyen Gia Hung - Head of Solution Architects Van Hoang Kha - Cloud Security Engineer "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.1-workshop-overview/5.1.2-services/","title":"Services","tags":[],"description":"","content":"The solution architecture is built upon the coordination of the following 4 key service components:\nKnowledge Bases for Amazon Bedrock This is a fully managed capability that helps connect Foundation Models to the enterprise\u0026rsquo;s internal data sources.\nRAG workflow automation: Manages the entire end-to-end workflow, including ingestion, chunking, embedding, and retrieval. Contextual connection: Enables AI applications to answer questions based on private data rather than relying solely on generic training data. No infrastructure management: Eliminates the need to build and maintain complex data pipelines. Amazon Simple Storage Service (Amazon S3) An object storage service with scalability, 99.999999999% (11 nines) data durability, and top-tier security.\nData Source Role: Acts as the \u0026ldquo;source of truth\u0026rdquo;. Document storage: Contains unstructured files such as PDF, Word, or Text that the business wants the AI to learn. Synchronization: The Knowledge Base will periodically scan this S3 bucket to synchronize and update the latest knowledge. Amazon OpenSearch Serverless A serverless deployment option for Amazon OpenSearch Service that helps run search and analytics workloads without managing clusters.\nVector Store Role: Stores vector embeddings generated from original documents. Semantic Search: Performs similarity search algorithms (k-NN) to identify text segments with meanings closest to the user\u0026rsquo;s question. Auto-scaling: Automatically adjusts compute and storage resources based on actual demand. Amazon Bedrock Foundation Models (FMs) Provides access to leading AI models via a unified API. In this architecture, we use two types of models with distinct roles:\nEmbedding Model (Amazon Titan Embeddings v2): Converts text (documents from S3 and user questions) into numerical vectors. Enables computers to compare semantic similarity between text segments. Text Generation Model (Anthropic Claude 3): Acts as the reasoning \u0026ldquo;brain\u0026rdquo;. Receives the question along with contextual information retrieved from the Vector Store. Synthesizes information and generates natural, accurate answers with source citations. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Complete the implementation of Transaction Service (Create, Read, Update, Delete) with a tightly controlled money flow via Wallet Service (balance check and deduction). Implement advanced Transaction features: Advanced Filtering, Full-Text Search, Tags, and AI Metadata support. Fully establish the Event-Driven Architecture (EDA) using RabbitMQ/MassTransit for data synchronization across services (User, Wallet, Transaction). Finalize Integration Testing and performance optimization for core business flows. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Transaction Service Foundation \u0026amp; Database Setup - Initialize Transaction Service, set up PostgreSQL Database with necessary Entities (Transaction, TransactionTag, etc.). - Configure HTTP Client Factory with Polly (Retry/Circuit Breaker) for communication with Wallet Service. 10/11/2025 10/11/2025 Day 6 Docs 2 Transaction CRUD APIs \u0026amp; Wallet Integration - Implement full CRUD APIs (Create, Read, Update, Delete) for transactions. - Ensure Atomic operations in the creation flow: Deduct Balance via HTTP Client, then save the record to DB. - Implement Soft Delete logic with automatic refund mechanism to the Jar. - Implement Basic Statistics APIs (total expenses, daily spending). 11/11/2025 11/11/2025 Day 7 Docs 3 Advanced Filtering, Tags \u0026amp; Budget Support - Complete the GET /api/transactions API with Advanced Filtering (by Date Range, Jar ID, Category ID, Amount Range) and Full-Text Search (search=). - Implement Transaction Tags system (Add/Remove tags) and API to retrieve tag lists. - Add a support API for Budget Checking for the Wallet Service. 12/11/2025 12/11/2025 Day 8 Docs 4 Event-Driven Architecture \u0026amp; Service Integration - Set up RabbitMQ and MassTransit across services. - Implement Consumers in Wallet Service to handle the TransactionCreated event (update Jar balance). - Implement Consumers in Transaction Service to auto-create transactions from Wallet events (e.g., WalletDeposited, WalletWithdrawn). - Ensure Idempotency for all Consumers. 13/11/2025 13/11/2025 Day 9 Docs 5 Sprint 2 Testing, Bug Fixes \u0026amp; Polish - Conduct End-to-End Integration Testing for the entire business flow (Registration -\u0026gt; Wallet -\u0026gt; Jar -\u0026gt; Transaction -\u0026gt; Budget Alerts). - Load Test with 50 concurrent requests. - Performance Optimization (add DB Indexes, Caching). - Update Documentation (Swagger, Postman, README). 14/11/2025 14/11/2025 Day 10 Docs Week 10 Achievements: Transaction Service Completed: Full CRUD implemented, tight money flow control, and automatic refund mechanism on deletion. Advanced Feature Support: Effective multi-dimensional filtering and searching integrated. Support for Transaction Tags and storing AI Metadata (JSONB). Stable Event-Driven Architecture: RabbitMQ/MassTransit is operational, ensuring Jar balance synchronization between Transaction and Wallet Services. Successfully handled events for automatic transaction creation. Stable and Tested System: The entire Sprint was covered by Integration Testing and performance optimization was completed. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Completed the deployment of the Report Service using CQRS architecture to separate read/write flows, ensuring high performance for complex reports. Implemented advanced analytics features: Spending Trends, Budget vs. Actual Comparison, and multi-sheet Excel Report Export. Built a comprehensive Notification Service supporting multi-channel delivery: Email (via AWS SES) and In-App (via SignalR), allowing users to customize notification preferences. Established the Period End Automation process to handle surplus funds and prepare the Backend for AI (OCR/Voice) integration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Report Service Foundation \u0026amp; CQRS Setup - Initialized Report Service, setup PostgreSQL (report_service_db) and Read Model entities (TransactionSummary, JarAnalytics). - Implemented CQRS pattern: Handled events from Transaction Service (TransactionCreated, Updated, Deleted) to update report data in real-time. - Built Basic Report APIs to retrieve period summaries and basic jar statistics. 17/11/2025 17/11/2025 Day 11 Docs 2 Advanced Analytics \u0026amp; Excel Export - Developed advanced analytics APIs: Spending trends (day/week/month), budget vs. actual comparison, and goal progress. - Integrated Excel library (EPPlus/ClosedXML) to export Multi-sheet reports (Transaction History, Jar Analytics, Category Analytics). - Configured Caching for reports to optimize query performance. 18/11/2025 18/11/2025 Day 12 Docs 3 Notification Service \u0026amp; Email Integration - Initialized Notification Service connected to MongoDB (UserPreferences, EmailHistory). - Integrated AWS SES and created email templates for critical alerts (Budget Exceeded, Low Balance, Goal Achieved). - Built Preferences management API allowing users to choose notification frequency (Daily/Weekly) and types. 19/11/2025 19/11/2025 Day 13 Docs 4 Period End Automation \u0026amp; In-App Alerts - Setup Job Scheduler (Hangfire/Quartz) to automate the period-end process: Calculate surplus, return to main wallet, and generate summary reports. - Implemented real-time In-App notification system via SignalR Hub. - Built APIs to mark as read (mark-as-read) and manage unread badge counts. 20/11/2025 20/11/2025 Day 14 Docs 5 AI Service Integration (Backend Side) - Updated Transaction Entity to include AI metadata fields (AiExtracted, ConfidenceScore, OriginalText). - Developed Batch Transaction API (POST /batch) to support processing multi-item OCR bills in a single call. - Updated processing flow so Backend receives user-reviewed data from Client instead of auto-creating from AI events. 21/11/2025 21/11/2025 Day 15 Docs Week 11 Achievements: Successfully operated the reporting system with the CQRS pattern, providing real-time analytics data. Excel Export feature works perfectly with detailed multi-sheets (Transaction History, Jar Analysis, Category Analysis). Multi-Channel Notification System: AWS SES integrated to send alert emails (Budget Exceeded, Low Balance). SignalR system stable for real-time in-app notifications. Automation \u0026amp; AI Readiness: Period-end processing job automated: accurately calculates surplus and returns it to the main wallet. Backend completed API support for Batch Transactions and storage of AI Metadata (Confidence Score, Original Text). "},{"uri":"https://danielleit241.github.io/aws-fcj-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Completed End-to-End System Integration Testing, ensuring complex business flows (registration, transactions, inter-jar transfers) operate smoothly. Performed comprehensive Performance Optimization: Database indexing, API Caching, and Load Testing supporting 100 concurrent users. Enhanced System Security: Conducted authentication/authorization audits, managed secrets via environment variables, and configured HTTPS. Packaging and Documentation: Finalized Dockerization of services, completed Swagger documentation, and prepared AWS deployment scripts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Integration Testing - Full System - Performed End-to-End flow testing: User Registration ‚Üí Wallet ‚Üí Budget ‚Üí Transaction ‚Üí Period End. - Tested Event-Driven system: Verified data integrity via RabbitMQ and error handling capabilities (retry, dead letter). - Tested Error Scenarios: Invalid tokens, DB/RabbitMQ connection failures. 24/11/2025 24/11/2025 Day 16 Docs 2 Performance Optimization - Database Optimization: Added Indexes for slow queries, configured Connection Pooling. - API Optimization: Implemented Response Caching, Gzip compression, and pagination for large datasets. - Conducted Load Testing (k6/JMeter): Simulated 50-100 concurrent users, ensuring response time \u0026lt; 2s. 25/11/2025 25/11/2025 Day 17 Docs 3 Security Hardening - Security Audit: Reviewed Authorization, validated inputs, and prevented SQL Injection. - Secrets Management: Moved all sensitive information to environment variables and .NET User Secrets. - HTTPS Configuration: Setup self-signed SSL for local dev and updated CORS policies. 26/11/2025 26/11/2025 Day 18 Docs 4 Bug Fixes \u0026amp; Code Quality - Bug Fixing: Focused on resolving P0 and P1 bugs discovered during testing. - Code Quality Improvements: Refactored complex methods, added XML documentation, and Unit tests for critical logic. - Enhanced Logging: Integrated Serilog with Structured Logging and Correlation IDs. 27/11/2025 27/11/2025 Day 19 Docs 5 Documentation \u0026amp; Deployment Prep - Finalized API Documentation: Updated Swagger with full request/response examples and exported Postman Collection. - Deployment Preparation: Created Dockerfiles for each service, docker-compose.yml. - Demo \u0026amp; Handover: Updated README, architecture diagrams, and conducted a full system demo for the team. 28/11/2025 28/11/2025 Day 20 Docs Week 12 Achievements: System Quality Assured: Passed Integration Tests covering both happy paths and error scenarios, including asynchronous event handling. Critical bugs (P0/P1) were resolved, achieving the Code Coverage goal of \u0026gt; 80%. Optimized Performance \u0026amp; Security: API response times met the target of \u0026lt; 2s thanks to Database optimization and Caching. The system is hardened with Rate limiting, Input validation, and strict CORS configuration. Deployment Ready: All 6 microservices have Dockerfiles and run stably in the local environment using docker-compose. Technical documentation (Architecture diagram, Swagger, Postman) is complete. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.3-knowledge-base/","title":"Create and Configure Knowledge Base","tags":[],"description":"","content":"Objectives After completing the environment and data preparation, the next step is to set up the core component of the RAG architecture. In this section, we will initialize the Knowledge Base, acting as an intelligent intermediary mechanism that connects unstructured data sources with the reasoning capabilities of foundation models.\nWe will accomplish 3 key technical objectives:\nEstablish an Automated Pipeline: Configure the Knowledge Base to automate the entire RAG data processing workflow (including extraction, text chunking, and vector creation) to eliminate manual processing tasks. Initialize Vector Store: Deploy a collection on Amazon OpenSearch Serverless to store semantic vectors, serving accurate and efficient information retrieval. Data Synchronization (Data Ingestion): Perform the initial data ingestion process, converting static documents from S3 into searchable vectors within the system. Key Components During this configuration process, we will interact with and connect the following services:\nKnowledge Bases for Amazon Bedrock: A managed service acting as the orchestrator of data flow, connecting information sources, and executing semantic queries. Amazon Titan Embeddings G1 - Text v2: A specialized model for converting text data into numerical vectors (Embeddings) with high accuracy and multi-language support. Amazon OpenSearch Serverless: A fully managed vector database responsible for storage and executing similarity search algorithms (k-NN). Implementation Steps Initialize Knowledge Base Check Vector Store and Sync Data "},{"uri":"https://danielleit241.github.io/aws-fcj-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerating generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI This blog introduces fully managed MLflow 3.0 on Amazon SageMaker AI, which accelerates Generative AI development by unifying experiment tracking, behavior monitoring, and model lifecycle management into a single platform. This version adds tracing and version tracking features, enabling developers to capture inputs, outputs, and metadata for easier debugging and performance optimization. With deep integration into Amazon Bedrock and SageMaker HyperPod, MLflow 3.0 enhances observability, speeds up troubleshooting, and shortens the time to production for AI models.\nBlog 2 - AI-Enhanced Subsurface Infrastructure Mapping on AWS This blog describes how S2 Labs, Empact AI, and Kraken Robotics use physics-informed AI on AWS HPC to enhance subsurface infrastructure mapping. This approach combines magnetic imaging and deep learning (using a U-Net model) to accurately reconstruct underground or underwater structures such as pipes and tanks. Leveraging AWS Batch, EC2, and S3 for parallel computing, the system achieves high precision in detecting subsurface features up to 40 meters deep‚Äîsignificantly outperforming traditional mapping methods.\nBlog 3 - Unlocking the full potential of Amazon Connect This blog explores how to unlock the full potential of Amazon Connect, an AI- and AWS-powered contact center platform. It highlights the importance of effective change management, including identifying the right stakeholders, securing an executive sponsor, building change ambassadors, and tracking meaningful performance metrics. Success relies on understanding business needs, targeted training, and strong internal communication. When properly implemented, Amazon Connect can enhance operations, enable automation, and deliver superior customer experiences‚Äîmaximizing ROI and long-term value for organizations.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DATA SCIENCE WORKSHOP ON AWS\nTime: 09:30, October 16, 2025\nLocation: FPT University, D1 Street, High-Tech Park, Tang Nhon Phu Ward, Ho Chi Minh City.\nRole in the event: Attendees\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nTime: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nTime: 09:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nTime: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole in the event: Attendees\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.4-test-chatbox/","title":"Testing Chatbot (RAG)","tags":[],"description":"","content":"Target After successfully ingesting data into the Vector Store, it is time to verify the results. In this section, you will act as an end-user, asking the Chatbot questions directly within the AWS Console interface to observe how the RAG system operates.\nWe will focus on 2 factors:\nAccuracy: Does the AI answer correctly based on the documents? Transparency: Can the AI cite the source (Citation) of the information? Implementation Steps Step 1: Configure the test window\nTo start chatting, we need to select a Foundation Model that will act as the \u0026ldquo;responder\u0026rdquo;.\nIn your Knowledge Base details interface, look at the right panel titled Test knowledge base. Click the Select model button.\nIn the selection panel that appears: Category: Select Anthropic. Model: Select Claude 3 Sonnet (or Claude 3.5 Sonnet / Haiku depending on the model you enabled). Throughput: Keep On-demand. Click Apply. Step 2: Conduct conversation (Chat)\nNow, try asking a question related to the document content you uploaded.\nIn the input box (Message input), type your question. Example: If you uploaded the \u0026ldquo;AWS Overview\u0026rdquo; document, ask: \u0026ldquo;Can you explain to me what EC2 is?\u0026rdquo;. Click Run. Observe the result: The AI will think for a few seconds (querying the Vector Store). Then, it will answer in natural language, summarizing the found information. Step 3: Verify data source\nThis is the most important feature of RAG that distinguishes it from standard ChatGPT: the ability to prove the source of information.\nIn the AI\u0026rsquo;s response, pay attention to the small numbers (footnotes) or the text Show source details. Click on those numbers or the details button. A Source details window will appear, displaying: Source chunk: The exact original text segment that the AI found in the document. Score: Similarity score (relevance). S3 Location: Path to the original file. Seeing this original text segment proves that the AI is not \u0026ldquo;hallucinating\u0026rdquo; but is actually reading your documents.\nStep 4: Test with irrelevant questions (Optional)\nTo see how the system reacts when information is not found.\nAsk a question completely unrelated to the documents. Example: \u0026ldquo;Can you explain some knowledge about personal finance?\u0026rdquo; (While your documents are about Cloud Computing). Expected Result: The AI might answer based on its general knowledge (if not restricted). OR the AI will answer \u0026ldquo;Sorry, I am unable to answer your question based on the retrieved data\u0026rdquo; - This is the ideal behavior for an enterprise RAG application. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.5-client-integration/","title":"Client Application Integration (Optional)","tags":[],"description":"","content":"Target You will turn Python code into a professional Web Chatbot GUI (Graphical User Interface) that is user-friendly (similar to the ChatGPT interface) in just a few minutes.\nWe use:\nBackend: Python. Frontend: Streamlit. AI Model: Claude 3.5 Sonnet. Implementation Steps Part I: Configure AWS Credentials\nStep 1: Install AWS CLI\nOpen Terminal on your computer.\n# macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Step 2: Configure credentials\naws configure Enter the information when prompted:\nAWS Access Key ID: YOUR_ACCESS_KEY AWS Secret Access Key: YOUR_SECRET_KEY Default region name: us-east-1 Default output format: json Step 3: Verify configuration\n# Check credentials aws sts get-caller-identity # Check Bedrock connection aws bedrock-agent-runtime list-knowledge-bases --region ap-southeast-1 Security notes:\nDO NOT commit credentials to Git DO NOT share credentials with others Use IAM roles when possible Rotate credentials periodically Required permissions:\nIAM User needs the following permissions:\nbedrock:InvokeModel bedrock:RetrieveAndGenerate bedrock:Retrieve s3:GetObject (for Knowledge Base) Troubleshooting:\nError \u0026ldquo;Unable to locate credentials\u0026rdquo;:\nCheck if ~/.aws/credentials file exists Check file format is correct Try running aws configure again Error \u0026ldquo;AccessDeniedException\u0026rdquo;:\nCheck IAM permissions Ensure region is correct (ap-southeast-1) Check Knowledge Base ID is correct Error \u0026ldquo;ExpiredToken\u0026rdquo;:\nCredentials have expired Need to create new credentials from AWS Console Part II: Clone Project from pre-made GitHub\nStep 1: Access the following GitHub link\nPlease download and open the folder above using Visual Studio Code:\nhttps://github.com/DazielNguyen/chatbot_with_bedrock.git\nStep 2: Install libraries and Python environment\nInstall environment:\nMacOS: python3 -m venv .venv Win: python -m venv .venv Activate environment:\nMacOS: source .venv/bin/activate Win: .venv\\Scripts\\activate Install libraries:\nMacOS/ Win: pip install -r requirements.txt Step 3: Get the ID of the created Knowledge Base\nAccess Amazon Bedrock -\u0026gt; Knowledge Base -\u0026gt; knowledge-base-demo Update \u0026ldquo;KB_ID=\u0026ldquo;YOUR_KNOWLEDGE_BASE_ID\u0026rdquo;\u0026rdquo; Step 4: Run Streamlit - Chatbot UI and Experience\nRun Terminal: streamlit run start.py When the command finishes running, the following page will appear: Try asking some questions you uploaded to the Knowledge Base earlier. The Chatbot has returned results based on the data file you provided, with citations of your data sources. Conclusion Congratulations on successfully building a Web Chatbot built with Amazon Bedrock\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a RAG Application using Knowledge Bases for Amazon Bedrock Overview Knowledge Bases for Amazon Bedrock is a fully managed feature that helps you implement RAG (Retrieval-Augmented Generation) techniques by connecting Foundation Models to your internal data sources to deliver accurate, cited, and contextually relevant responses.\nRAG is a technique to optimize Large Language Model (LLM) output by retrieving information from a trusted external database (Retrieval) and adding it to the context (Augmentation) before generating the answer (Generation). This method helps overcome limitations regarding outdated training data and ensures the AI answers based on the actual provided information.\nIn this lab, we will learn how to build an AI assistant capable of \u0026ldquo;reading and understanding\u0026rdquo; proprietary enterprise documents. You will perform the process from data ingestion and creating vector indexes to configuring the model to answer questions based on those documents without managing any servers.\nWe will use three main components to set up a complete RAG processing workflow:\nData Source (Amazon S3) - Acts as the repository of \u0026ldquo;truth\u0026rdquo;. You will upload documents (PDF, Word, Text) to an S3 bucket. The Knowledge Base will use this source to synchronize data. Vector Store (OpenSearch Serverless) - The place to store vector embeddings (numerically encoded data). When a user asks a question, the system will perform a semantic search here to extract the most relevant text segments instead of standard keyword searching. Foundation Model (Claude 3) - The Large Language Model acting as the processing brain. It receives the user\u0026rsquo;s question along with information found from the Vector Store, then synthesizes and generates a natural, accurate answer accompanied by source citations. Outcomes By the end of the workshop, you will have a practical, functioning Chatbot system with the following features:\nQ\u0026amp;A chat regarding proprietary document content. Accurate answers, no hallucinations. Source citations (knowing exactly which page the answer comes from). Rapid deployment without writing complex data processing code. Contents Workshop Overview Environment Preparation Create and Configure Knowledge Base Test Chatbot (RAG) Client Application Integration (Optional) Update Data Clean Up Resources "},{"uri":"https://danielleit241.github.io/aws-fcj-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my internship at the AWS/First cloud journey program from 08/08/2025 to 12/12/2025, I had the opportunity to learn, train, and apply the knowledge equipped at school into a real-world working environment. I participated in learning AWS services, as well as applying learned knowledge into a group project with a FinTech theme, thereby improving teamwork skills, task allocation for team members, and especially achieving desired skills such as Docker, DevOps, and applying AWS services to support programming work.\nRegarding professional conduct, I always strive to complete tasks well, adhere to regulations, and actively exchange with colleagues to improve work efficiency.\nTo reflect objectively on the internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the industry, applying knowledge to reality, tool usage skills, work quality ‚úÖ ‚òê ‚òê 2 Learning ability Absorbing new knowledge, learning quickly ‚òê ‚úÖ ‚òê 3 Proactivity Self-researching, accepting tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to time, regulations, and work processes ‚úÖ ‚òê ‚òê 6 Eagerness to improve Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas, reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues, participating in the group ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving mindset Identifying problems, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/organization Work efficiency, improvement initiatives, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General assessment of the entire internship process ‚úÖ ‚òê ‚òê Areas for Improvement Enhance discipline and strictly adhere to the regulations of the company or any organization. Train myself in persistence with specific tasks instead of drifting to other issues that cause distractions. Improve the mindset for problem-solving. Improve communication skills and the articulation of personal issues to Mentors as well as teammates. Need to research and learn more about new technologies, and learn to apply them rather than just learning for knowledge\u0026rsquo;s sake. Support group members more in work as well as in learning. "},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.6-update-data/","title":"Update data","tags":[],"description":"","content":"Target One of the biggest advantages of RAG compared to Fine-tuning (retraining) a model is the ability to update data quickly. When a business has new regulations, you simply need to ingest them into the Knowledge Base, and the AI will \u0026ldquo;learn\u0026rdquo; them immediately.\nIn this section, we will simulate the following scenario:\nAsk the AI for a piece of non-existent information (The AI will answer that it doesn\u0026rsquo;t know). Provide that information to the system by uploading a new file. Ask the same question again to witness the AI answer correctly immediately. Implementation Steps Step 1: Verify the initial \u0026ldquo;lack of knowledge\u0026rdquo;\nWe need to confirm that the current AI knows nothing about the confidential information we are about to create.\nReturn to the Streamlit Chatbot interface (created in Part 5) or use the Test Knowledge Base window on the Console. Ask a question about hypothetical fake information. Example: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Observe the result: The AI will answer that it cannot find the information in the provided documents or will attempt a generic answer (if not restricted). Step 2: Create new data\nWe will create a text file containing this \u0026ldquo;secret\u0026rdquo; to ingest into the system.\nOn your computer, open Notepad (Windows) or TextEdit (Mac). Copy and paste the following content into the file: CONFIDENTIAL NOTICE: The secret Project Omega officially launches on 01/12/2025. The activation code is: \u0026#34;AWS-ROCKS-2025-SINGAPORE\u0026#34;. The Project Manager is Mr. Robot. Please keep this information strictly confidential. Save the file as: secret-project.txt. You can download the file here: TXT format file\nStep 3: Upload and Sync\nNow, we will feed this new knowledge into the AI\u0026rsquo;s \u0026ldquo;brain\u0026rdquo;.\nAccess the S3 Console, navigate to your old bucket (rag-workshop-demo).\nClick Upload -\u0026gt; Add files -\u0026gt; Select the secret-project.txt file -\u0026gt; Upload.\nSwitch to the Amazon Bedrock Console -\u0026gt; Select Knowledge bases from the left menu. Click on your Knowledge Base name. Scroll down to the Data source section, select the data source (s3-datasource). Click the Sync button (Orange). Wait: Wait for about 30 seconds to 1 minute until the Status column changes from Syncing to Available. Step 4: Verify again (The \u0026ldquo;Wow\u0026rdquo; Moment)\nThe system now possesses the new knowledge. Let\u0026rsquo;s challenge the AI once again.\nReturn to the Streamlit Chatbot interface (No need to reload the page or restart the server). Ask the exact same question as before: \u0026ldquo;What is the activation code for Project Omega?\u0026rdquo; Expected Result: The AI answers correctly: \u0026ldquo;The activation code is AWS-ROCKS-2025-SINGAPORE\u0026rdquo;. The AI cites the source as the secret-project.txt file. Conclusion You have just witnessed the true power of RAG!\nNo code editing required. No model retraining required. Simply Sync the data. Your Chatbot has become smarter and updated with the latest information in just a few simple steps. This is exactly why businesses choose this solution to build internal virtual assistants.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/5-workshop/5.7-cleanup/","title":"Clean Resources","tags":[],"description":"","content":"Target To avoid incurring unwanted costs after finishing the practice lab, we need to delete the created resources.\n‚ö†Ô∏è WARNING: Deleting the Knowledge Base DOES NOT automatically delete the Vector Store (OpenSearch Serverless). You must manually delete the OpenSearch Serverless Collection as this is the costliest service in this Lab.\nImplementation Steps Step 1: Delete Knowledge Base\nAccess the Amazon Bedrock Console -\u0026gt; Knowledge bases.\nSelect the radio button next to your Knowledge Base name.\nClick the Delete button.\nA dialog box appears, enter the Knowledge Base name to confirm (or type delete).\nClick Delete. This process takes 10-15 minutes to complete successfully, so please be patient.\nStep 2: Delete Vector Store\nAccess the Amazon OpenSearch Service. In the left menu, under Serverless, select Collections. You will see a Collection named like bedrock-knowledge-base-.... Select the radio button next to that Collection name. Click the Delete button. Type confirm or the collection name to confirm deletion. Click Delete. Step 3: Delete Data on S3\nAccess the Amazon S3 service. Select the bucket rag-workshop-demo. Click the Empty button first. Type permanently delete to confirm deleting all files inside. After the bucket is empty, return to the Buckets list. Select that bucket again and click the Delete button. Enter the bucket name to confirm. Completion Congratulations on fully completing the Workshop \u0026ldquo;Building a RAG Application with Amazon Bedrock\u0026rdquo;. Your system has been cleaned up and is safe!\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is professional; members of the FCJ community are very friendly and open. In particular, the FCJ seniors as well as fellow FCJ members in my cohort were always ready to support and answer questions when my team and I faced difficulties in designing and using AWS services. Because of that help, I realized my mistakes earlier. Instead of having to pay a large amount of money [for cloud costs], that assistance helped me gain more knowledge with the only trade-off being the questions asked. The workspace is neat and comfortable, and the learning spirit of my peers motivated me to be more focused and determined with the goals I had set.\nHowever, I think that next time, if possible, there should be a fixed schedule so that team members can come to the office more easily instead of competing [for slots] as is currently the case. This would allow everyone in the group to access and exchange ideas directly more easily, as some members did not have time to get to know each other outside and could only interact online initially.\n2. Support from Mentor / Team Admin\nThe Mentors are extremely dedicated and conscientious; not only is the quality high, but the number of Mentors is also significant, and they are ready to support us extensively. Regardless of the time‚Äîsometimes even at 2-3 AM‚Äîthey still provided feedback so we could make edits in time for the deadline. Not stopping at support, they also shared useful knowledge accumulated through their work experience, which are invaluable lessons that FCJ brought to me. What I like about them is that they spark curiosity regarding the mistakes I made during the learning and working process. Instead of giving the answer, they guided me to official documentation so I could find the answer myself and deeply ingrain those mistakes so as not to repeat them next time.\n3. Relevance of Work to Academic Major\nThe work I performed was like the missing puzzle piece from my studies at university. Developing products locally is different from developing a product that is more user-oriented, which requires handling more issues, especially regarding security as well as software consistency. Doing harder and more advanced tasks in my field of study has helped me no longer feel \u0026ldquo;afraid\u0026rdquo; of working with them. Thanks to this, I both consolidated my programming knowledge and learned DevOps skills and other soft skills such as teamwork, communication, etc., which has helped me develop significantly in the present and even for the future.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills, not only regarding my major but also other soft skills. I got to work with new people, with people not from the same major, thereby gaining different perspectives of each major as well as skills through sharing among group members. I attended many cool events hosted by seniors with extensive experience in large corporations, learning how they apply AI to work as well as other advanced programming skills, helping me better orient the career I am pursuing.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive; everyone supports each other enthusiastically with a primary spirit of learning and sharing knowledge. During work, everyone is extremely focused, but there are also times that are extremely fun. When deadlines approach, everyone strives together, supporting each other regardless of position; those who are finished help those who are not, and those who know help those who don\u0026rsquo;t. As long as there is a problem, everyone will support solving that problem as quickly and effectively as possible. This helped me easily integrate with everyone, even just in the role of an intern.\nAdditional Questions The thing I am most satisfied with during the internship period is the spirit of the FCJers, a spirit ready to share everything within their knowledge, leaving no one behind. Such a spirit helped me partly understand that FCJ is not just a program, but an entire community eager to learn, ready to share new knowledge with interns‚Äînot just me, but many others who have not yet had access to it.\n"},{"uri":"https://danielleit241.github.io/aws-fcj-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://danielleit241.github.io/aws-fcj-report/tags/","title":"Tags","tags":[],"description":"","content":""}]